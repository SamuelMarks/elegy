{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Elegy Elegy is a Neural Networks framework based on Jax and Haiku. Elegy implements the Keras API but makes changes to play better with Jax & Haiku and give more flexibility around losses and metrics (more on this soon). Elegy is still in a very early stage, feel free to test it and send us your feedback! Main Features Familiar : Elegy should feel very familiar to Keras users. Flexible : Elegy improves upon the basic Keras API by letting users optionally take more control over the definition of losses and metrics. Easy-to-use : Elegy maintains all the simplicity and ease of use that Keras brings with it. Compatible : Elegy strives to be compatible with the rest of the Jax and Haiku ecosystem. For more information take a look at the Documentation . Installation Install Elegy using pip: pip install elegy For Windows users we recomend the Windows subsystem for linux 2 WSL2 since jax does not have support for it yet. Quick Start Elegy greatly simplifies the training of Deep Learning models compared to pure Jax / Haiku where, due to Jax functional nature, users have to do a lot of book keeping around the state of the model. In Elegy just you just have to follow 3 basic steps: 1. Define the architecture inside an elegy.Module : class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) 2. Create a Model from this module and specify additional things like losses, metrics, and optimizers: model = elegy . Model ( module = MLP . defer (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy . defer (), optimizer = optix . rmsprop ( 1e-3 ), ) 3. Train the model using the fit method: model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . TensorBoard ( \"summaries\" )] ) And you are done! For a more information checkout: Our Getting Started tutorial. Couple of examples in examples directory. Haiku's User Manual and Documentation What is Jax? Why Jax + Haiku? Jax is a linear algebra library with the perfect recipe: * Numpy's familiar API * The speed and hardware support of XLA * Automatic Differentiation The awesome thing about Jax that Deep Learning is just a usecase that it happens to excel at but you can use it for most task you would use Numpy for. On the other hand, Haiku is a Neural Networks library built on top of Jax that implements a Module system, common Neural Network layers, and even some full architectures. Compared to other Jax-based libraries like Trax or Flax, Haiku is very minimal, polished, well documented, and makes it super easy / clean to implement Deep Learning code! We believe that Elegy can offer the best experience for coding Deep Learning applications by leveraging the power and familiarity of Jax API, the ease-of-use of Haiku's Module system, and packaging everything on top of a convenient Keras-like API. Features Model estimator class losses module metrics module regularizers module callbacks module nn layers module For more information checkout the Reference API section in the Documentation . Contributing Deep Learning is evolving at an incredible rate, there is so much to do and so few hands. If you wish to contibute anything from a loss or metrics to a new awesome feature for Elegy just open an issue or send a PR! For more information checkout our Contibuting Guide . About Us We are a couple friends passionate about ML. License Apache Citing Elegy To cite this project: BibTeX @software{elegy2020repository, author = {PoetsAI}, title = {Elegy: A Keras-like deep learning framework based on Jax & Haiku}, url = {https://github.com/poets-ai/elegy}, version = {0.1.3}, year = {2020}, } Where the current version may be retrieved either from the Release tag or the file elegy/__init__.py and the year corresponds to the project's release year.","title":"Introduction"},{"location":"#elegy","text":"Elegy is a Neural Networks framework based on Jax and Haiku. Elegy implements the Keras API but makes changes to play better with Jax & Haiku and give more flexibility around losses and metrics (more on this soon). Elegy is still in a very early stage, feel free to test it and send us your feedback!","title":"Elegy"},{"location":"#main-features","text":"Familiar : Elegy should feel very familiar to Keras users. Flexible : Elegy improves upon the basic Keras API by letting users optionally take more control over the definition of losses and metrics. Easy-to-use : Elegy maintains all the simplicity and ease of use that Keras brings with it. Compatible : Elegy strives to be compatible with the rest of the Jax and Haiku ecosystem. For more information take a look at the Documentation .","title":"Main Features"},{"location":"#installation","text":"Install Elegy using pip: pip install elegy For Windows users we recomend the Windows subsystem for linux 2 WSL2 since jax does not have support for it yet.","title":"Installation"},{"location":"#quick-start","text":"Elegy greatly simplifies the training of Deep Learning models compared to pure Jax / Haiku where, due to Jax functional nature, users have to do a lot of book keeping around the state of the model. In Elegy just you just have to follow 3 basic steps: 1. Define the architecture inside an elegy.Module : class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) 2. Create a Model from this module and specify additional things like losses, metrics, and optimizers: model = elegy . Model ( module = MLP . defer (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy . defer (), optimizer = optix . rmsprop ( 1e-3 ), ) 3. Train the model using the fit method: model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . TensorBoard ( \"summaries\" )] ) And you are done! For a more information checkout: Our Getting Started tutorial. Couple of examples in examples directory. Haiku's User Manual and Documentation What is Jax?","title":"Quick Start"},{"location":"#why-jax-haiku","text":"Jax is a linear algebra library with the perfect recipe: * Numpy's familiar API * The speed and hardware support of XLA * Automatic Differentiation The awesome thing about Jax that Deep Learning is just a usecase that it happens to excel at but you can use it for most task you would use Numpy for. On the other hand, Haiku is a Neural Networks library built on top of Jax that implements a Module system, common Neural Network layers, and even some full architectures. Compared to other Jax-based libraries like Trax or Flax, Haiku is very minimal, polished, well documented, and makes it super easy / clean to implement Deep Learning code! We believe that Elegy can offer the best experience for coding Deep Learning applications by leveraging the power and familiarity of Jax API, the ease-of-use of Haiku's Module system, and packaging everything on top of a convenient Keras-like API.","title":"Why Jax + Haiku?"},{"location":"#features","text":"Model estimator class losses module metrics module regularizers module callbacks module nn layers module For more information checkout the Reference API section in the Documentation .","title":"Features"},{"location":"#contributing","text":"Deep Learning is evolving at an incredible rate, there is so much to do and so few hands. If you wish to contibute anything from a loss or metrics to a new awesome feature for Elegy just open an issue or send a PR! For more information checkout our Contibuting Guide .","title":"Contributing"},{"location":"#about-us","text":"We are a couple friends passionate about ML.","title":"About Us"},{"location":"#license","text":"Apache","title":"License"},{"location":"#citing-elegy","text":"To cite this project: BibTeX @software{elegy2020repository, author = {PoetsAI}, title = {Elegy: A Keras-like deep learning framework based on Jax & Haiku}, url = {https://github.com/poets-ai/elegy}, version = {0.1.3}, year = {2020}, } Where the current version may be retrieved either from the Release tag or the file elegy/__init__.py and the year corresponds to the project's release year.","title":"Citing Elegy"},{"location":"getting-started/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); In this tutorial we will explore the basic features of Elegy . If you are a Keras user you should feel at home, if you are currently using Jax or Haiku things will appear much more streamlined. To get started you will first need to install the following dependencies: In [ ]: ! pip install elegy dataget matplotlib Note that Elegy doesn't depend on jax since there are both cpu and gpu version you can choose from so you will need to install it separately. Loading the Data \u00b6 In this tutorial we will train a Neural Network on the MNIST dataset, for this we will first need to download and load the data into memory. Here we will use dataget for simplicity but you can use you favorite datasets library. In [2]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () print ( \"X_train:\" , X_train . shape , X_train . dtype ) print ( \"y_train:\" , y_train . shape , y_train . dtype ) print ( \"X_test:\" , X_test . shape , X_test . dtype ) print ( \"y_test:\" , y_test . shape , y_test . dtype ) X_train: (60000, 28, 28) uint8 y_train: (60000,) uint8 X_test: (10000, 28, 28) uint8 y_test: (10000,) uint8 In this case dataget loads the data from Yann LeCun's website. Creating the Model \u00b6 Now that we have the data we can define our model. In Elegy you can do this by inheriting from elegy.Module and defining a call method. This method should take in some inputs, perform a series of transformation using Jax and Haiku expressions, and returns the outputs of the network. In this example we will create a simple 2 layer MLP using Haiku modules: In [3]: import jax.numpy as jnp import jax import haiku as hk import elegy class MLP ( elegy . Module ): \"\"\"Standard LeNet-300-100 MLP network.\"\"\" def __init__ ( self , n1 : int = 300 , n2 : int = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . n1 = n1 self . n2 = n2 def call ( self , image : jnp . ndarray ) -> jnp . ndarray : image = image . astype ( jnp . float32 ) / 255.0 mlp = hk . Sequential ( [ hk . Flatten (), hk . Linear ( self . n1 ), jax . nn . relu , hk . Linear ( self . n2 ), jax . nn . relu , hk . Linear ( 10 ), ] ) return mlp ( image ) Here we are using Sequential to stack two layers with relu activations and a final Linear layer with 10 units that represents the logits of the network. This code should feel familiar to most Keras / PyTorch users, the main difference here is that instead of assigning layers / modules as fields inside __init__ and later using them in call / forward , here we can just use them inplace since Haiku tracks the state for us \"behind the scenes\". Writing model code in Elegy / Haiku often feels easier since there tends to be a lot less boilerplate thanks to Haiku hooks. For a premier on Haiku please refer to this Quick Start . Note elegy.Module is just a thin wrapper over haiku.Module that adds certain Elegy-related functionalities, you can inherit from from haiku.Module instead if you wish, just remember to also rename call to __call__ Now that we have this module we can create an Elegy Model . In [4]: from jax.experimental import optix model = elegy . Model ( module = lambda : MLP ( n1 = 300 , n2 = 100 ), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), optimizer = optix . rmsprop ( 1e-3 ), ) Much like keras.Model , an Elegy Model is tasked with performing training, evalaution, and inference. The constructor of this class accepts most of the arguments accepted by keras.Model.compile as you might have seen but there are some notable differences: It requires you to pass a module as first argument. Loss can be a list even if we don't have multiple corresponding outputs/labels, this is because Elegy exposes a more flexible system for defining losses and metrics based on Dependency Injection. You might have notice some weird lambda expressions around module and metrics , these arise because Haiku prohibits the creation of haiku.Module s outside of a haiku.transform . To go around this restriction we just defer instantiation of these object by wrapping them inside a lambda and calling them later. For convenience both the elegy.Module and elegy.Metric classes define a defer classmethod that which you can use to make things more readable: In [5]: model = elegy . Model ( module = MLP . defer ( n1 = 300 , n2 = 100 ), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-4 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy . defer (), optimizer = optix . rmsprop ( 1e-3 ), ) Training the Model \u00b6 Having our model instance ready we now need to pass it some data to start training. Like in Keras this is done via the fit method which contains more or less the same signature. We try to be as compatible with Keras as possible here but also remove a lot of the Tensorflow specific stuff. The following code will train our model for 100 epochs while limiting each epoch to 200 steps and using a batch size of 64 : In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . ModelCheckpoint ( \"model\" , save_best_only = True )], ) ... Epoch 99/100 200/200 [==============================] - 1s 5ms/step - l2_regularization_loss: 0.0094 - loss: 0.0105 - sparse_categorical_accuracy: 0.9958 - sparse_categorical_crossentropy_loss: 0.0011 - val_l2_regularization_loss: 0.0094 - val_loss: 0.0094 - val_sparse_categorical_accuracy: 0.9813 - val_sparse_categorical_crossentropy_loss: 7.4506e-09 Epoch 100/100 200/200 [==============================] - 1s 5ms/step - l2_regularization_loss: 0.0094 - loss: 0.0271 - sparse_categorical_accuracy: 0.9966 - sparse_categorical_crossentropy_loss: 0.0177 - val_l2_regularization_loss: 0.0094 - val_loss: 0.0094 - val_sparse_categorical_accuracy: 0.9806 - val_sparse_categorical_crossentropy_loss: 4.4703e-08 We've ported Keras beloved progress bar and also implemented its Callback and History APIs. fit returns a history object which we will use next to visualize how the metrics and losses evolved during training. In [7]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history ) Doing Inference \u00b6 Having our trained model we can now get some samples from the test set and generate some predictions. First we will just pick some random samples using numpy : In [8]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) x_sample = X_test [ idxs ] Here we selected 9 random images. Now we can use the predict method to get their labels: In [9]: y_pred = model . predict ( x = x_sample ) Easy right? Finally lets plot the results to see if they are accurate. In [10]: plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( x_sample [ k ], cmap = \"gray\" ) Perfect! Loading Saved Model \u00b6 Since we used elegy.callbacks.ModelCheckpoint we can always restore our model from disk in the future. In [11]: try : # current model reference print ( \"current model reference:\" , model ) model = elegy . model . load ( \"model\" ) except : print ( \"Could not load model, this is pobably due to a bug in `cloudpickle \" \"on certain python versions. For better results try Python >= 3.8. \" \"An alternative way to load the model is to manually build the model from \" \"the source code and use `model.load('model')` which will only load the weights + state.\" ) model . load ( \"model\" ) # new model reference print ( \"new model reference: \" , model ) # check that it works! model . predict ( x = x_sample ) . shape current model reference: <elegy.model.Model object at 0x7fd8beee4ac8> new model reference: <elegy.model.Model object at 0x7fd8a353e358> Out[11]: (9, 10) We hope you've enjoyed this tutorial. Next Steps \u00b6 Elegy is still in a very early stage, there are probably tons of bugs and missing features but we will get there. If you have some ideas or feedback on the current design we are eager to hear from you, feel free to open an issue.","title":"Getting Started"},{"location":"getting-started/#loading-the-data","text":"In this tutorial we will train a Neural Network on the MNIST dataset, for this we will first need to download and load the data into memory. Here we will use dataget for simplicity but you can use you favorite datasets library. In [2]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () print ( \"X_train:\" , X_train . shape , X_train . dtype ) print ( \"y_train:\" , y_train . shape , y_train . dtype ) print ( \"X_test:\" , X_test . shape , X_test . dtype ) print ( \"y_test:\" , y_test . shape , y_test . dtype ) X_train: (60000, 28, 28) uint8 y_train: (60000,) uint8 X_test: (10000, 28, 28) uint8 y_test: (10000,) uint8 In this case dataget loads the data from Yann LeCun's website.","title":"Loading the Data"},{"location":"getting-started/#creating-the-model","text":"Now that we have the data we can define our model. In Elegy you can do this by inheriting from elegy.Module and defining a call method. This method should take in some inputs, perform a series of transformation using Jax and Haiku expressions, and returns the outputs of the network. In this example we will create a simple 2 layer MLP using Haiku modules: In [3]: import jax.numpy as jnp import jax import haiku as hk import elegy class MLP ( elegy . Module ): \"\"\"Standard LeNet-300-100 MLP network.\"\"\" def __init__ ( self , n1 : int = 300 , n2 : int = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . n1 = n1 self . n2 = n2 def call ( self , image : jnp . ndarray ) -> jnp . ndarray : image = image . astype ( jnp . float32 ) / 255.0 mlp = hk . Sequential ( [ hk . Flatten (), hk . Linear ( self . n1 ), jax . nn . relu , hk . Linear ( self . n2 ), jax . nn . relu , hk . Linear ( 10 ), ] ) return mlp ( image ) Here we are using Sequential to stack two layers with relu activations and a final Linear layer with 10 units that represents the logits of the network. This code should feel familiar to most Keras / PyTorch users, the main difference here is that instead of assigning layers / modules as fields inside __init__ and later using them in call / forward , here we can just use them inplace since Haiku tracks the state for us \"behind the scenes\". Writing model code in Elegy / Haiku often feels easier since there tends to be a lot less boilerplate thanks to Haiku hooks. For a premier on Haiku please refer to this Quick Start . Note elegy.Module is just a thin wrapper over haiku.Module that adds certain Elegy-related functionalities, you can inherit from from haiku.Module instead if you wish, just remember to also rename call to __call__ Now that we have this module we can create an Elegy Model . In [4]: from jax.experimental import optix model = elegy . Model ( module = lambda : MLP ( n1 = 300 , n2 = 100 ), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), optimizer = optix . rmsprop ( 1e-3 ), ) Much like keras.Model , an Elegy Model is tasked with performing training, evalaution, and inference. The constructor of this class accepts most of the arguments accepted by keras.Model.compile as you might have seen but there are some notable differences: It requires you to pass a module as first argument. Loss can be a list even if we don't have multiple corresponding outputs/labels, this is because Elegy exposes a more flexible system for defining losses and metrics based on Dependency Injection. You might have notice some weird lambda expressions around module and metrics , these arise because Haiku prohibits the creation of haiku.Module s outside of a haiku.transform . To go around this restriction we just defer instantiation of these object by wrapping them inside a lambda and calling them later. For convenience both the elegy.Module and elegy.Metric classes define a defer classmethod that which you can use to make things more readable: In [5]: model = elegy . Model ( module = MLP . defer ( n1 = 300 , n2 = 100 ), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-4 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy . defer (), optimizer = optix . rmsprop ( 1e-3 ), )","title":"Creating the Model"},{"location":"getting-started/#training-the-model","text":"Having our model instance ready we now need to pass it some data to start training. Like in Keras this is done via the fit method which contains more or less the same signature. We try to be as compatible with Keras as possible here but also remove a lot of the Tensorflow specific stuff. The following code will train our model for 100 epochs while limiting each epoch to 200 steps and using a batch size of 64 : In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . ModelCheckpoint ( \"model\" , save_best_only = True )], ) ... Epoch 99/100 200/200 [==============================] - 1s 5ms/step - l2_regularization_loss: 0.0094 - loss: 0.0105 - sparse_categorical_accuracy: 0.9958 - sparse_categorical_crossentropy_loss: 0.0011 - val_l2_regularization_loss: 0.0094 - val_loss: 0.0094 - val_sparse_categorical_accuracy: 0.9813 - val_sparse_categorical_crossentropy_loss: 7.4506e-09 Epoch 100/100 200/200 [==============================] - 1s 5ms/step - l2_regularization_loss: 0.0094 - loss: 0.0271 - sparse_categorical_accuracy: 0.9966 - sparse_categorical_crossentropy_loss: 0.0177 - val_l2_regularization_loss: 0.0094 - val_loss: 0.0094 - val_sparse_categorical_accuracy: 0.9806 - val_sparse_categorical_crossentropy_loss: 4.4703e-08 We've ported Keras beloved progress bar and also implemented its Callback and History APIs. fit returns a history object which we will use next to visualize how the metrics and losses evolved during training. In [7]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history )","title":"Training the Model"},{"location":"getting-started/#doing-inference","text":"Having our trained model we can now get some samples from the test set and generate some predictions. First we will just pick some random samples using numpy : In [8]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) x_sample = X_test [ idxs ] Here we selected 9 random images. Now we can use the predict method to get their labels: In [9]: y_pred = model . predict ( x = x_sample ) Easy right? Finally lets plot the results to see if they are accurate. In [10]: plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( x_sample [ k ], cmap = \"gray\" ) Perfect!","title":"Doing Inference"},{"location":"getting-started/#loading-saved-model","text":"Since we used elegy.callbacks.ModelCheckpoint we can always restore our model from disk in the future. In [11]: try : # current model reference print ( \"current model reference:\" , model ) model = elegy . model . load ( \"model\" ) except : print ( \"Could not load model, this is pobably due to a bug in `cloudpickle \" \"on certain python versions. For better results try Python >= 3.8. \" \"An alternative way to load the model is to manually build the model from \" \"the source code and use `model.load('model')` which will only load the weights + state.\" ) model . load ( \"model\" ) # new model reference print ( \"new model reference: \" , model ) # check that it works! model . predict ( x = x_sample ) . shape current model reference: <elegy.model.Model object at 0x7fd8beee4ac8> new model reference: <elegy.model.Model object at 0x7fd8a353e358> Out[11]: (9, 10) We hope you've enjoyed this tutorial.","title":"Loading Saved Model"},{"location":"getting-started/#next-steps","text":"Elegy is still in a very early stage, there are probably tons of bugs and missing features but we will get there. If you have some ideas or feedback on the current design we are eager to hear from you, feel free to open an issue.","title":"Next Steps"},{"location":"api/Model/","text":"elegy.Model Model is tasked with performing training, evaluation, and inference for a given elegy.Module or haiku.Module . To create a Model you first have to define its architecture in a Module : class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) Then you can pass this Module to the Model 's constructor and specify additional things like losses, metrics, optimizer, and callbacks: model = elegy . Model ( module = MLP . defer (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy . defer (), optimizer = optix . rmsprop ( 1e-3 ), ) Once the model is created, you can train the model with model.fit() , or use the model to do prediction with model.predict() . Checkout Getting Started for additional details. Attributes: Name Type Description params Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.Params structure with the weights of the model. state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.State structure with non-trainable parameters of the model. optimizer_state Optional[NamedTuple] A optix.OptState structure with state of the optimizer. metrics_state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.State structure with the state of the metrics. initial_metrics_state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.State structure with the initial state of the metrics. run_eagerly bool Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code, instead of using Jax's jit to optimize the computation. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. seed: Union [ numpy . ndarray , int ] property writable Current random state of the model. __init__ ( self , module , loss = None , metrics = None , optimizer = None , run_eagerly = False , params = None , state = None , optimizer_state = None , metrics_state = None , initial_metrics_state = None , seed = 42 ) special [summary] Parameters: Name Type Description Default module Callable A 0-argument function that returns a Haiku or Elegy Module instance. required loss Optional[Union[Callable, List, Dict]] A elegy.Loss or Callable instance representing the loss function of the network. You can define more loss terms by simply passing a possibly nested structure of lists and dictionaries of elegy.Loss or Callable s. Usually a plain list of losses is enough but using dictionaries will create namescopes for the names of the losses which might be useful e.g. to group things in tensorboard. Contrary to Keras convention, in Elegy there is no relation between the structure of loss with the structure of the labels and outputs of the network. Elegy's loss system is more flexible than the one provided by Keras, for more information on how to mimick Keras behavior checkout the Losses and Metrics Guide `. None metrics Optional[Union[Callable, List, Dict]] A elegy.Metric or Callable instance representing the loss function of the network. You can define more metrics terms by simply passing a possibly nested structure of lists and dictionaries of elegy.Metric or Callable s. Usually a plain list of metrics is enough but using dictionaries will create namescopes for the names of the metrics which might be useful e.g. to group things in tensorboard. Contrary to Keras convention, in Elegy there is no relation between the structure of metrics with the structure of the labels and outputs of the network. Elegy's metrics system is more flexible than the one provided by Keras, for more information on how to mimick Keras behavior checkout the Losses and Metrics Guide `. None optimizer Optional[jax.experimental.optix.GradientTransformation] A optix optimizer instance. Optix is a very flexible library for defining optimization pipelines with things like learning rate schedules, this means that there is no need for a LearningRateScheduler callback in Elegy. None run_eagerly bool Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code, instead of using Jax's jit to. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. False params Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.Params structure with the weights of the model. None state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.State structure with non-trainable parameters of the model. None optimizer_state Optional[NamedTuple] A optix.OptState structure with state of the optimizer. None metrics_state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.State structure with the state of the metrics. None initial_metrics_state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.State structure with the initial state of the metrics. None seed Union[numpy.ndarray, int] The initial random state of the model. 42 Source code in elegy/model.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def __init__ ( self , module : tp . Callable , loss : tp . Union [ tp . Callable , tp . List , tp . Dict , None ] = None , metrics : tp . Union [ tp . Callable , tp . List , tp . Dict , None ] = None , optimizer : tp . Optional [ optix . GradientTransformation ] = None , run_eagerly : bool = False , params : tp . Optional [ hk . Params ] = None , state : tp . Optional [ hk . State ] = None , optimizer_state : tp . Optional [ optix . OptState ] = None , metrics_state : tp . Optional [ hk . State ] = None , initial_metrics_state : tp . Optional [ hk . State ] = None , seed : tp . Union [ np . ndarray , int ] = 42 , ): \"\"\"[summary] Arguments: module: A 0-argument function that returns a Haiku or Elegy `Module` instance. loss: A `elegy.Loss` or `Callable` instance representing the loss function of the network. You can define more loss terms by simply passing a possibly nested structure of lists and dictionaries of `elegy.Loss` or `Callable`s. Usually a plain list of losses is enough but using dictionaries will create namescopes for the names of the losses which might be useful e.g. to group things in tensorboard. Contrary to Keras convention, in Elegy there is no relation between the structure of `loss` with the structure of the labels and outputs of the network. Elegy's loss system is more flexible than the one provided by Keras, for more information on how to mimick Keras behavior checkout the [Losses and Metrics Guide](https://poets-ai.github.io/elegy/guides/losses-and-metrics)`. metrics: A `elegy.Metric` or `Callable` instance representing the loss function of the network. You can define more metrics terms by simply passing a possibly nested structure of lists and dictionaries of `elegy.Metric` or `Callable`s. Usually a plain list of metrics is enough but using dictionaries will create namescopes for the names of the metrics which might be useful e.g. to group things in tensorboard. Contrary to Keras convention, in Elegy there is no relation between the structure of `metrics` with the structure of the labels and outputs of the network. Elegy's metrics system is more flexible than the one provided by Keras, for more information on how to mimick Keras behavior checkout the [Losses and Metrics Guide](https://poets-ai.github.io/elegy/guides/losses-and-metrics)`. optimizer: A `optix` optimizer instance. Optix is a very flexible library for defining optimization pipelines with things like learning rate schedules, this means that there is no need for a `LearningRateScheduler` callback in Elegy. run_eagerly: Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code, instead of using Jax's `jit` to. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. params: A `haiku.Params` structure with the weights of the model. state: A `haiku.State` structure with non-trainable parameters of the model. optimizer_state: A `optix.OptState` structure with state of the optimizer. metrics_state: A `haiku.State` structure with the state of the metrics. initial_metrics_state: A `haiku.State` structure with the initial state of the metrics. seed: The initial random state of the model. \"\"\" if metrics is not None : metrics = metric_modes . forward_all ( metrics ) if loss is None : def loss_ ( y_true , y_pred ): return 0.0 loss = loss_ loss = loss_modes . forward_all ( loss ) def model_fn ( * args , ** kwargs ): module = self . _module_fn () return utils . inject_dependencies ( module )( * args , ** kwargs ) self . _module_fn = module self . _model_transform = hk . transform_with_state ( model_fn ) self . _loss_fn = utils . inject_dependencies ( loss ) self . _metrics_transform = ( hk . transform_with_state ( utils . inject_dependencies ( metrics , rename = { \"__params\" : \"params\" , \"__state\" : \"state\" } ) ) if metrics else None ) self . _optimizer = optimizer if optimizer is not None else optix . adam ( 1e-3 ) self . _rngs = hk . PRNGSequence ( seed ) self . params = params self . state = state self . optimizer_state = optimizer_state self . metrics_state = metrics_state self . initial_metrics_state = initial_metrics_state self . run_eagerly = run_eagerly evaluate ( self , x , y = None , verbose = 1 , batch_size = None , sample_weight = None , steps = None , callbacks = None ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required y Optional[Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 1 batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None steps Optional[int] Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . This argument is not supported with array inputs. None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.Model.fit]. Returns: Type Description Dict[str, numpy.ndarray] A dictionary for mapping the losses and metrics names to the values obtained. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model.py 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 def evaluate ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], y : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. This argument is not supported with array inputs. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.Model.fit]. Returns: A dictionary for mapping the losses and metrics names to the values obtained. Raises: ValueError: in case of invalid arguments. \"\"\" data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , is_training = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . test_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs fit ( self , x , y = None , batch_size = None , epochs = 1 , verbose = 1 , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 ) Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for generator type is given below. required y Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator. 0.0 validation_data Optional[Union[Tuple, Iterable]] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy/Jax arrays, list of arrays or mappings tuple (x_val, y_val, val_sample_weights) of Numpy/Jax arrays, list of arrays or mappings generator For the first two cases, batch_size must be provided. For the last case, validation_steps should be provided, and should follow the same convention for yielding data as x . Note that validation_data does not support all the data types that are supported in x , eg, dict. None shuffle bool Boolean (whether to shuffle the training data before each epoch). This argument is ignored when x is a generator. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of generators (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description History A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model.py 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 def fit ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], y : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ np . ndarray ] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , ) -> History : \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for generator type is given below. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a generator. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy/Jax arrays, list of arrays or mappings - tuple `(x_val, y_val, val_sample_weights)` of Numpy/Jax arrays, list of arrays or mappings - generator For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` should be provided, and should follow the same convention for yielding data as `x`. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict. shuffle: Boolean (whether to shuffle the training data before each epoch). This argument is ignored when `x` is a generator. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of generators (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) # sample_weight = batch[2] if len(batch) == 3 else None x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . train_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) epoch_logs = copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) # print( # f\"epoch: {epoch} - \" # + \" - \".join(f\"{key}: {value:.3f}\" for key, value in epoch_logs.items()) # ) if self . stop_training : break callbacks . on_train_end () return self . history load ( self , path ) Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the haiku.Params + haiku.State structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Source code in elegy/model.py 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 def load ( self , path : tp . Union [ str , Path ]) -> None : \"\"\" Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the `haiku.Params` + `haiku.State` structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Arguments: path: path to a saved model's directory. \"\"\" if isinstance ( path , str ): path = Path ( path ) state : tp . Dict = deepdish . io . load ( path / \"parameters.h5\" ) optimizer_state_path = path / \"optimizer_state.pkl\" if optimizer_state_path . exists (): with open ( optimizer_state_path , \"rb\" ) as f : state [ \"optimizer_state\" ] = pickle . load ( f ) self . full_state = state predict ( self , x , verbose = 0 , batch_size = None , steps = None , callbacks = None ) Generates output predictions for the input samples. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generators (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.Model.fit]. Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description ndarray Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model.py 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 def predict ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> np . ndarray : \"\"\"Generates output predictions for the input samples. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generators (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.Model.fit]. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs ,) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs predict_on_batch ( self , x , seed = None ) Returns predictions for a single batch of samples. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. required Returns: Type Description Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Jax array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between given number of inputs and expectations of the model. Source code in elegy/model.py 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 def predict_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], seed : tp . Union [ jnp . ndarray , int , None ] = None , ) -> tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ]: \"\"\" Returns predictions for a single batch of samples. Arguments: x: Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. Returns: Jax array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. \"\"\" self . _maybe_initialize ( x = x , y = None , sample_weight = None , class_weight = None , mode = Mode . predict , ) predict_fn = self . _predict if self . run_eagerly else self . _predict_jit y_pred , _ = predict_fn ( x = x , params = self . params , state = self . state , net_rng = next ( self . _rngs ), is_training = False , ) return y_pred save ( self , path , include_optimizer = True ) Saves the model to disk. It creates a directory that includes: The Model object instance serialized with pickle as as {path}/model.pkl , this allows you to re-instantiate the model later. The model parameters + states serialized into HDF5 as {path}/parameters.h5 . The state of the optimizer serialized with pickle as as {path}/optimizer_state.pkl , allowing to resume training exactly where you left off. We hope to use HDF5 in the future but optix state is incompatible with deepdish . This allows you to save the entirety of the state of a model in a directory structure which can be fully restored via Model.load if the model is already instiated or elegy.model.load to load the model instance from its pickled version. import elegy model . save ( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load ( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path where model structure will be saved. required include_optimizer bool If True, save optimizer's state together. True Source code in elegy/model.py 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 def save ( self , path : tp . Union [ str , Path ], include_optimizer : bool = True ) -> None : \"\"\" Saves the model to disk. It creates a directory that includes: - The `Model` object instance serialized with `pickle` as as `{path}/model.pkl`, this allows you to re-instantiate the model later. - The model parameters + states serialized into HDF5 as `{path}/parameters.h5`. - The state of the optimizer serialized with `pickle` as as `{path}/optimizer_state.pkl`, allowing to resume training exactly where you left off. We hope to use HDF5 in the future but `optix` state is incompatible with `deepdish`. This allows you to save the entirety of the state of a model in a directory structure which can be fully restored via `Model.load` if the model is already instiated or `elegy.model.load` to load the model instance from its pickled version. ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path where model structure will be saved. include_optimizer: If True, save optimizer's state together. \"\"\" if isinstance ( path , str ): path = Path ( path ) path . mkdir ( parents = True , exist_ok = True ) state = self . full_state original_state = copy ( state ) state . pop ( \"metrics_state\" , None ) state . pop ( \"initial_metrics_state\" , None ) optimizer_state = state . pop ( \"optimizer_state\" , None ) deepdish . io . save ( path / \"parameters.h5\" , state ) if include_optimizer and optimizer_state is not None : with open ( path / \"optimizer_state.pkl\" , \"wb\" ) as f : pickle . dump ( optimizer_state , f ) # getting pickle errors self . _clear_state () try : path = path / \"model.pkl\" with open ( path , \"wb\" ) as f : cloudpickle . dump ( self , f ) except BaseException as e : print ( f \"Error occurred saving the model object at { path } \\n Continuing....\" ) self . full_state = original_state test_on_batch ( self , x , y = None , sample_weight = None , class_weight = None ) Test the model on a single batch of samples. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). None sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None Returns: Type Description Dict[str, jax.numpy.lax_numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model.py 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 def test_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ jnp . ndarray ] = None , class_weight : tp . Optional [ jnp . ndarray ] = None , ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . _maybe_initialize ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , mode = Mode . test , ) test_fn = self . _test if self . run_eagerly else self . _test_jit ( logs , self . metrics_state ,) = test_fn ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , params = self . params , state = self . state , optimizer_state = self . optimizer_state , metrics_state = self . metrics_state , net_rng = next ( self . _rngs ), metrics_rng = next ( self . _rngs ), ) return { key : np . asarray ( value ) for key , value in logs . items ()} train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None ) Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). It should be consistent with x (you cannot have Numpy inputs and array targets, or inversely). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). None class_weight Optional[numpy.ndarray] Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None Returns: Type Description Dict[str, numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 def train_on_batch ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ np . ndarray ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\" Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). It should be consistent with `x` (you cannot have Numpy inputs and array targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . _maybe_initialize ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , mode = Mode . train , ) update_fn = self . _update if self . run_eagerly else self . _update_jit ( logs , self . params , self . state , self . optimizer_state , self . metrics_state , ) = update_fn ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , params = self . params , state = self . state , optimizer_state = self . optimizer_state , metrics_state = self . metrics_state , net_rng = next ( self . _rngs ), metrics_rng = next ( self . _rngs ), ) return { key : np . asarray ( value ) for key , value in logs . items ()}","title":"Model"},{"location":"api/Model/#elegymodel","text":"","title":"elegy.Model"},{"location":"api/Model/#elegy.model.Model","text":"Model is tasked with performing training, evaluation, and inference for a given elegy.Module or haiku.Module . To create a Model you first have to define its architecture in a Module : class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) Then you can pass this Module to the Model 's constructor and specify additional things like losses, metrics, optimizer, and callbacks: model = elegy . Model ( module = MLP . defer (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy . defer (), optimizer = optix . rmsprop ( 1e-3 ), ) Once the model is created, you can train the model with model.fit() , or use the model to do prediction with model.predict() . Checkout Getting Started for additional details. Attributes: Name Type Description params Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.Params structure with the weights of the model. state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.State structure with non-trainable parameters of the model. optimizer_state Optional[NamedTuple] A optix.OptState structure with state of the optimizer. metrics_state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.State structure with the state of the metrics. initial_metrics_state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.State structure with the initial state of the metrics. run_eagerly bool Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code, instead of using Jax's jit to optimize the computation. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls.","title":"elegy.model.Model"},{"location":"api/Model/#elegy.model.Model.seed","text":"Current random state of the model.","title":"seed"},{"location":"api/Model/#elegy.model.Model.__init__","text":"[summary] Parameters: Name Type Description Default module Callable A 0-argument function that returns a Haiku or Elegy Module instance. required loss Optional[Union[Callable, List, Dict]] A elegy.Loss or Callable instance representing the loss function of the network. You can define more loss terms by simply passing a possibly nested structure of lists and dictionaries of elegy.Loss or Callable s. Usually a plain list of losses is enough but using dictionaries will create namescopes for the names of the losses which might be useful e.g. to group things in tensorboard. Contrary to Keras convention, in Elegy there is no relation between the structure of loss with the structure of the labels and outputs of the network. Elegy's loss system is more flexible than the one provided by Keras, for more information on how to mimick Keras behavior checkout the Losses and Metrics Guide `. None metrics Optional[Union[Callable, List, Dict]] A elegy.Metric or Callable instance representing the loss function of the network. You can define more metrics terms by simply passing a possibly nested structure of lists and dictionaries of elegy.Metric or Callable s. Usually a plain list of metrics is enough but using dictionaries will create namescopes for the names of the metrics which might be useful e.g. to group things in tensorboard. Contrary to Keras convention, in Elegy there is no relation between the structure of metrics with the structure of the labels and outputs of the network. Elegy's metrics system is more flexible than the one provided by Keras, for more information on how to mimick Keras behavior checkout the Losses and Metrics Guide `. None optimizer Optional[jax.experimental.optix.GradientTransformation] A optix optimizer instance. Optix is a very flexible library for defining optimization pipelines with things like learning rate schedules, this means that there is no need for a LearningRateScheduler callback in Elegy. None run_eagerly bool Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code, instead of using Jax's jit to. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. False params Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.Params structure with the weights of the model. None state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.State structure with non-trainable parameters of the model. None optimizer_state Optional[NamedTuple] A optix.OptState structure with state of the optimizer. None metrics_state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.State structure with the state of the metrics. None initial_metrics_state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] A haiku.State structure with the initial state of the metrics. None seed Union[numpy.ndarray, int] The initial random state of the model. 42 Source code in elegy/model.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def __init__ ( self , module : tp . Callable , loss : tp . Union [ tp . Callable , tp . List , tp . Dict , None ] = None , metrics : tp . Union [ tp . Callable , tp . List , tp . Dict , None ] = None , optimizer : tp . Optional [ optix . GradientTransformation ] = None , run_eagerly : bool = False , params : tp . Optional [ hk . Params ] = None , state : tp . Optional [ hk . State ] = None , optimizer_state : tp . Optional [ optix . OptState ] = None , metrics_state : tp . Optional [ hk . State ] = None , initial_metrics_state : tp . Optional [ hk . State ] = None , seed : tp . Union [ np . ndarray , int ] = 42 , ): \"\"\"[summary] Arguments: module: A 0-argument function that returns a Haiku or Elegy `Module` instance. loss: A `elegy.Loss` or `Callable` instance representing the loss function of the network. You can define more loss terms by simply passing a possibly nested structure of lists and dictionaries of `elegy.Loss` or `Callable`s. Usually a plain list of losses is enough but using dictionaries will create namescopes for the names of the losses which might be useful e.g. to group things in tensorboard. Contrary to Keras convention, in Elegy there is no relation between the structure of `loss` with the structure of the labels and outputs of the network. Elegy's loss system is more flexible than the one provided by Keras, for more information on how to mimick Keras behavior checkout the [Losses and Metrics Guide](https://poets-ai.github.io/elegy/guides/losses-and-metrics)`. metrics: A `elegy.Metric` or `Callable` instance representing the loss function of the network. You can define more metrics terms by simply passing a possibly nested structure of lists and dictionaries of `elegy.Metric` or `Callable`s. Usually a plain list of metrics is enough but using dictionaries will create namescopes for the names of the metrics which might be useful e.g. to group things in tensorboard. Contrary to Keras convention, in Elegy there is no relation between the structure of `metrics` with the structure of the labels and outputs of the network. Elegy's metrics system is more flexible than the one provided by Keras, for more information on how to mimick Keras behavior checkout the [Losses and Metrics Guide](https://poets-ai.github.io/elegy/guides/losses-and-metrics)`. optimizer: A `optix` optimizer instance. Optix is a very flexible library for defining optimization pipelines with things like learning rate schedules, this means that there is no need for a `LearningRateScheduler` callback in Elegy. run_eagerly: Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code, instead of using Jax's `jit` to. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. params: A `haiku.Params` structure with the weights of the model. state: A `haiku.State` structure with non-trainable parameters of the model. optimizer_state: A `optix.OptState` structure with state of the optimizer. metrics_state: A `haiku.State` structure with the state of the metrics. initial_metrics_state: A `haiku.State` structure with the initial state of the metrics. seed: The initial random state of the model. \"\"\" if metrics is not None : metrics = metric_modes . forward_all ( metrics ) if loss is None : def loss_ ( y_true , y_pred ): return 0.0 loss = loss_ loss = loss_modes . forward_all ( loss ) def model_fn ( * args , ** kwargs ): module = self . _module_fn () return utils . inject_dependencies ( module )( * args , ** kwargs ) self . _module_fn = module self . _model_transform = hk . transform_with_state ( model_fn ) self . _loss_fn = utils . inject_dependencies ( loss ) self . _metrics_transform = ( hk . transform_with_state ( utils . inject_dependencies ( metrics , rename = { \"__params\" : \"params\" , \"__state\" : \"state\" } ) ) if metrics else None ) self . _optimizer = optimizer if optimizer is not None else optix . adam ( 1e-3 ) self . _rngs = hk . PRNGSequence ( seed ) self . params = params self . state = state self . optimizer_state = optimizer_state self . metrics_state = metrics_state self . initial_metrics_state = initial_metrics_state self . run_eagerly = run_eagerly","title":"__init__()"},{"location":"api/Model/#elegy.model.Model.evaluate","text":"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required y Optional[Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 1 batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None steps Optional[int] Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . This argument is not supported with array inputs. None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.Model.fit]. Returns: Type Description Dict[str, numpy.ndarray] A dictionary for mapping the losses and metrics names to the values obtained. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model.py 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 def evaluate ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], y : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. This argument is not supported with array inputs. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.Model.fit]. Returns: A dictionary for mapping the losses and metrics names to the values obtained. Raises: ValueError: in case of invalid arguments. \"\"\" data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , is_training = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . test_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs","title":"evaluate()"},{"location":"api/Model/#elegy.model.Model.fit","text":"Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for generator type is given below. required y Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator. 0.0 validation_data Optional[Union[Tuple, Iterable]] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy/Jax arrays, list of arrays or mappings tuple (x_val, y_val, val_sample_weights) of Numpy/Jax arrays, list of arrays or mappings generator For the first two cases, batch_size must be provided. For the last case, validation_steps should be provided, and should follow the same convention for yielding data as x . Note that validation_data does not support all the data types that are supported in x , eg, dict. None shuffle bool Boolean (whether to shuffle the training data before each epoch). This argument is ignored when x is a generator. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of generators (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description History A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model.py 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 def fit ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], y : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ np . ndarray ] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , ) -> History : \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for generator type is given below. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a generator. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy/Jax arrays, list of arrays or mappings - tuple `(x_val, y_val, val_sample_weights)` of Numpy/Jax arrays, list of arrays or mappings - generator For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` should be provided, and should follow the same convention for yielding data as `x`. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict. shuffle: Boolean (whether to shuffle the training data before each epoch). This argument is ignored when `x` is a generator. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of generators (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) # sample_weight = batch[2] if len(batch) == 3 else None x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . train_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) epoch_logs = copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) # print( # f\"epoch: {epoch} - \" # + \" - \".join(f\"{key}: {value:.3f}\" for key, value in epoch_logs.items()) # ) if self . stop_training : break callbacks . on_train_end () return self . history","title":"fit()"},{"location":"api/Model/#elegy.model.Model.load","text":"Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the haiku.Params + haiku.State structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Source code in elegy/model.py 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 def load ( self , path : tp . Union [ str , Path ]) -> None : \"\"\" Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the `haiku.Params` + `haiku.State` structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Arguments: path: path to a saved model's directory. \"\"\" if isinstance ( path , str ): path = Path ( path ) state : tp . Dict = deepdish . io . load ( path / \"parameters.h5\" ) optimizer_state_path = path / \"optimizer_state.pkl\" if optimizer_state_path . exists (): with open ( optimizer_state_path , \"rb\" ) as f : state [ \"optimizer_state\" ] = pickle . load ( f ) self . full_state = state","title":"load()"},{"location":"api/Model/#elegy.model.Model.predict","text":"Generates output predictions for the input samples. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generators (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.Model.fit]. Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description ndarray Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model.py 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 def predict ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> np . ndarray : \"\"\"Generates output predictions for the input samples. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generators (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.Model.fit]. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs ,) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs","title":"predict()"},{"location":"api/Model/#elegy.model.Model.predict_on_batch","text":"Returns predictions for a single batch of samples. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. required Returns: Type Description Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Jax array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between given number of inputs and expectations of the model. Source code in elegy/model.py 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 def predict_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], seed : tp . Union [ jnp . ndarray , int , None ] = None , ) -> tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ]: \"\"\" Returns predictions for a single batch of samples. Arguments: x: Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. Returns: Jax array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. \"\"\" self . _maybe_initialize ( x = x , y = None , sample_weight = None , class_weight = None , mode = Mode . predict , ) predict_fn = self . _predict if self . run_eagerly else self . _predict_jit y_pred , _ = predict_fn ( x = x , params = self . params , state = self . state , net_rng = next ( self . _rngs ), is_training = False , ) return y_pred","title":"predict_on_batch()"},{"location":"api/Model/#elegy.model.Model.save","text":"Saves the model to disk. It creates a directory that includes: The Model object instance serialized with pickle as as {path}/model.pkl , this allows you to re-instantiate the model later. The model parameters + states serialized into HDF5 as {path}/parameters.h5 . The state of the optimizer serialized with pickle as as {path}/optimizer_state.pkl , allowing to resume training exactly where you left off. We hope to use HDF5 in the future but optix state is incompatible with deepdish . This allows you to save the entirety of the state of a model in a directory structure which can be fully restored via Model.load if the model is already instiated or elegy.model.load to load the model instance from its pickled version. import elegy model . save ( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load ( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path where model structure will be saved. required include_optimizer bool If True, save optimizer's state together. True Source code in elegy/model.py 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 def save ( self , path : tp . Union [ str , Path ], include_optimizer : bool = True ) -> None : \"\"\" Saves the model to disk. It creates a directory that includes: - The `Model` object instance serialized with `pickle` as as `{path}/model.pkl`, this allows you to re-instantiate the model later. - The model parameters + states serialized into HDF5 as `{path}/parameters.h5`. - The state of the optimizer serialized with `pickle` as as `{path}/optimizer_state.pkl`, allowing to resume training exactly where you left off. We hope to use HDF5 in the future but `optix` state is incompatible with `deepdish`. This allows you to save the entirety of the state of a model in a directory structure which can be fully restored via `Model.load` if the model is already instiated or `elegy.model.load` to load the model instance from its pickled version. ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path where model structure will be saved. include_optimizer: If True, save optimizer's state together. \"\"\" if isinstance ( path , str ): path = Path ( path ) path . mkdir ( parents = True , exist_ok = True ) state = self . full_state original_state = copy ( state ) state . pop ( \"metrics_state\" , None ) state . pop ( \"initial_metrics_state\" , None ) optimizer_state = state . pop ( \"optimizer_state\" , None ) deepdish . io . save ( path / \"parameters.h5\" , state ) if include_optimizer and optimizer_state is not None : with open ( path / \"optimizer_state.pkl\" , \"wb\" ) as f : pickle . dump ( optimizer_state , f ) # getting pickle errors self . _clear_state () try : path = path / \"model.pkl\" with open ( path , \"wb\" ) as f : cloudpickle . dump ( self , f ) except BaseException as e : print ( f \"Error occurred saving the model object at { path } \\n Continuing....\" ) self . full_state = original_state","title":"save()"},{"location":"api/Model/#elegy.model.Model.test_on_batch","text":"Test the model on a single batch of samples. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). None sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None Returns: Type Description Dict[str, jax.numpy.lax_numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model.py 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 def test_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ jnp . ndarray ] = None , class_weight : tp . Optional [ jnp . ndarray ] = None , ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . _maybe_initialize ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , mode = Mode . test , ) test_fn = self . _test if self . run_eagerly else self . _test_jit ( logs , self . metrics_state ,) = test_fn ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , params = self . params , state = self . state , optimizer_state = self . optimizer_state , metrics_state = self . metrics_state , net_rng = next ( self . _rngs ), metrics_rng = next ( self . _rngs ), ) return { key : np . asarray ( value ) for key , value in logs . items ()}","title":"test_on_batch()"},{"location":"api/Model/#elegy.model.Model.train_on_batch","text":"Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). It should be consistent with x (you cannot have Numpy inputs and array targets, or inversely). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). None class_weight Optional[numpy.ndarray] Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None Returns: Type Description Dict[str, numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 def train_on_batch ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ np . ndarray ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\" Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). It should be consistent with `x` (you cannot have Numpy inputs and array targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . _maybe_initialize ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , mode = Mode . train , ) update_fn = self . _update if self . run_eagerly else self . _update_jit ( logs , self . params , self . state , self . optimizer_state , self . metrics_state , ) = update_fn ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , params = self . params , state = self . state , optimizer_state = self . optimizer_state , metrics_state = self . metrics_state , net_rng = next ( self . _rngs ), metrics_rng = next ( self . _rngs ), ) return { key : np . asarray ( value ) for key , value in logs . items ()}","title":"train_on_batch()"},{"location":"api/Module/","text":"elegy.module.Module Basic Elegy Module","title":"elegy.module.Module"},{"location":"api/Module/#elegymodulemodule","text":"","title":"elegy.module.Module"},{"location":"api/Module/#elegy.module.Module","text":"Basic Elegy Module","title":"elegy.module.Module"},{"location":"api/callbacks/CSVLogger/","text":"elegy.callbacks.CSVLogger Callback that streams epoch results to a csv file. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . Examples: csv_logger = CSVLogger ( 'training.log' ) model . fit ( X_train , Y_train , callbacks = [ csv_logger ]) __init__ ( self , filename , separator = ',' , append = False ) special Parameters: Name Type Description Default filename str filename of the csv file, e.g. 'run/log.csv'. required separator str string used to separate elements in the csv file. ',' append bool True: append if file exists (useful for continuing training). False: overwrite existing file, False Source code in elegy/callbacks/csv_logger.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , filename : str , separator : str = \",\" , append : bool = False ): \"\"\" Arguments: filename: filename of the csv file, e.g. 'run/log.csv'. separator: string used to separate elements in the csv file. append: True: append if file exists (useful for continuing training). False: overwrite existing file, \"\"\" self . sep = separator self . filename = filename self . append = append self . writer = None self . keys = None self . append_header = True self . file_flags = \"\" self . _open_args = { \"newline\" : \" \\n \" } super ( CSVLogger , self ) . __init__ ()","title":"CSVLogger"},{"location":"api/callbacks/CSVLogger/#elegycallbackscsvlogger","text":"","title":"elegy.callbacks.CSVLogger"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger","text":"Callback that streams epoch results to a csv file. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . Examples: csv_logger = CSVLogger ( 'training.log' ) model . fit ( X_train , Y_train , callbacks = [ csv_logger ])","title":"elegy.callbacks.csv_logger.CSVLogger"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.__init__","text":"Parameters: Name Type Description Default filename str filename of the csv file, e.g. 'run/log.csv'. required separator str string used to separate elements in the csv file. ',' append bool True: append if file exists (useful for continuing training). False: overwrite existing file, False Source code in elegy/callbacks/csv_logger.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , filename : str , separator : str = \",\" , append : bool = False ): \"\"\" Arguments: filename: filename of the csv file, e.g. 'run/log.csv'. separator: string used to separate elements in the csv file. append: True: append if file exists (useful for continuing training). False: overwrite existing file, \"\"\" self . sep = separator self . filename = filename self . append = append self . writer = None self . keys = None self . append_header = True self . file_flags = \"\" self . _open_args = { \"newline\" : \" \\n \" } super ( CSVLogger , self ) . __init__ ()","title":"__init__()"},{"location":"api/callbacks/Callback/","text":"elegy.callbacks.Callback Abstract base class used to build new callbacks. The logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch. Currently, the .fit() method of the Model class will include the following quantities in the logs that it passes to its callbacks: on_epoch_end : logs include ` acc ` and ` loss ` , and optionally include ` val_loss ` ( if validation is enabled in ` fit ` ), and ` val_acc ` ( if validation and accuracy monitoring are enabled ) . on_train_batch_begin : logs include ` size ` , the number of samples in the current batch . on_train_batch_end : logs include ` loss ` , and optionally ` acc ` ( if accuracy monitoring is enabled ) . Attributes: Name Type Description params dict Training parameters (eg. verbosity, batch size, number of epochs...). model elegy.model.Model Reference of the model being trained. on_epoch_begin ( self , epoch , logs = None ) Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/callback.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass on_predict_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 178 179 180 181 182 183 184 185 186 187 188 189 190 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 241 242 243 244 245 246 247 248 249 250 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 253 254 255 256 257 258 259 260 261 262 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 217 218 219 220 221 222 223 224 225 226 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 229 230 231 232 233 234 235 236 237 238 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 110 111 112 113 114 115 116 117 118 119 120 121 122 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 193 194 195 196 197 198 199 200 201 202 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 205 206 207 208 209 210 211 212 213 214 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"Callback"},{"location":"api/callbacks/Callback/#elegycallbackscallback","text":"","title":"elegy.callbacks.Callback"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback","text":"Abstract base class used to build new callbacks. The logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch. Currently, the .fit() method of the Model class will include the following quantities in the logs that it passes to its callbacks: on_epoch_end : logs include ` acc ` and ` loss ` , and optionally include ` val_loss ` ( if validation is enabled in ` fit ` ), and ` val_acc ` ( if validation and accuracy monitoring are enabled ) . on_train_batch_begin : logs include ` size ` , the number of samples in the current batch . on_train_batch_end : logs include ` loss ` , and optionally ` acc ` ( if accuracy monitoring is enabled ) . Attributes: Name Type Description params dict Training parameters (eg. verbosity, batch size, number of epochs...). model elegy.model.Model Reference of the model being trained.","title":"elegy.callbacks.callback.Callback"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/callback.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass","title":"on_epoch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 178 179 180 181 182 183 184 185 186 187 188 189 190 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 241 242 243 244 245 246 247 248 249 250 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 253 254 255 256 257 258 259 260 261 262 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 217 218 219 220 221 222 223 224 225 226 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 229 230 231 232 233 234 235 236 237 238 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 110 111 112 113 114 115 116 117 118 119 120 121 122 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 193 194 195 196 197 198 199 200 201 202 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 205 206 207 208 209 210 211 212 213 214 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/EarlyStopping/","text":"elegy.callbacks.EarlyStopping Stop training when a monitored metric has stopped improving. Assuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates. The quantity to be monitored needs to be available in logs dict. To make it so, pass the loss or metrics at model.__init__() . Examples: np . random . seed ( 42 ) class MLP ( elegy . Module ): def call ( self , input ): mlp = hk . Sequential ([ hk . Linear ( 10 ),]) return mlp ( input ) callback = elegy . callbacks . EarlyStopping ( monitor = \"loss\" , patience = 3 ) # This callback will stop the training when there is no improvement in # the for three consecutive epochs. model = elegy . Model ( module = MLP . defer (), loss = elegy . losses . MeanSquaredError (), optimizer = optix . rmsprop ( 0.01 ), ) history = model . fit ( np . arange ( 100 ) . reshape ( 5 , 20 ) . astype ( np . float32 ), np . zeros ( 5 ), epochs = 10 , batch_size = 1 , callbacks = [ callback ], verbose = 0 , ) assert len ( history . history [ \"loss\" ]) == 7 # Only 7 epochs are run. __init__ ( self , monitor = 'val_loss' , min_delta = 0 , patience = 0 , verbose = 0 , mode = 'auto' , baseline = None , restore_best_weights = False ) special Initialize an EarlyStopping callback. Parameters: Name Type Description Default monitor str Quantity to be monitored. 'val_loss' min_delta int Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. 0 patience int Number of epochs with no improvement after which training will be stopped. 0 verbose int verbosity mode. 0 mode str One of {\"auto\", \"min\", \"max\"} . In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. 'auto' baseline Optional[float] Baseline value for the monitored quantity. Training will stop if the model doesn't show improvement over the baseline. None restore_best_weights bool Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used. False Source code in elegy/callbacks/early_stopping.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def __init__ ( self , monitor : str = \"val_loss\" , min_delta : int = 0 , patience : int = 0 , verbose : int = 0 , mode : str = \"auto\" , baseline : tp . Optional [ float ] = None , restore_best_weights : bool = False , ): \"\"\"Initialize an EarlyStopping callback. Arguments: monitor: Quantity to be monitored. min_delta: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. patience: Number of epochs with no improvement after which training will be stopped. verbose: verbosity mode. mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode, training will stop when the quantity monitored has stopped decreasing; in `max` mode it will stop when the quantity monitored has stopped increasing; in `auto` mode, the direction is automatically inferred from the name of the monitored quantity. baseline: Baseline value for the monitored quantity. Training will stop if the model doesn't show improvement over the baseline. restore_best_weights: Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used. \"\"\" super ( EarlyStopping , self ) . __init__ () self . monitor = monitor self . patience = patience self . verbose = verbose self . baseline = baseline self . min_delta = abs ( min_delta ) self . wait = 0 self . stopped_epoch = 0 self . restore_best_weights = restore_best_weights self . best_weights = None if mode not in [ \"auto\" , \"min\" , \"max\" ]: logging . warning ( \"EarlyStopping mode %s is unknown, \" \"fallback to auto mode.\" , mode ) mode = \"auto\" if mode == \"min\" : self . monitor_op = np . less elif mode == \"max\" : self . monitor_op = np . greater else : if \"acc\" in self . monitor : self . monitor_op = np . greater else : self . monitor_op = np . less if self . monitor_op == np . greater : self . min_delta *= 1 else : self . min_delta *= - 1","title":"EarlyStopping"},{"location":"api/callbacks/EarlyStopping/#elegycallbacksearlystopping","text":"","title":"elegy.callbacks.EarlyStopping"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping","text":"Stop training when a monitored metric has stopped improving. Assuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates. The quantity to be monitored needs to be available in logs dict. To make it so, pass the loss or metrics at model.__init__() . Examples: np . random . seed ( 42 ) class MLP ( elegy . Module ): def call ( self , input ): mlp = hk . Sequential ([ hk . Linear ( 10 ),]) return mlp ( input ) callback = elegy . callbacks . EarlyStopping ( monitor = \"loss\" , patience = 3 ) # This callback will stop the training when there is no improvement in # the for three consecutive epochs. model = elegy . Model ( module = MLP . defer (), loss = elegy . losses . MeanSquaredError (), optimizer = optix . rmsprop ( 0.01 ), ) history = model . fit ( np . arange ( 100 ) . reshape ( 5 , 20 ) . astype ( np . float32 ), np . zeros ( 5 ), epochs = 10 , batch_size = 1 , callbacks = [ callback ], verbose = 0 , ) assert len ( history . history [ \"loss\" ]) == 7 # Only 7 epochs are run.","title":"elegy.callbacks.early_stopping.EarlyStopping"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.__init__","text":"Initialize an EarlyStopping callback. Parameters: Name Type Description Default monitor str Quantity to be monitored. 'val_loss' min_delta int Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. 0 patience int Number of epochs with no improvement after which training will be stopped. 0 verbose int verbosity mode. 0 mode str One of {\"auto\", \"min\", \"max\"} . In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. 'auto' baseline Optional[float] Baseline value for the monitored quantity. Training will stop if the model doesn't show improvement over the baseline. None restore_best_weights bool Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used. False Source code in elegy/callbacks/early_stopping.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def __init__ ( self , monitor : str = \"val_loss\" , min_delta : int = 0 , patience : int = 0 , verbose : int = 0 , mode : str = \"auto\" , baseline : tp . Optional [ float ] = None , restore_best_weights : bool = False , ): \"\"\"Initialize an EarlyStopping callback. Arguments: monitor: Quantity to be monitored. min_delta: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. patience: Number of epochs with no improvement after which training will be stopped. verbose: verbosity mode. mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode, training will stop when the quantity monitored has stopped decreasing; in `max` mode it will stop when the quantity monitored has stopped increasing; in `auto` mode, the direction is automatically inferred from the name of the monitored quantity. baseline: Baseline value for the monitored quantity. Training will stop if the model doesn't show improvement over the baseline. restore_best_weights: Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used. \"\"\" super ( EarlyStopping , self ) . __init__ () self . monitor = monitor self . patience = patience self . verbose = verbose self . baseline = baseline self . min_delta = abs ( min_delta ) self . wait = 0 self . stopped_epoch = 0 self . restore_best_weights = restore_best_weights self . best_weights = None if mode not in [ \"auto\" , \"min\" , \"max\" ]: logging . warning ( \"EarlyStopping mode %s is unknown, \" \"fallback to auto mode.\" , mode ) mode = \"auto\" if mode == \"min\" : self . monitor_op = np . less elif mode == \"max\" : self . monitor_op = np . greater else : if \"acc\" in self . monitor : self . monitor_op = np . greater else : self . monitor_op = np . less if self . monitor_op == np . greater : self . min_delta *= 1 else : self . min_delta *= - 1","title":"__init__()"},{"location":"api/callbacks/LambdaCallback/","text":"elegy.callbacks.LambdaCallback Callback for creating simple, custom callbacks on-the-fly. This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as: on_epoch_begin and on_epoch_end expect two positional arguments: epoch , logs on_train_batch_begin and on_train_batch_end expect two positional arguments: batch , logs on_train_begin and on_train_end expect one positional argument: logs Examples: # Print the batch number at the beginning of every batch. batch_print_callback = LambdaCallback ( on_train_batch_begin = lambda batch , logs : print ( batch )) # Stream the epoch loss to a file in JSON format. The file content # is not well-formed JSON but rather has a JSON object per line. import json json_log = open ( 'loss_log.json' , mode = 'wt' , buffering = 1 ) json_logging_callback = LambdaCallback ( on_epoch_end = lambda epoch , logs : json_log . write ( json . dumps ({ 'epoch' : epoch , 'loss' : logs [ 'loss' ]}) + ' \\n ' ), on_train_end = lambda logs : json_log . close () ) # Terminate some processes after having finished model training. processes = ... cleanup_callback = LambdaCallback ( on_train_end = lambda logs : [ p . terminate () for p in processes if p . is_alive ()]) model . fit ( ... , callbacks = [ batch_print_callback , json_logging_callback , cleanup_callback ]) __init__ ( self , on_epoch_begin = None , on_epoch_end = None , on_train_batch_begin = None , on_train_batch_end = None , on_train_begin = None , on_train_end = None , ** kwargs ) special Parameters: Name Type Description Default on_epoch_begin Optional[Callable[[int, Dict[str, numpy.ndarray]], NoneType]] called at the beginning of every epoch. None on_epoch_end Optional[Callable[[int, Dict[str, numpy.ndarray]], NoneType]] called at the end of every epoch. None on_train_batch_begin Optional[Callable[[int, Dict[str, numpy.ndarray]], NoneType]] called at the beginning of every batch. None on_train_batch_end Optional[Callable[[int, Dict[str, numpy.ndarray]], NoneType]] called at the end of every batch. None on_train_begin Optional[Callable[[Dict[str, numpy.ndarray]], NoneType]] called at the beginning of model training. None on_train_end Optional[Callable[[Dict[str, numpy.ndarray]], NoneType]] called at the end of model training. None Source code in elegy/callbacks/lambda_callback.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def __init__ ( self , on_epoch_begin : tp . Optional [ tp . Callable [[ int , tp . Dict [ str , np . ndarray ]], None ] ] = None , on_epoch_end : tp . Optional [ tp . Callable [[ int , tp . Dict [ str , np . ndarray ]], None ] ] = None , on_train_batch_begin : tp . Optional [ tp . Callable [[ int , tp . Dict [ str , np . ndarray ]], None ] ] = None , on_train_batch_end : tp . Optional [ tp . Callable [[ int , tp . Dict [ str , np . ndarray ]], None ] ] = None , on_train_begin : tp . Optional [ tp . Callable [[ tp . Dict [ str , np . ndarray ]], None ] ] = None , on_train_end : tp . Optional [ tp . Callable [[ tp . Dict [ str , np . ndarray ]], None ]] = None , ** kwargs ): \"\"\" Arguments: on_epoch_begin: called at the beginning of every epoch. on_epoch_end: called at the end of every epoch. on_train_batch_begin: called at the beginning of every batch. on_train_batch_end: called at the end of every batch. on_train_begin: called at the beginning of model training. on_train_end: called at the end of model training. \"\"\" super ( LambdaCallback , self ) . __init__ () self . __dict__ . update ( kwargs ) if on_epoch_begin is not None : self . on_epoch_begin = on_epoch_begin else : self . on_epoch_begin = lambda epoch , logs : None if on_epoch_end is not None : self . on_epoch_end = on_epoch_end else : self . on_epoch_end = lambda epoch , logs : None if on_train_batch_begin is not None : self . on_train_batch_begin = on_train_batch_begin else : self . on_train_batch_begin = lambda batch , logs : None if on_train_batch_end is not None : self . on_train_batch_end = on_train_batch_end else : self . on_train_batch_end = lambda batch , logs : None if on_train_begin is not None : self . on_train_begin = on_train_begin else : self . on_train_begin = lambda logs : None if on_train_end is not None : self . on_train_end = on_train_end else : self . on_train_end = lambda logs : None","title":"LambdaCallback"},{"location":"api/callbacks/LambdaCallback/#elegycallbackslambdacallback","text":"","title":"elegy.callbacks.LambdaCallback"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback","text":"Callback for creating simple, custom callbacks on-the-fly. This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as: on_epoch_begin and on_epoch_end expect two positional arguments: epoch , logs on_train_batch_begin and on_train_batch_end expect two positional arguments: batch , logs on_train_begin and on_train_end expect one positional argument: logs Examples: # Print the batch number at the beginning of every batch. batch_print_callback = LambdaCallback ( on_train_batch_begin = lambda batch , logs : print ( batch )) # Stream the epoch loss to a file in JSON format. The file content # is not well-formed JSON but rather has a JSON object per line. import json json_log = open ( 'loss_log.json' , mode = 'wt' , buffering = 1 ) json_logging_callback = LambdaCallback ( on_epoch_end = lambda epoch , logs : json_log . write ( json . dumps ({ 'epoch' : epoch , 'loss' : logs [ 'loss' ]}) + ' \\n ' ), on_train_end = lambda logs : json_log . close () ) # Terminate some processes after having finished model training. processes = ... cleanup_callback = LambdaCallback ( on_train_end = lambda logs : [ p . terminate () for p in processes if p . is_alive ()]) model . fit ( ... , callbacks = [ batch_print_callback , json_logging_callback , cleanup_callback ])","title":"elegy.callbacks.lambda_callback.LambdaCallback"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.__init__","text":"Parameters: Name Type Description Default on_epoch_begin Optional[Callable[[int, Dict[str, numpy.ndarray]], NoneType]] called at the beginning of every epoch. None on_epoch_end Optional[Callable[[int, Dict[str, numpy.ndarray]], NoneType]] called at the end of every epoch. None on_train_batch_begin Optional[Callable[[int, Dict[str, numpy.ndarray]], NoneType]] called at the beginning of every batch. None on_train_batch_end Optional[Callable[[int, Dict[str, numpy.ndarray]], NoneType]] called at the end of every batch. None on_train_begin Optional[Callable[[Dict[str, numpy.ndarray]], NoneType]] called at the beginning of model training. None on_train_end Optional[Callable[[Dict[str, numpy.ndarray]], NoneType]] called at the end of model training. None Source code in elegy/callbacks/lambda_callback.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def __init__ ( self , on_epoch_begin : tp . Optional [ tp . Callable [[ int , tp . Dict [ str , np . ndarray ]], None ] ] = None , on_epoch_end : tp . Optional [ tp . Callable [[ int , tp . Dict [ str , np . ndarray ]], None ] ] = None , on_train_batch_begin : tp . Optional [ tp . Callable [[ int , tp . Dict [ str , np . ndarray ]], None ] ] = None , on_train_batch_end : tp . Optional [ tp . Callable [[ int , tp . Dict [ str , np . ndarray ]], None ] ] = None , on_train_begin : tp . Optional [ tp . Callable [[ tp . Dict [ str , np . ndarray ]], None ] ] = None , on_train_end : tp . Optional [ tp . Callable [[ tp . Dict [ str , np . ndarray ]], None ]] = None , ** kwargs ): \"\"\" Arguments: on_epoch_begin: called at the beginning of every epoch. on_epoch_end: called at the end of every epoch. on_train_batch_begin: called at the beginning of every batch. on_train_batch_end: called at the end of every batch. on_train_begin: called at the beginning of model training. on_train_end: called at the end of model training. \"\"\" super ( LambdaCallback , self ) . __init__ () self . __dict__ . update ( kwargs ) if on_epoch_begin is not None : self . on_epoch_begin = on_epoch_begin else : self . on_epoch_begin = lambda epoch , logs : None if on_epoch_end is not None : self . on_epoch_end = on_epoch_end else : self . on_epoch_end = lambda epoch , logs : None if on_train_batch_begin is not None : self . on_train_batch_begin = on_train_batch_begin else : self . on_train_batch_begin = lambda batch , logs : None if on_train_batch_end is not None : self . on_train_batch_end = on_train_batch_end else : self . on_train_batch_end = lambda batch , logs : None if on_train_begin is not None : self . on_train_begin = on_train_begin else : self . on_train_begin = lambda logs : None if on_train_end is not None : self . on_train_end = on_train_end else : self . on_train_end = lambda logs : None","title":"__init__()"},{"location":"api/callbacks/ModelCheckpoint/","text":"elegy.callbacks.ModelCheckpoint Callback to save the Elegy model or model weights at some frequency. ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights at some interval, so the model or weights can be loaded later to continue the training from the state saved. A few options this callback provides include: Whether to only keep the model that has achieved the \"best performance\" so far, or whether to save the model at the end of every epoch regardless of performance. Definition of 'best'; which quantity to monitor and whether it should be maximized or minimized. The frequency it should save at. Currently, the callback supports saving at the end of every epoch, or after a fixed number of training batches. Examples: EPOCHS = 10 checkpoint_path = '/tmp/checkpoint' model_checkpoint_callback = elegy . callbacks . ModelCheckpoint ( path = checkpoint_path , monitor = 'val_acc' , mode = 'max' , save_best_only = True ) # Model is saved at the end of every epoch, if it's the best seen # so far. model . fit ( epochs = EPOCHS , callbacks = [ model_checkpoint_callback ]) # The model status (that are considered the best) are loaded into the model. model . load ( checkpoint_path ) __init__ ( self , path , monitor = 'val_loss' , verbose = 0 , save_best_only = False , mode = 'auto' , save_freq = 'epoch' , period = 1 ) special Parameters: Name Type Description Default path str string, path to directory to save the model state. path can contain named formatting options, which will be filled the value of epoch and keys in logs (passed in on_epoch_end ). For example: if path is weights.{epoch:02d}-{val_loss:.2f} , then the model checkpoints will be saved with the epoch number and the validation loss in the filename. required monitor str quantity to monitor. 'val_loss' verbose int verbosity mode, 0 or 1. 0 save_best_only bool if save_best_only=True , the latest best model according to the quantity monitored will not be overwritten. If path doesn't contain formatting options like {epoch} then path will be overwritten by each new better model. False mode str one of {auto, min, max}. If save_best_only=True , the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc , this should be max , for val_loss this should be min , etc. In auto mode, the direction is automatically inferred from the name of the monitored quantity. 'auto' save_freq str 'epoch' or integer. When using 'epoch' , the callback saves the model after each epoch. When using integer, the callback saves the model at end of this many batches. Note that if the saving isn't aligned to epochs, the monitored metric may potentially be less reliable (it could reflect as little as 1 batch, since the metrics get reset every epoch). Defaults to 'epoch' 'epoch' period int the number of epochs between which the model is saved. This only works if save_freq is 'epoch', otherwise the save_freq will override this period. 1 Source code in elegy/callbacks/model_checkpoint.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def __init__ ( self , path : str , monitor : str = \"val_loss\" , verbose : int = 0 , save_best_only : bool = False , mode : str = \"auto\" , save_freq : str = \"epoch\" , period : int = 1 , ): \"\"\" Arguments: path: string, path to directory to save the model state. `path` can contain named formatting options, which will be filled the value of `epoch` and keys in `logs` (passed in `on_epoch_end`). For example: if `path` is `weights.{epoch:02d}-{val_loss:.2f}`, then the model checkpoints will be saved with the epoch number and the validation loss in the filename. monitor: quantity to monitor. verbose: verbosity mode, 0 or 1. save_best_only: if `save_best_only=True`, the latest best model according to the quantity monitored will not be overwritten. If `path` doesn't contain formatting options like `{epoch}` then `path` will be overwritten by each new better model. mode: one of {auto, min, max}. If `save_best_only=True`, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For `val_acc`, this should be `max`, for `val_loss` this should be `min`, etc. In `auto` mode, the direction is automatically inferred from the name of the monitored quantity. save_freq: `'epoch'` or integer. When using `'epoch'`, the callback saves the model after each epoch. When using integer, the callback saves the model at end of this many batches. Note that if the saving isn't aligned to epochs, the monitored metric may potentially be less reliable (it could reflect as little as 1 batch, since the metrics get reset every epoch). Defaults to `'epoch'` period: the number of epochs between which the model is saved. This only works if `save_freq` is 'epoch', otherwise the `save_freq` will override this period. \"\"\" super ( ModelCheckpoint , self ) . __init__ () self . monitor = monitor self . verbose = verbose self . path = path self . save_best_only = save_best_only self . save_freq = save_freq self . period = period self . epochs_since_last_save = 0 self . _batches_seen_since_last_saving = 0 if mode not in [ \"auto\" , \"min\" , \"max\" ]: logging . warning ( \"ModelCheckpoint mode %s is unknown, \" \"fallback to auto mode.\" , mode ) mode = \"auto\" if mode == \"min\" : self . monitor_op = np . less self . best = np . Inf elif mode == \"max\" : self . monitor_op = np . greater self . best = - np . Inf else : if \"acc\" in self . monitor or self . monitor . startswith ( \"fmeasure\" ): self . monitor_op = np . greater self . best = - np . Inf else : self . monitor_op = np . less self . best = np . Inf if self . save_freq != \"epoch\" and not isinstance ( self . save_freq , int ): raise ValueError ( \"Unrecognized save_freq: {} \" . format ( self . save_freq ))","title":"ModelCheckpoint"},{"location":"api/callbacks/ModelCheckpoint/#elegycallbacksmodelcheckpoint","text":"","title":"elegy.callbacks.ModelCheckpoint"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint","text":"Callback to save the Elegy model or model weights at some frequency. ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights at some interval, so the model or weights can be loaded later to continue the training from the state saved. A few options this callback provides include: Whether to only keep the model that has achieved the \"best performance\" so far, or whether to save the model at the end of every epoch regardless of performance. Definition of 'best'; which quantity to monitor and whether it should be maximized or minimized. The frequency it should save at. Currently, the callback supports saving at the end of every epoch, or after a fixed number of training batches. Examples: EPOCHS = 10 checkpoint_path = '/tmp/checkpoint' model_checkpoint_callback = elegy . callbacks . ModelCheckpoint ( path = checkpoint_path , monitor = 'val_acc' , mode = 'max' , save_best_only = True ) # Model is saved at the end of every epoch, if it's the best seen # so far. model . fit ( epochs = EPOCHS , callbacks = [ model_checkpoint_callback ]) # The model status (that are considered the best) are loaded into the model. model . load ( checkpoint_path )","title":"elegy.callbacks.model_checkpoint.ModelCheckpoint"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.__init__","text":"Parameters: Name Type Description Default path str string, path to directory to save the model state. path can contain named formatting options, which will be filled the value of epoch and keys in logs (passed in on_epoch_end ). For example: if path is weights.{epoch:02d}-{val_loss:.2f} , then the model checkpoints will be saved with the epoch number and the validation loss in the filename. required monitor str quantity to monitor. 'val_loss' verbose int verbosity mode, 0 or 1. 0 save_best_only bool if save_best_only=True , the latest best model according to the quantity monitored will not be overwritten. If path doesn't contain formatting options like {epoch} then path will be overwritten by each new better model. False mode str one of {auto, min, max}. If save_best_only=True , the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc , this should be max , for val_loss this should be min , etc. In auto mode, the direction is automatically inferred from the name of the monitored quantity. 'auto' save_freq str 'epoch' or integer. When using 'epoch' , the callback saves the model after each epoch. When using integer, the callback saves the model at end of this many batches. Note that if the saving isn't aligned to epochs, the monitored metric may potentially be less reliable (it could reflect as little as 1 batch, since the metrics get reset every epoch). Defaults to 'epoch' 'epoch' period int the number of epochs between which the model is saved. This only works if save_freq is 'epoch', otherwise the save_freq will override this period. 1 Source code in elegy/callbacks/model_checkpoint.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def __init__ ( self , path : str , monitor : str = \"val_loss\" , verbose : int = 0 , save_best_only : bool = False , mode : str = \"auto\" , save_freq : str = \"epoch\" , period : int = 1 , ): \"\"\" Arguments: path: string, path to directory to save the model state. `path` can contain named formatting options, which will be filled the value of `epoch` and keys in `logs` (passed in `on_epoch_end`). For example: if `path` is `weights.{epoch:02d}-{val_loss:.2f}`, then the model checkpoints will be saved with the epoch number and the validation loss in the filename. monitor: quantity to monitor. verbose: verbosity mode, 0 or 1. save_best_only: if `save_best_only=True`, the latest best model according to the quantity monitored will not be overwritten. If `path` doesn't contain formatting options like `{epoch}` then `path` will be overwritten by each new better model. mode: one of {auto, min, max}. If `save_best_only=True`, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For `val_acc`, this should be `max`, for `val_loss` this should be `min`, etc. In `auto` mode, the direction is automatically inferred from the name of the monitored quantity. save_freq: `'epoch'` or integer. When using `'epoch'`, the callback saves the model after each epoch. When using integer, the callback saves the model at end of this many batches. Note that if the saving isn't aligned to epochs, the monitored metric may potentially be less reliable (it could reflect as little as 1 batch, since the metrics get reset every epoch). Defaults to `'epoch'` period: the number of epochs between which the model is saved. This only works if `save_freq` is 'epoch', otherwise the `save_freq` will override this period. \"\"\" super ( ModelCheckpoint , self ) . __init__ () self . monitor = monitor self . verbose = verbose self . path = path self . save_best_only = save_best_only self . save_freq = save_freq self . period = period self . epochs_since_last_save = 0 self . _batches_seen_since_last_saving = 0 if mode not in [ \"auto\" , \"min\" , \"max\" ]: logging . warning ( \"ModelCheckpoint mode %s is unknown, \" \"fallback to auto mode.\" , mode ) mode = \"auto\" if mode == \"min\" : self . monitor_op = np . less self . best = np . Inf elif mode == \"max\" : self . monitor_op = np . greater self . best = - np . Inf else : if \"acc\" in self . monitor or self . monitor . startswith ( \"fmeasure\" ): self . monitor_op = np . greater self . best = - np . Inf else : self . monitor_op = np . less self . best = np . Inf if self . save_freq != \"epoch\" and not isinstance ( self . save_freq , int ): raise ValueError ( \"Unrecognized save_freq: {} \" . format ( self . save_freq ))","title":"__init__()"},{"location":"api/callbacks/RemoteMonitor/","text":"elegy.callbacks.RemoteMonitor Callback used to stream events to a server. Requires the requests library. Events are sent to root + '/publish/epoch/end/' by default. Calls are HTTP POST, with a data argument which is a JSON-encoded dictionary of event data. If send_as_json is set to True, the content type of the request will be application/json. Otherwise the serialized JSON will be sent within a form. __init__ ( self , root = 'http://localhost:9000' , path = '/publish/epoch/end/' , field = 'data' , headers = None , send_as_json = False ) special Parameters: Name Type Description Default root str String; root url of the target server. 'http://localhost:9000' path str String; path relative to root to which the events will be sent. '/publish/epoch/end/' field str String; JSON field under which the data will be stored. The field is used only if the payload is sent within a form (i.e. send_as_json is set to False). 'data' headers Optional[Dict[str, str]] Dictionary; optional custom HTTP headers. None send_as_json bool Boolean; whether the request should be sent as application/json. False Source code in elegy/callbacks/remote_monitor.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , root : str = \"http://localhost:9000\" , path : str = \"/publish/epoch/end/\" , field : str = \"data\" , headers : tp . Optional [ tp . Dict [ str , str ]] = None , send_as_json : bool = False , ): \"\"\" Arguments: root: String; root url of the target server. path: String; path relative to `root` to which the events will be sent. field: String; JSON field under which the data will be stored. The field is used only if the payload is sent within a form (i.e. send_as_json is set to False). headers: Dictionary; optional custom HTTP headers. send_as_json: Boolean; whether the request should be sent as application/json. \"\"\" super ( RemoteMonitor , self ) . __init__ () self . root = root self . path = path self . field = field self . headers = headers self . send_as_json = send_as_json","title":"RemoteMonitor"},{"location":"api/callbacks/RemoteMonitor/#elegycallbacksremotemonitor","text":"","title":"elegy.callbacks.RemoteMonitor"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor","text":"Callback used to stream events to a server. Requires the requests library. Events are sent to root + '/publish/epoch/end/' by default. Calls are HTTP POST, with a data argument which is a JSON-encoded dictionary of event data. If send_as_json is set to True, the content type of the request will be application/json. Otherwise the serialized JSON will be sent within a form.","title":"elegy.callbacks.remote_monitor.RemoteMonitor"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.__init__","text":"Parameters: Name Type Description Default root str String; root url of the target server. 'http://localhost:9000' path str String; path relative to root to which the events will be sent. '/publish/epoch/end/' field str String; JSON field under which the data will be stored. The field is used only if the payload is sent within a form (i.e. send_as_json is set to False). 'data' headers Optional[Dict[str, str]] Dictionary; optional custom HTTP headers. None send_as_json bool Boolean; whether the request should be sent as application/json. False Source code in elegy/callbacks/remote_monitor.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , root : str = \"http://localhost:9000\" , path : str = \"/publish/epoch/end/\" , field : str = \"data\" , headers : tp . Optional [ tp . Dict [ str , str ]] = None , send_as_json : bool = False , ): \"\"\" Arguments: root: String; root url of the target server. path: String; path relative to `root` to which the events will be sent. field: String; JSON field under which the data will be stored. The field is used only if the payload is sent within a form (i.e. send_as_json is set to False). headers: Dictionary; optional custom HTTP headers. send_as_json: Boolean; whether the request should be sent as application/json. \"\"\" super ( RemoteMonitor , self ) . __init__ () self . root = root self . path = path self . field = field self . headers = headers self . send_as_json = send_as_json","title":"__init__()"},{"location":"api/callbacks/TensorBoard/","text":"elegy.callbacks.TensorBoard Callback that streams epoch results to tensorboard events folder. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . Examples: tensorboard_logger = TensorBoard ( 'runs' ) model . fit ( X_train , Y_train , callbacks = [ tensorboard_logger ]) __init__ ( self , logdir = None , update_freq = 'epoch' , ** kwargs ) special Parameters: Name Type Description Default logdir Optional[str] Save directory location. Default is runs/ CURRENT_DATETIME_HOSTNAME , which changes after each run. Use hierarchical folder structure to compare between runs easily. e.g. pass in 'runs/exp1', 'runs/exp2', etc. for each new experiment to compare across them. None update_freq Union[str, int] 'batch' or 'epoch' or integer. When using 'batch' , writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch' . 'epoch' **kwargs Options to pass to SummaryWriter object {} Source code in elegy/callbacks/tensorboard.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , logdir : Optional [ str ] = None , update_freq : Union [ str , int ] = \"epoch\" , ** kwargs ) -> None : \"\"\" Arguments: logdir: Save directory location. Default is runs/**CURRENT_DATETIME_HOSTNAME**, which changes after each run. Use hierarchical folder structure to compare between runs easily. e.g. pass in 'runs/exp1', 'runs/exp2', etc. for each new experiment to compare across them. update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`, writes the losses and metrics to TensorBoard after each batch. The same applies for `'epoch'`. **kwargs: Options to pass to `SummaryWriter` object \"\"\" self . logdir = logdir self . writer = None self . keys = None if update_freq == \"batch\" : self . update_freq = 1 else : self . update_freq = update_freq self . _open_args = kwargs if kwargs else {} super ( TensorBoard , self ) . __init__ ()","title":"Tensorboard"},{"location":"api/callbacks/TensorBoard/#elegycallbackstensorboard","text":"","title":"elegy.callbacks.TensorBoard"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard","text":"Callback that streams epoch results to tensorboard events folder. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . Examples: tensorboard_logger = TensorBoard ( 'runs' ) model . fit ( X_train , Y_train , callbacks = [ tensorboard_logger ])","title":"elegy.callbacks.tensorboard.TensorBoard"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.__init__","text":"Parameters: Name Type Description Default logdir Optional[str] Save directory location. Default is runs/ CURRENT_DATETIME_HOSTNAME , which changes after each run. Use hierarchical folder structure to compare between runs easily. e.g. pass in 'runs/exp1', 'runs/exp2', etc. for each new experiment to compare across them. None update_freq Union[str, int] 'batch' or 'epoch' or integer. When using 'batch' , writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch' . 'epoch' **kwargs Options to pass to SummaryWriter object {} Source code in elegy/callbacks/tensorboard.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , logdir : Optional [ str ] = None , update_freq : Union [ str , int ] = \"epoch\" , ** kwargs ) -> None : \"\"\" Arguments: logdir: Save directory location. Default is runs/**CURRENT_DATETIME_HOSTNAME**, which changes after each run. Use hierarchical folder structure to compare between runs easily. e.g. pass in 'runs/exp1', 'runs/exp2', etc. for each new experiment to compare across them. update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`, writes the losses and metrics to TensorBoard after each batch. The same applies for `'epoch'`. **kwargs: Options to pass to `SummaryWriter` object \"\"\" self . logdir = logdir self . writer = None self . keys = None if update_freq == \"batch\" : self . update_freq = 1 else : self . update_freq = update_freq self . _open_args = kwargs if kwargs else {} super ( TensorBoard , self ) . __init__ ()","title":"__init__()"},{"location":"api/callbacks/TerminateOnNaN/","text":"elegy.callbacks.TerminateOnNaN Callback that terminates training when a NaN loss is encountered.","title":"TerminateOnNaN"},{"location":"api/callbacks/TerminateOnNaN/#elegycallbacksterminateonnan","text":"","title":"elegy.callbacks.TerminateOnNaN"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN","text":"Callback that terminates training when a NaN loss is encountered.","title":"elegy.callbacks.terminate_nan.TerminateOnNaN"},{"location":"api/losses/BinaryCrossentropy/","text":"elegy.losses.BinaryCrossentropy Computes the cross-entropy loss between true labels and predicted labels. Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction. In the snippet below, each of the four examples has only a single floating-pointing value, and both y_pred and y_true have the shape [batch_size] . Usage: y_true = jnp . array ([[ 0. , 1. ], [ 0. , 0. ]]) y_pred = jnp . array [[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. bce = elegy . losses . BinaryCrossentropy () result = bce ( y_true , y_pred ) assert jnp . isclose ( result , 0.815 , rtol = 0.01 ) # Calling with 'sample_weight'. bce = elegy . losses . BinaryCrossentropy () result = bce ( y_true , y_pred , sample_weight = jnp . array ([ 1 , 0 ])) assert jnp . isclose ( result , 0.458 , rtol = 0.01 ) # Using 'sum' reduction type. bce = elegy . losses . BinaryCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = bce ( y_true , y_pred ) assert jnp . isclose ( result , 1.630 , rtol = 0.01 ) # Using 'none' reduction type. bce = elegy . losses . BinaryCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = bce ( y_true , y_pred ) assert jnp . all ( jnp . isclose ( result , [ 0.916 , 0.713 ], rtol = 0.01 )) Usage with the compile API: model = elegy . Model ( module_fn , loss = elegy . losses . BinaryCrossentropy (), metrics = elegy . metrics . Accuracy . defer (), optimizer = optix . adam ( 1e-3 ), ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the BinaryCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/binary_crossentropy.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `BinaryCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return binary_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"BinaryCrossentropy"},{"location":"api/losses/BinaryCrossentropy/#elegylossesbinarycrossentropy","text":"","title":"elegy.losses.BinaryCrossentropy"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy","text":"Computes the cross-entropy loss between true labels and predicted labels. Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction. In the snippet below, each of the four examples has only a single floating-pointing value, and both y_pred and y_true have the shape [batch_size] . Usage: y_true = jnp . array ([[ 0. , 1. ], [ 0. , 0. ]]) y_pred = jnp . array [[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. bce = elegy . losses . BinaryCrossentropy () result = bce ( y_true , y_pred ) assert jnp . isclose ( result , 0.815 , rtol = 0.01 ) # Calling with 'sample_weight'. bce = elegy . losses . BinaryCrossentropy () result = bce ( y_true , y_pred , sample_weight = jnp . array ([ 1 , 0 ])) assert jnp . isclose ( result , 0.458 , rtol = 0.01 ) # Using 'sum' reduction type. bce = elegy . losses . BinaryCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = bce ( y_true , y_pred ) assert jnp . isclose ( result , 1.630 , rtol = 0.01 ) # Using 'none' reduction type. bce = elegy . losses . BinaryCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = bce ( y_true , y_pred ) assert jnp . all ( jnp . isclose ( result , [ 0.916 , 0.713 ], rtol = 0.01 )) Usage with the compile API: model = elegy . Model ( module_fn , loss = elegy . losses . BinaryCrossentropy (), metrics = elegy . metrics . Accuracy . defer (), optimizer = optix . adam ( 1e-3 ), )","title":"elegy.losses.binary_crossentropy.BinaryCrossentropy"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.call","text":"Invokes the BinaryCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/binary_crossentropy.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `BinaryCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return binary_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"call()"},{"location":"api/losses/CategoricalCrossentropy/","text":"elegy.losses.CategoricalCrossentropy Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Usage: y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 0 , 1 ]]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cce = elegy . losses . CategoricalCrossentropy () assert cce ( y_true , y_pred ) == 1.177 # Calling with 'sample_weight'. assert cce ( y_true , y_pred , sample_weight = tf . constant ([ 0.3 , 0.7 ])) == 0.814 # Using 'sum' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) assert cce ( y_true , y_pred ) == 2.354 # Using 'none' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) assert list ( cce ( y_true , y_pred )) == [ 0.0513 , 2.303 ] Usage with the compile API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy . defer (), optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , from_logits = False , label_smoothing = 0 , reduction = None , name = None , weight = None , on = None ) special Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE` will raise an error. None name Optional[str] Optional name for the op. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/categorical_crossentropy.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, ` or `SUM_OVER_BATCH_SIZE` will raise an error. name: Optional name for the op. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , name = name , weight = weight , on = on ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing call ( self , y_true , y_pred , sample_weight = None ) Invokes the CategoricalCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/categorical_crossentropy.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `CategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegylossescategoricalcrossentropy","text":"","title":"elegy.losses.CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy","text":"Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Usage: y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 0 , 1 ]]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cce = elegy . losses . CategoricalCrossentropy () assert cce ( y_true , y_pred ) == 1.177 # Calling with 'sample_weight'. assert cce ( y_true , y_pred , sample_weight = tf . constant ([ 0.3 , 0.7 ])) == 0.814 # Using 'sum' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) assert cce ( y_true , y_pred ) == 2.354 # Using 'none' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) assert list ( cce ( y_true , y_pred )) == [ 0.0513 , 2.303 ] Usage with the compile API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy . defer (), optimizer = optix . adam ( 1e-3 ), )","title":"elegy.losses.categorical_crossentropy.CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.__init__","text":"Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE` will raise an error. None name Optional[str] Optional name for the op. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/categorical_crossentropy.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, ` or `SUM_OVER_BATCH_SIZE` will raise an error. name: Optional name for the op. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , name = name , weight = weight , on = on ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.call","text":"Invokes the CategoricalCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/categorical_crossentropy.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `CategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"call()"},{"location":"api/losses/Loss/","text":"elegy.losses.Loss Wraps a loss function in the Loss class. __init__ ( self , reduction = None , name = None , weight = None , on = None ) special Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None name Optional[str] Optional name for the loss. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/loss.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. name: Optional name for the loss. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . name = ( name if name is not None else re . sub ( r \"(?<!^)(?=[A-Z])\" , \"_\" , self . __class__ . __name__ ) . lower () ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . call = utils . inject_dependencies ( self . call )","title":"Loss"},{"location":"api/losses/Loss/#elegylossesloss","text":"","title":"elegy.losses.Loss"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss","text":"Wraps a loss function in the Loss class.","title":"elegy.losses.loss.Loss"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None name Optional[str] Optional name for the loss. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/loss.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. name: Optional name for the loss. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . name = ( name if name is not None else re . sub ( r \"(?<!^)(?=[A-Z])\" , \"_\" , self . __class__ . __name__ ) . lower () ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . call = utils . inject_dependencies ( self . call )","title":"__init__()"},{"location":"api/losses/MeanAbsoluteError/","text":"elegy.losses.MeanAbsoluteError Computes the mean absolute errors between labels and predictions. loss = mean(abs(y_true - y_pred)) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = elegy . losses . MeanAbsoluteError () assert mae ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mae ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = elegy . losses . MeanAbsoluteError ( reduction = elegy . losses . Reduction . SUM ) assert mae ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mae = elegy . losses . MeanAbsoluteError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mae ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean . defer (), ) __init__ ( self , reduction = None , name = None , weight = None , on = None ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None name Optional[str] Optional name for the loss. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/mean_absolute_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. name: Optional name for the loss. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , name = name , weight = weight , on = on ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_error.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( y_true , y_pred )","title":"MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#elegylossesmeanabsoluteerror","text":"","title":"elegy.losses.MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError","text":"Computes the mean absolute errors between labels and predictions. loss = mean(abs(y_true - y_pred)) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = elegy . losses . MeanAbsoluteError () assert mae ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mae ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = elegy . losses . MeanAbsoluteError ( reduction = elegy . losses . Reduction . SUM ) assert mae ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mae = elegy . losses . MeanAbsoluteError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mae ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean . defer (), )","title":"elegy.losses.mean_absolute_error.MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None name Optional[str] Optional name for the loss. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/mean_absolute_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. name: Optional name for the loss. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , name = name , weight = weight , on = on )","title":"__init__()"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.call","text":"Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_error.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/MeanSquaredError/","text":"elegy.losses.MeanSquaredError Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = elegy . losses . MeanSquaredError () assert mse ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mse ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . SUM ) assert mse ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mse ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredError (), metrics = elegy . metrics . Mean . defer (), ) __init__ ( self , reduction = None , name = None , weight = None , on = None ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None name Optional[str] Optional name for the loss. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/mean_squared_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. name: Optional name for the loss. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , name = name , weight = weight , on = on ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanSquaredError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_error.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( y_true , y_pred )","title":"MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegylossesmeansquarederror","text":"","title":"elegy.losses.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError","text":"Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = elegy . losses . MeanSquaredError () assert mse ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mse ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . SUM ) assert mse ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mse ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredError (), metrics = elegy . metrics . Mean . defer (), )","title":"elegy.losses.mean_squared_error.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None name Optional[str] Optional name for the loss. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/mean_squared_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. name: Optional name for the loss. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , name = name , weight = weight , on = on )","title":"__init__()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.call","text":"Invokes the MeanSquaredError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_error.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/SparseCategoricalCrossentropy/","text":"elegy.losses.SparseCategoricalCrossentropy Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Usage: y_true = jnp . array ([ 1 , 2 ]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy () result = scce ( y_true , y_pred ) # 1.177 assert jnp . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( y_true , y_pred , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert jnp . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = scce ( y_true , y_pred ) # 2.354 assert jnp . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = scce ( y_true , y_pred ) # [0.0513, 2.303] assert jnp . all ( jnp . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the compile API: model = elegy . Model ( module_fn , loss = lelegy . losses . SparseCategoricalCrossentropy (), metrics = lelegy . metrics . Accuracy . defer (), optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , from_logits = False , reduction = None , name = None , weight = None , on = None ) special Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. None name Optional[str] Optional name for the op. Defaults to 'sparse_categorical_crossentropy'. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/sparse_categorical_crossentropy.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def __init__ ( self , from_logits : bool = False , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. name: Optional name for the op. Defaults to 'sparse_categorical_crossentropy'. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , name = name , weight = weight , on = on ) self . _from_logits = from_logits call ( self , y_true , y_pred , sample_weight = None ) Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default y_true Ground truth values. required y_pred The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/sparse_categorical_crossentropy.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def call ( self , y_true , y_pred , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return sparse_categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegylossessparsecategoricalcrossentropy","text":"","title":"elegy.losses.SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy","text":"Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Usage: y_true = jnp . array ([ 1 , 2 ]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy () result = scce ( y_true , y_pred ) # 1.177 assert jnp . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( y_true , y_pred , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert jnp . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = scce ( y_true , y_pred ) # 2.354 assert jnp . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = scce ( y_true , y_pred ) # [0.0513, 2.303] assert jnp . all ( jnp . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the compile API: model = elegy . Model ( module_fn , loss = lelegy . losses . SparseCategoricalCrossentropy (), metrics = lelegy . metrics . Accuracy . defer (), optimizer = optix . adam ( 1e-3 ), )","title":"elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.__init__","text":"Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. None name Optional[str] Optional name for the op. Defaults to 'sparse_categorical_crossentropy'. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/sparse_categorical_crossentropy.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def __init__ ( self , from_logits : bool = False , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. name: Optional name for the op. Defaults to 'sparse_categorical_crossentropy'. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , name = name , weight = weight , on = on ) self . _from_logits = from_logits","title":"__init__()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.call","text":"Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default y_true Ground truth values. required y_pred The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/sparse_categorical_crossentropy.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def call ( self , y_true , y_pred , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return sparse_categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"call()"},{"location":"api/losses/mean_squared_error/","text":"elegy.losses.mean_squared_error Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"mean_squared_error"},{"location":"api/losses/mean_squared_error/#elegylossesmean_squared_error","text":"","title":"elegy.losses.mean_squared_error"},{"location":"api/losses/mean_squared_error/#elegy.losses.mean_squared_error.mean_squared_error","text":"Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"elegy.losses.mean_squared_error.mean_squared_error"},{"location":"api/metrics/Accuracy/","text":"elegy.metrics.Accuracy Calculates how often predictions equals labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. accuracy = elegy . metrics . Accuracy () result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ]) ) assert result == 0.75 # 3 / 4 result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ]) ) assert result == 0.5 # 4 / 8 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy . defer (), optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , name = None , dtype = None , on = None ) special Creates a Accuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/accuracy.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Creates a `Accuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype , on = on ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/accuracy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"Accuracy"},{"location":"api/metrics/Accuracy/#elegymetricsaccuracy","text":"","title":"elegy.metrics.Accuracy"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy","text":"Calculates how often predictions equals labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. accuracy = elegy . metrics . Accuracy () result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ]) ) assert result == 0.75 # 3 / 4 result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ]) ) assert result == 0.5 # 4 / 8 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy . defer (), optimizer = optix . adam ( 1e-3 ), )","title":"elegy.metrics.accuracy.Accuracy"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.__init__","text":"Creates a Accuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/accuracy.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Creates a `Accuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype , on = on )","title":"__init__()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/accuracy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/CategoricalAccuracy/","text":"elegy.metrics.CategoricalAccuracy Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . CategoricalAccuracy () result = accuracy ( y_true = jnp . array ([[ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . CategoricalAccuracy . defer (), optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , name = None , dtype = None , on = None ) special Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/categorical_accuracy.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype , on = on ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/categorical_accuracy.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegymetricscategoricalaccuracy","text":"","title":"elegy.metrics.CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy","text":"Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . CategoricalAccuracy () result = accuracy ( y_true = jnp . array ([[ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . CategoricalAccuracy . defer (), optimizer = optix . adam ( 1e-3 ), )","title":"elegy.metrics.categorical_accuracy.CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.__init__","text":"Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/categorical_accuracy.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype , on = on )","title":"__init__()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/categorical_accuracy.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/Mean/","text":"elegy.metrics.Mean Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredError (), metrics = elegy . metrics . Mean . defer (), ) __init__ ( self , name = None , dtype = None , on = None ) special Creates a Mean instance. Parameters: Name Type Description Default name Optional[str] (Optional) string name of the metric instance. None dtype Optional[numpy.dtype] (Optional) data type of the metric result. None Source code in elegy/metrics/mean.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. \"\"\" super () . __init__ ( reduction = reduce . Reduction . WEIGHTED_MEAN , name = name , dtype = dtype , on = on ) call ( self , values , sample_weight = None ) Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ndarray Array with the cumulative mean. Source code in elegy/metrics/mean.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" return super () . call ( values = values , sample_weight = sample_weight )","title":"Mean"},{"location":"api/metrics/Mean/#elegymetricsmean","text":"","title":"elegy.metrics.Mean"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean","text":"Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredError (), metrics = elegy . metrics . Mean . defer (), )","title":"elegy.metrics.mean.Mean"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.__init__","text":"Creates a Mean instance. Parameters: Name Type Description Default name Optional[str] (Optional) string name of the metric instance. None dtype Optional[numpy.dtype] (Optional) data type of the metric result. None Source code in elegy/metrics/mean.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. \"\"\" super () . __init__ ( reduction = reduce . Reduction . WEIGHTED_MEAN , name = name , dtype = dtype , on = on )","title":"__init__()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.call","text":"Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ndarray Array with the cumulative mean. Source code in elegy/metrics/mean.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" return super () . call ( values = values , sample_weight = sample_weight )","title":"call()"},{"location":"api/metrics/MeanAbsoluteError/","text":"elegy.metrics.MeanAbsoluteError Computes the cumulative mean absolute error between y_true and y_pred . Usage: mae = elegy . metrics . MeanAbsoluteError () result = mae ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mae ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . MeanAbsoluteError . defer (), ) __init__ ( self , name = None , dtype = None , on = None ) special Creates a MeanAbsoluteError instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/mean_absolute_error.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Creates a `MeanAbsoluteError` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype , on = on ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_absolute_error.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_absolute_error ( y_true = y_true , y_pred = y_pred ))","title":"MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#elegymetricsmeanabsoluteerror","text":"","title":"elegy.metrics.MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError","text":"Computes the cumulative mean absolute error between y_true and y_pred . Usage: mae = elegy . metrics . MeanAbsoluteError () result = mae ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mae ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . MeanAbsoluteError . defer (), )","title":"elegy.metrics.mean_absolute_error.MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.__init__","text":"Creates a MeanAbsoluteError instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/mean_absolute_error.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Creates a `MeanAbsoluteError` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype , on = on )","title":"__init__()"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_absolute_error.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_absolute_error ( y_true = y_true , y_pred = y_pred ))","title":"call()"},{"location":"api/metrics/MeanSquaredError/","text":"elegy.metrics.MeanSquaredError Computes the cumulative mean squared error between y_true and y_pred . Usage: mse = elegy . metrics . MeanSquaredError () result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . MeanSquaredError . defer (), ) __init__ ( self , name = None , dtype = None , on = None ) special Creates a MeanSquaredError instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/mean_squared_error.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Creates a `MeanSquaredError` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype , on = on ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_squared_error.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_squared_error ( y_true = y_true , y_pred = y_pred ))","title":"MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegymetricsmeansquarederror","text":"","title":"elegy.metrics.MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError","text":"Computes the cumulative mean squared error between y_true and y_pred . Usage: mse = elegy . metrics . MeanSquaredError () result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . MeanSquaredError . defer (), )","title":"elegy.metrics.mean_squared_error.MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.__init__","text":"Creates a MeanSquaredError instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/mean_squared_error.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Creates a `MeanSquaredError` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype , on = on )","title":"__init__()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_squared_error.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_squared_error ( y_true = y_true , y_pred = y_pred ))","title":"call()"},{"location":"api/metrics/Metric/","text":"elegy.metrics.Metric Encapsulates metric logic and state. Usage: m = SomeMetric ( ... ) for input in ... : m . update_state ( input ) print ( 'Final result: ' , m . result () . numpy ()) Usage with elegy API: import haiku as hk import jax from jax.experimental import optix def module_fn ( x ): return hk . Sequential ([ hk . Linear ( 64 ), jax . nn . relu , hk . Linear ( 64 ), jax . nn . relu , hk . Linear ( 10 ), jax . nn . softmax , ])( x ) model = elegy . Model ( module_fn , optimizer = optix . rmsprop ( 0.01 ) loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy . defer (), ) To be implemented by subclasses: call() : Computes the actual metric Example subclass implementation: TODO","title":"Metric"},{"location":"api/metrics/Metric/#elegymetricsmetric","text":"","title":"elegy.metrics.Metric"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric","text":"Encapsulates metric logic and state. Usage: m = SomeMetric ( ... ) for input in ... : m . update_state ( input ) print ( 'Final result: ' , m . result () . numpy ()) Usage with elegy API: import haiku as hk import jax from jax.experimental import optix def module_fn ( x ): return hk . Sequential ([ hk . Linear ( 64 ), jax . nn . relu , hk . Linear ( 64 ), jax . nn . relu , hk . Linear ( 10 ), jax . nn . softmax , ])( x ) model = elegy . Model ( module_fn , optimizer = optix . rmsprop ( 0.01 ) loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy . defer (), ) To be implemented by subclasses: call() : Computes the actual metric Example subclass implementation: TODO","title":"elegy.metrics.metric.Metric"},{"location":"api/metrics/SparseCategoricalAccuracy/","text":"elegy.metrics.SparseCategoricalAccuracy Calculates how often predictions matches integer labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . SparseCategoricalAccuracy () result = accuracy ( y_true = jnp . array ([ 2 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]) ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([ 1 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . SparseCategoricalAccuracy . defer (), optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , name = None , dtype = None , on = None ) special Creates a SparseCategoricalAccuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/sparse_categorical_accuracy.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Creates a `SparseCategoricalAccuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype , on = on ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape except y_true should not have the last dimension of y_pred . Parameters: Name Type Description Default y_true ndarray Sparse ground truth values. shape = [batch_size, d0, .. dN-1] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN-1, dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/sparse_categorical_accuracy.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape except `y_true` should not have the last dimension of `y_pred`. Arguments: y_true: Sparse ground truth values. shape = `[batch_size, d0, .. dN-1]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN-1, dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = sparse_categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegymetricssparsecategoricalaccuracy","text":"","title":"elegy.metrics.SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy","text":"Calculates how often predictions matches integer labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . SparseCategoricalAccuracy () result = accuracy ( y_true = jnp . array ([ 2 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]) ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([ 1 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . SparseCategoricalAccuracy . defer (), optimizer = optix . adam ( 1e-3 ), )","title":"elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.__init__","text":"Creates a SparseCategoricalAccuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/sparse_categorical_accuracy.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , on : tp . Optional [ types . IndexLike ] = None , ): \"\"\" Creates a `SparseCategoricalAccuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype , on = on )","title":"__init__()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape except y_true should not have the last dimension of y_pred . Parameters: Name Type Description Default y_true ndarray Sparse ground truth values. shape = [batch_size, d0, .. dN-1] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN-1, dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/sparse_categorical_accuracy.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape except `y_true` should not have the last dimension of `y_pred`. Arguments: y_true: Sparse ground truth values. shape = `[batch_size, d0, .. dN-1]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN-1, dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = sparse_categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/model/load/","text":"elegy.model.load Loads a model from disk. This function will restore both the model architecture, that is, its Model class instance, along with all of its parameters, state, and optimizer state. Examples: import elegy model . save ( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load ( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Exceptions: Type Description OSError in case the model was not found or could not be loaded from disk successfully. Source code in elegy/model.py 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 def load ( path : tp . Union [ str , Path ]) -> Model : \"\"\" Loads a model from disk. This function will restore both the model architecture, that is, its `Model` class instance, along with all of its parameters, state, and optimizer state. Example: ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path to a saved model's directory. Raises: OSError: in case the model was not found or could not be loaded from disk successfully. \"\"\" if isinstance ( path , str ): path = Path ( path ) with open ( path / \"model.pkl\" , \"rb\" ) as f : try : model = pickle . load ( f ) except BaseException as e : raise OSError ( f \"Could not load the model. Got exception: { e } \" ) model . load ( path ) return model","title":"load"},{"location":"api/model/load/#elegymodelload","text":"","title":"elegy.model.load"},{"location":"api/model/load/#elegy.model.load","text":"Loads a model from disk. This function will restore both the model architecture, that is, its Model class instance, along with all of its parameters, state, and optimizer state. Examples: import elegy model . save ( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load ( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Exceptions: Type Description OSError in case the model was not found or could not be loaded from disk successfully. Source code in elegy/model.py 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 def load ( path : tp . Union [ str , Path ]) -> Model : \"\"\" Loads a model from disk. This function will restore both the model architecture, that is, its `Model` class instance, along with all of its parameters, state, and optimizer state. Example: ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path to a saved model's directory. Raises: OSError: in case the model was not found or could not be loaded from disk successfully. \"\"\" if isinstance ( path , str ): path = Path ( path ) with open ( path / \"model.pkl\" , \"rb\" ) as f : try : model = pickle . load ( f ) except BaseException as e : raise OSError ( f \"Could not load the model. Got exception: { e } \" ) model . load ( path ) return model","title":"elegy.model.load"},{"location":"api/regularizers/GlobalL1/","text":"elegy.regularizers.GlobalL1 Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . regularizers . GlobalL1 ( l = 1e-5 ) ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L1 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L1 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l1.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def GlobalL1 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l1_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.regularizers.GlobalL1(l=1e-5) ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L1 regularization factor. Returns: An L1 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l1 = l , reduction = reduction , name = name )","title":"GlobalL1"},{"location":"api/regularizers/GlobalL1/#elegyregularizersgloball1","text":"","title":"elegy.regularizers.GlobalL1"},{"location":"api/regularizers/GlobalL1/#elegy.regularizers.global_l1.GlobalL1","text":"Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . regularizers . GlobalL1 ( l = 1e-5 ) ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L1 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L1 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l1.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def GlobalL1 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l1_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.regularizers.GlobalL1(l=1e-5) ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L1 regularization factor. Returns: An L1 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l1 = l , reduction = reduction , name = name )","title":"elegy.regularizers.global_l1.GlobalL1"},{"location":"api/regularizers/GlobalL1L2/","text":"elegy.regularizers.GlobalL1L2 A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . regularizers . GlobalL1L2 ( l1 = 1e-5 , l2 = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Attributes: Name Type Description l1 L1 regularization factor. l2 L2 regularization factor. call ( self , params ) Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default params Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]] A structure with all the parameters of the model. required Source code in elegy/regularizers/global_l1l2.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def call ( self , params : hk . Params ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: params: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in jax . tree_leaves ( params ) ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in jax . tree_leaves ( params ) ) return regularization","title":"GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegyregularizersgloball1l2","text":"","title":"elegy.regularizers.GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2","text":"A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . regularizers . GlobalL1L2 ( l1 = 1e-5 , l2 = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Attributes: Name Type Description l1 L1 regularization factor. l2 L2 regularization factor.","title":"elegy.regularizers.global_l1l2.GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.call","text":"Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default params Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]] A structure with all the parameters of the model. required Source code in elegy/regularizers/global_l1l2.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def call ( self , params : hk . Params ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: params: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in jax . tree_leaves ( params ) ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in jax . tree_leaves ( params ) ) return regularization","title":"call()"},{"location":"api/regularizers/GlobalL2/","text":"elegy.regularizers.GlobalL2 Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . losses . GlobaL2Regularization ( l = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L2 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L2 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l2.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def GlobalL2 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l2_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.losses.GlobaL2Regularization(l=1e-4), ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L2 regularization factor. Returns: An L2 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l2 = l , reduction = reduction , name = name )","title":"GlobalL2"},{"location":"api/regularizers/GlobalL2/#elegyregularizersgloball2","text":"","title":"elegy.regularizers.GlobalL2"},{"location":"api/regularizers/GlobalL2/#elegy.regularizers.global_l2.GlobalL2","text":"Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . losses . GlobaL2Regularization ( l = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L2 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L2 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l2.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def GlobalL2 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l2_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.losses.GlobaL2Regularization(l=1e-4), ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L2 regularization factor. Returns: An L2 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l2 = l , reduction = reduction , name = name )","title":"elegy.regularizers.global_l2.GlobalL2"},{"location":"guides/contributing/","text":"Contributing This is a short guide on how to start contibuting to Elegy along with some best practices for the project. Setup We use poetry so the easiest way to setup a development environment is run poetry install Creating Losses and Metrics For this you can follow these guidelines: Each loss / metric should be defined in its own file. Inherit from either elegy.losses.loss.Loss or elegy.metrics.metric.Metric or an existing class that inherits from them. Try to use an existing metric or loss as a template You must provide documentation for the following: The class definition. The __init__ method. The call method. Try to port the documentation + signature from its Keras counter part. If so you must give credits to the original source file. You must include tests. If you there exists an equivalent loss/metric in Keras you must test numerical equivalence between both. Testing To execute all the tests just run pytest Documentation We use mkdocs . If you create a new object that requires documentation please do the following: Add a markdown file inside /docs/api in the appropriate location according to the project's structure. This file must: Contain the path of function / class as header Use mkdocstring to render the API information. Example: # elegy . losses . BinaryCrossentropy ::: elegy . losses . BinaryCrossentropy selection : inherited_members : true members : - call - __init__ Add and entry to mkdocs.yml inside nav pointing to this file. Checkout mkdocs.yml . To build and visualize the documentation locally run mkdocs serve","title":"Contibuting"},{"location":"guides/contributing/#contributing","text":"This is a short guide on how to start contibuting to Elegy along with some best practices for the project.","title":"Contributing"},{"location":"guides/contributing/#setup","text":"We use poetry so the easiest way to setup a development environment is run poetry install","title":"Setup"},{"location":"guides/contributing/#creating-losses-and-metrics","text":"For this you can follow these guidelines: Each loss / metric should be defined in its own file. Inherit from either elegy.losses.loss.Loss or elegy.metrics.metric.Metric or an existing class that inherits from them. Try to use an existing metric or loss as a template You must provide documentation for the following: The class definition. The __init__ method. The call method. Try to port the documentation + signature from its Keras counter part. If so you must give credits to the original source file. You must include tests. If you there exists an equivalent loss/metric in Keras you must test numerical equivalence between both.","title":"Creating Losses and Metrics"},{"location":"guides/contributing/#testing","text":"To execute all the tests just run pytest","title":"Testing"},{"location":"guides/contributing/#documentation","text":"We use mkdocs . If you create a new object that requires documentation please do the following: Add a markdown file inside /docs/api in the appropriate location according to the project's structure. This file must: Contain the path of function / class as header Use mkdocstring to render the API information. Example: # elegy . losses . BinaryCrossentropy ::: elegy . losses . BinaryCrossentropy selection : inherited_members : true members : - call - __init__ Add and entry to mkdocs.yml inside nav pointing to this file. Checkout mkdocs.yml . To build and visualize the documentation locally run mkdocs serve","title":"Documentation"},{"location":"guides/modules-losses-metrics/","text":"Modules, Losses, and Metrics This guide goes into depth on how modules, losses and metrics work in Elegy and how to create your own. One of our goals with Elegy was to solve Keras restrictions around the type of losses and metrics you can define. When creating a complex model with multiple outputs in Keras, say output_a and output_b , you are forced to define losses and metrics per-output only: model . compile ( loss = { \"output_a\" : keras . losses . BinaryCrossentropy ( from_logits = True ), \"output_b\" : keras . losses . CategoricalCrossentropy ( from_logits = True ), }, metrics = { \"output_a\" : keras . losses . BinaryAccuracy ( from_logits = True ), \"output_b\" : keras . losses . CategoricalAccuracy ( from_logits = True ), }, ... ) This very restrictive, in particular it doesn't allow the following: Losses and metrics that combine multiple outputs with multiple labels. A single loss/metrics based on multiple outputs (a especial case of the previous). Losses and metrics that depend on the inputs of the model. Most of these are usually solvable by concatenating the outputs / labels or passing the inputs as labels. However it is clear that these solution are hacky at best and depending on the problem they can be insufficient. Dependency Injection Elegy solves the previous problems by introducing a dependency injection mechanism that allows the user to express complex functions by simply declaring the variables it wants to use by their name . The following parameters are available for the different callables you pass to Elegy: parameter Description Module Metric Loss x Inputs of the model corresponding to the x argument of fit * x x y_true The input labels corresponding to the y argument of fit x x y_pred Outputs of the model x x sample_weight Importance of each sample x x class_weight Importance of each class x x is_training Whether training is currently in progress x x x params The learnable parameters of the model x x state The non-learnable parameters of the model x x Note The content of x is technically passed to the model's Module but the parameter name x will bare no special meaning in that context. Modules Modules define the architecture of the network, their primary task (in Elegy terms) is transforming the inputs x into outputs y_pred . To make it easy to consume the content of x , Elegy has some special but very simple rules on how the signature of any Module can be structured: 1. If x is a tuple , then x will be expanded positional arguments a.k.a. *args , this means that the module will have define exactly as many arguments as there are inputs. For example: class SomeModule ( elegy . Module ): def call ( self , m , n ): ... ... a , b = get_inputs () model . fit ( x = ( a , b ), ... ) In this case a is passed as m and b is passed as n . 2. If x is a single array it will be converted internally into a tuple containing that array so the module can expect it as a positional argument. 3. If x is a dict , then x will be expanded as keyword arguments a.k.a. **kwargs , in this case the module can optionally request any key defined in x as an argument. For example: class SomeModule ( elegy . Module ): def call ( self , n ): ... ... a , b = get_inputs () model . fit ( x = { \"m\" : a , \"n\" : b }, ... ) Here n is requested by name and you get as input its value b , and m with the content of a is safely ignored. Losses Losses can request all the available parameters that Elegy provides for dependency injection. A typical loss will request the y_true and y_pred values (as its common / enforced in Keras). The Mean Squared Error loss for example is easily defined in these terms: class MSE ( elegy . Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 ) ... X_train , y_train = get_inputs () model . fit ( x = X_train , y = y_train , ... ) Here the input y is passed as y_true to MSE . However, if you for example want to build an autoencoder then, according to the math, you actually don't need y because you are actually trying to reconstruct x . It makes perfect sense for this lossto be defined in terms of x and Elegy lets you do exactly that: class AutoEncoderLoss ( elegy . Loss ): def call ( self , x , y_pred ): return jnp . mean ( jnp . square ( x - y_pred ), axis =- 1 ) ... X_train , _ = get_inputs () model . fit ( x = X_train ... ) Notice thanks to this we didn't have to define y on the fit method. Note An alternative here is to just use the previous definition of MSE and define y=X_train . However, avoiding the creation of redundant information is good in general and being explicit about dependencies might help documenting the behaviour of the model in general. Partitioning a loss If you have a complex loss function that is just a sum of different parts that have to be compute together you might define something like this: class SomeComplexFunction ( elegy . Loss ): def call ( self , x , y_true , y_pred , params , ... ): ... return a + b + c Elegy lets you return a dict specifying the name of each part: class SomeComplexFunction ( elegy . Loss ): def call ( self , x , y_true , y_pred , params , ... ): ... return { \"a\" : a , \"b\" : b , \"c\" : c , } Elegy will use this information to show you each loss separate in the logs / Tensorboard / History with the names: some_complex_function_loss/a some_complex_function_loss/b some_complex_function_loss/c Each individual loss will still be subject to the sample_weight and reduction behavior as specified to SomeComplexFunction . Multiple Outputs + Labels The Model 's constructor loss argument can accept a single Loss , a list or dict of losses, and even nested structures of the previous, yet in Elegy the form of loss is not strictly related to structure of input labels and outputs of the model. This is very different to Keras where each loss has to be matched with exactly one (label, output) pair. Elegy's method of dealing with multiple outputs and labels is super simple: Quote y_true will contain the entire structure passed to y . y_pred will contain the entire structure output by the Module . This means there are no restrictions on how you structure the loss function. According to this rule Keras and Elegy behave the same when there is only one output and one label because there is no structure. Both framework will allow you to define something like: model = Model ( ... loss = elegy . losses . CategoricalCrossentropy ( from_logits = True ) ) However, if you have many outputs and many labels, Elegy will just pass their structures to your loss and you will be able to do whatever you want by e.g. indexing these structures: class MyLoss ( Elegy . Loss ): def call ( self , y_true , y_pred ): return some_function ( y_true [ \"label_a\" ], y_pred [ \"output_a\" ], y_true [ \"label_b\" ] ) model = Model ( ... loss = elegy . losses . MyLoss () ) This example assumes the y_true and y_pred are dictionaries but they can also be tuples or nested structures. This strategy gives you maximal flexibility but come with the additional cost of having to implement your own loss function. Keras-like behavior While having this flexibility available is good, there is a common scenario that Keras covers really well: what if you really just need one loss per (label, output) pair? In other words, how can we achieve equivalent of the following Keras code in Elege? class MyModel ( keras . Model ): def call ( self , x ): ... return { \"key_a\" : key_a , \"key_b\" : key_b , ... } ... model . compile ( loss = { \"key_a\" : keras . losses . BinaryCrossentropy (), \"key_b\" : keras . losses . MeanSquaredError (), ... }, loss_weights = { \"key_a\" : 10.0 , \"key_b\" : 1.0 , ... }, ) To recover this behavior Elegy lets each Loss optionally filter / index the y_true and y_pred arguments based on a string key (for dict s) or integer key (for tuple s) in the constructor's on parameter: class MyModule ( elegy . Module ): def call ( self , x ): ... return { \"key_a\" : key_a , \"key_b\" : key_b , ... } ... model = elegy . Model ( module = MyModule . defer (), loss = [ elegy . losses . BinaryCrossentropy ( on = \"key_a\" , weight = 10.0 ), elegy . losses . MeanSquaredError ( on = \"key_b\" , weight = 1.0 ), ... ] ) This is almost exactly how Keras behaves except each loss is explicitly aware of which part of the output / label its supposed to attend to. The previous is roughly equivalent to manually indexing y_true and y_pred and passing the resulting value to the loss in question like this: model = elegy . Model ( module = MyModule . defer (), loss = [ lambda y_true , y_pred : elegy . losses . BinaryCrossentropy ( weight = 10.0 )( y_true = y_true [ \"key_a\" ], y_pred = y_pred [ \"key_a\" ], ), lambda y_true , y_pred : elegy . losses . MeanSquaredError ( weight = 1.0 )( y_true = y_true [ \"key_b\" ], y_pred = y_pred [ \"key_b\" ], ), ... ] ) Note For the same reasons Elegy doesn't support the loss_weights parameter as defined in keras.compile . Nonetheless, each loss accepts a weight argument directly, as seen in the examples above, which you can use to recover this behavior. Metrics Metrics behave exactly like losses except for one thing: Metrics can hold state. As in Keras, Elegy metrics are cumulative metrics which update their internal state on every step. From an user's perspective this means a couple of things: Metrics are implemented using Haiku Module , this means that you can't instantiate them normally outside of Haiku, hence the lambda / defer trick. You can use hk.get_state and hk.set_state when implementing your own metrics. Here is an example of a simple implementation of Accuracy which uses this cumulative behavior: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = hk . get_state ( \"total\" , [], jnp . zeros ) count = hk . get_state ( \"count\" , [], jnp . zeros ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) hk . set_state ( \"total\" , total ) hk . set_state ( \"count\" , count ) return total / count A little secret We think users should use the base classes provided by Elegy (Module, Loss, Metric) for convenience, being true to Haiku and Jax in general Elegy also lets you use plain functions. Be cautious when doing this since you can easily run into trouble with Haiku's scoping rules.","title":"Modules, Losses, and Metrics"},{"location":"guides/modules-losses-metrics/#modules-losses-and-metrics","text":"This guide goes into depth on how modules, losses and metrics work in Elegy and how to create your own. One of our goals with Elegy was to solve Keras restrictions around the type of losses and metrics you can define. When creating a complex model with multiple outputs in Keras, say output_a and output_b , you are forced to define losses and metrics per-output only: model . compile ( loss = { \"output_a\" : keras . losses . BinaryCrossentropy ( from_logits = True ), \"output_b\" : keras . losses . CategoricalCrossentropy ( from_logits = True ), }, metrics = { \"output_a\" : keras . losses . BinaryAccuracy ( from_logits = True ), \"output_b\" : keras . losses . CategoricalAccuracy ( from_logits = True ), }, ... ) This very restrictive, in particular it doesn't allow the following: Losses and metrics that combine multiple outputs with multiple labels. A single loss/metrics based on multiple outputs (a especial case of the previous). Losses and metrics that depend on the inputs of the model. Most of these are usually solvable by concatenating the outputs / labels or passing the inputs as labels. However it is clear that these solution are hacky at best and depending on the problem they can be insufficient.","title":"Modules, Losses, and Metrics"},{"location":"guides/modules-losses-metrics/#dependency-injection","text":"Elegy solves the previous problems by introducing a dependency injection mechanism that allows the user to express complex functions by simply declaring the variables it wants to use by their name . The following parameters are available for the different callables you pass to Elegy: parameter Description Module Metric Loss x Inputs of the model corresponding to the x argument of fit * x x y_true The input labels corresponding to the y argument of fit x x y_pred Outputs of the model x x sample_weight Importance of each sample x x class_weight Importance of each class x x is_training Whether training is currently in progress x x x params The learnable parameters of the model x x state The non-learnable parameters of the model x x Note The content of x is technically passed to the model's Module but the parameter name x will bare no special meaning in that context.","title":"Dependency Injection"},{"location":"guides/modules-losses-metrics/#modules","text":"Modules define the architecture of the network, their primary task (in Elegy terms) is transforming the inputs x into outputs y_pred . To make it easy to consume the content of x , Elegy has some special but very simple rules on how the signature of any Module can be structured: 1. If x is a tuple , then x will be expanded positional arguments a.k.a. *args , this means that the module will have define exactly as many arguments as there are inputs. For example: class SomeModule ( elegy . Module ): def call ( self , m , n ): ... ... a , b = get_inputs () model . fit ( x = ( a , b ), ... ) In this case a is passed as m and b is passed as n . 2. If x is a single array it will be converted internally into a tuple containing that array so the module can expect it as a positional argument. 3. If x is a dict , then x will be expanded as keyword arguments a.k.a. **kwargs , in this case the module can optionally request any key defined in x as an argument. For example: class SomeModule ( elegy . Module ): def call ( self , n ): ... ... a , b = get_inputs () model . fit ( x = { \"m\" : a , \"n\" : b }, ... ) Here n is requested by name and you get as input its value b , and m with the content of a is safely ignored.","title":"Modules"},{"location":"guides/modules-losses-metrics/#losses","text":"Losses can request all the available parameters that Elegy provides for dependency injection. A typical loss will request the y_true and y_pred values (as its common / enforced in Keras). The Mean Squared Error loss for example is easily defined in these terms: class MSE ( elegy . Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 ) ... X_train , y_train = get_inputs () model . fit ( x = X_train , y = y_train , ... ) Here the input y is passed as y_true to MSE . However, if you for example want to build an autoencoder then, according to the math, you actually don't need y because you are actually trying to reconstruct x . It makes perfect sense for this lossto be defined in terms of x and Elegy lets you do exactly that: class AutoEncoderLoss ( elegy . Loss ): def call ( self , x , y_pred ): return jnp . mean ( jnp . square ( x - y_pred ), axis =- 1 ) ... X_train , _ = get_inputs () model . fit ( x = X_train ... ) Notice thanks to this we didn't have to define y on the fit method. Note An alternative here is to just use the previous definition of MSE and define y=X_train . However, avoiding the creation of redundant information is good in general and being explicit about dependencies might help documenting the behaviour of the model in general.","title":"Losses"},{"location":"guides/modules-losses-metrics/#partitioning-a-loss","text":"If you have a complex loss function that is just a sum of different parts that have to be compute together you might define something like this: class SomeComplexFunction ( elegy . Loss ): def call ( self , x , y_true , y_pred , params , ... ): ... return a + b + c Elegy lets you return a dict specifying the name of each part: class SomeComplexFunction ( elegy . Loss ): def call ( self , x , y_true , y_pred , params , ... ): ... return { \"a\" : a , \"b\" : b , \"c\" : c , } Elegy will use this information to show you each loss separate in the logs / Tensorboard / History with the names: some_complex_function_loss/a some_complex_function_loss/b some_complex_function_loss/c Each individual loss will still be subject to the sample_weight and reduction behavior as specified to SomeComplexFunction .","title":"Partitioning a loss"},{"location":"guides/modules-losses-metrics/#multiple-outputs-labels","text":"The Model 's constructor loss argument can accept a single Loss , a list or dict of losses, and even nested structures of the previous, yet in Elegy the form of loss is not strictly related to structure of input labels and outputs of the model. This is very different to Keras where each loss has to be matched with exactly one (label, output) pair. Elegy's method of dealing with multiple outputs and labels is super simple: Quote y_true will contain the entire structure passed to y . y_pred will contain the entire structure output by the Module . This means there are no restrictions on how you structure the loss function. According to this rule Keras and Elegy behave the same when there is only one output and one label because there is no structure. Both framework will allow you to define something like: model = Model ( ... loss = elegy . losses . CategoricalCrossentropy ( from_logits = True ) ) However, if you have many outputs and many labels, Elegy will just pass their structures to your loss and you will be able to do whatever you want by e.g. indexing these structures: class MyLoss ( Elegy . Loss ): def call ( self , y_true , y_pred ): return some_function ( y_true [ \"label_a\" ], y_pred [ \"output_a\" ], y_true [ \"label_b\" ] ) model = Model ( ... loss = elegy . losses . MyLoss () ) This example assumes the y_true and y_pred are dictionaries but they can also be tuples or nested structures. This strategy gives you maximal flexibility but come with the additional cost of having to implement your own loss function.","title":"Multiple Outputs + Labels"},{"location":"guides/modules-losses-metrics/#keras-like-behavior","text":"While having this flexibility available is good, there is a common scenario that Keras covers really well: what if you really just need one loss per (label, output) pair? In other words, how can we achieve equivalent of the following Keras code in Elege? class MyModel ( keras . Model ): def call ( self , x ): ... return { \"key_a\" : key_a , \"key_b\" : key_b , ... } ... model . compile ( loss = { \"key_a\" : keras . losses . BinaryCrossentropy (), \"key_b\" : keras . losses . MeanSquaredError (), ... }, loss_weights = { \"key_a\" : 10.0 , \"key_b\" : 1.0 , ... }, ) To recover this behavior Elegy lets each Loss optionally filter / index the y_true and y_pred arguments based on a string key (for dict s) or integer key (for tuple s) in the constructor's on parameter: class MyModule ( elegy . Module ): def call ( self , x ): ... return { \"key_a\" : key_a , \"key_b\" : key_b , ... } ... model = elegy . Model ( module = MyModule . defer (), loss = [ elegy . losses . BinaryCrossentropy ( on = \"key_a\" , weight = 10.0 ), elegy . losses . MeanSquaredError ( on = \"key_b\" , weight = 1.0 ), ... ] ) This is almost exactly how Keras behaves except each loss is explicitly aware of which part of the output / label its supposed to attend to. The previous is roughly equivalent to manually indexing y_true and y_pred and passing the resulting value to the loss in question like this: model = elegy . Model ( module = MyModule . defer (), loss = [ lambda y_true , y_pred : elegy . losses . BinaryCrossentropy ( weight = 10.0 )( y_true = y_true [ \"key_a\" ], y_pred = y_pred [ \"key_a\" ], ), lambda y_true , y_pred : elegy . losses . MeanSquaredError ( weight = 1.0 )( y_true = y_true [ \"key_b\" ], y_pred = y_pred [ \"key_b\" ], ), ... ] ) Note For the same reasons Elegy doesn't support the loss_weights parameter as defined in keras.compile . Nonetheless, each loss accepts a weight argument directly, as seen in the examples above, which you can use to recover this behavior.","title":"Keras-like behavior"},{"location":"guides/modules-losses-metrics/#metrics","text":"Metrics behave exactly like losses except for one thing: Metrics can hold state. As in Keras, Elegy metrics are cumulative metrics which update their internal state on every step. From an user's perspective this means a couple of things: Metrics are implemented using Haiku Module , this means that you can't instantiate them normally outside of Haiku, hence the lambda / defer trick. You can use hk.get_state and hk.set_state when implementing your own metrics. Here is an example of a simple implementation of Accuracy which uses this cumulative behavior: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = hk . get_state ( \"total\" , [], jnp . zeros ) count = hk . get_state ( \"count\" , [], jnp . zeros ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) hk . set_state ( \"total\" , total ) hk . set_state ( \"count\" , count ) return total / count","title":"Metrics"},{"location":"guides/modules-losses-metrics/#a-little-secret","text":"We think users should use the base classes provided by Elegy (Module, Loss, Metric) for convenience, being true to Haiku and Jax in general Elegy also lets you use plain functions. Be cautious when doing this since you can easily run into trouble with Haiku's scoping rules.","title":"A little secret"}]}