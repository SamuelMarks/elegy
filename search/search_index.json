{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Elegy Elegy is a Neural Networks framework based on Jax inspired by Keras. Elegy implements the Keras API but makes changes to play better with Jax and gives more flexibility around losses and metrics and excellent module system that makes it super easy to use. Elegy is in an early stage, feel free to send us your feedback! Main Features Familiar : Elegy should feel very familiar to Keras users. Flexible : Elegy improves upon the basic Keras API by letting users optionally take more control over the definition of losses and metrics. Easy-to-use : Elegy maintains all the simplicity and ease of use that Keras brings with it. Compatible : Elegy strives to be compatible with the rest of the Jax ecosystem. For more information take a look at the Documentation . Installation Install Elegy using pip: pip install elegy For Windows users we recommend the Windows subsystem for linux 2 WSL2 since jax does not support it yet. Quick Start Elegy greatly simplifies the training of Deep Learning models compared to pure Jax where, due to Jax's functional nature, users have to do a lot of book keeping around the state of the model. In Elegy you just have to follow 3 basic steps: 1. Define the architecture inside an elegy.Module : class MLP ( elegy . Module ): def call ( self , x : jnp . ndarray ) -> jnp . ndarray : x = elegy . nn . Linear ( 300 )( x ) x = jax . nn . relu ( x ) x = elegy . nn . Linear ( 10 )( x ) return x Note that we can define sub-modules on-the-fly directly in the call (forward) method. 2. Create a Model from this module and specify additional things like losses, metrics, and optimizers: model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . rmsprop ( 1e-3 ), ) 3. Train the model using the fit method: model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . TensorBoard ( \"summaries\" )] ) And you are done! For more information check out: Our Getting Started tutorial. Elegy's Documentation . The examples directory. What is Jax? Why Jax & Elegy? Given all the well-stablished Deep Learning framework like TensorFlow + Keras or Pytorch + Pytorch-Lightning/Skorch, it is fair to ask why we need something like Jax + Elegy? Here are some of the reasons why this framework exists. Why Jax? Jax is a linear algebra library with the perfect recipe: * Numpy's familiar API * The speed and hardware support of XLA * Automatic Differentiation The awesome thing about Jax is that Deep Learning is just a use-case that it happens to excel at but you can use it for most task you would use NumPy for. Jax is so compatible with Numpy that is array type actually inherits from np.ndarray . In a sense, Jax takes the best of both TensorFlow and Pytorch in a principled manner: while both TF and Pytorch historically converged to the same set of features, their APIs still contain quirks they have to keep for compatibility. Why Elegy? We believe that Elegy can offer the best experience for coding Deep Learning applications by leveraging the power and familiarity of Jax API, an easy-to-use and succinct Module system, and packaging everything on top of a convenient Keras-like API. Elegy improves upon other Deep Learning frameworks in the following ways: Its hook-based Module System makes it easier (less verbose) to write model code compared to Keras & Pytorch since it lets you declare sub-modules, parameters, and states directly on your call (forward) method. Thanks to this you get shape inference for free so there is no need for a build method (Keras) or propagating shape information all over the place (Pytorch). A naive implementation of Linear could be as simple as: class Linear ( elegy . Module ): def __init__ ( self , units ): super () . __init__ () self . units = units def call ( self , x ): w = self . add_parameter ( \"w\" , [ x . shape [ - 1 ], self . units ], initializer = jnp . ones ) b = self . add_parameter ( \"b\" , [ self . units ], initializer = jnp . ones ) return jnp . dot ( x , w ) + b It has a very flexible system for defining the inputs for losses and metrics based on dependency injection in opposition to Keras rigid requirement to have matching (output, label) pairs, and being unable to use additional information like inputs, parameters, and states in the definition of losses and metrics. Its hook system preserve's reference information from a module to its sub-modules, parameters, and states while maintaining a functional API. This is crucial since most Jax-based frameworks like Flax and Haiku tend to loose this information which makes it very tricky to perform tasks like transfer learning where you need to mix a pre-trained models into a new model (easier to do if you keep references). Features Model estimator class losses module metrics module regularizers module callbacks module nn layers module For more information checkout the Reference API section in the Documentation . Contributing Deep Learning is evolving at an incredible pace, there is so much to do and so few hands. If you wish to contibute anything from a loss or metric to a new awesome feature for Elegy just open an issue or send a PR! For more information check out our Contributing Guide . About Us We are some friends passionate about ML. License Apache Citing Elegy To cite this project: BibTeX @software{elegy2020repository, author = {PoetsAI}, title = {Elegy: A Keras-like deep learning framework based on Jax}, url = {https://github.com/poets-ai/elegy}, version = {0.2.2}, year = {2020}, } Where the current version may be retrieved either from the Release tag or the file elegy/__init__.py and the year corresponds to the project's release year.","title":"Introduction"},{"location":"#elegy","text":"Elegy is a Neural Networks framework based on Jax inspired by Keras. Elegy implements the Keras API but makes changes to play better with Jax and gives more flexibility around losses and metrics and excellent module system that makes it super easy to use. Elegy is in an early stage, feel free to send us your feedback!","title":"Elegy"},{"location":"#main-features","text":"Familiar : Elegy should feel very familiar to Keras users. Flexible : Elegy improves upon the basic Keras API by letting users optionally take more control over the definition of losses and metrics. Easy-to-use : Elegy maintains all the simplicity and ease of use that Keras brings with it. Compatible : Elegy strives to be compatible with the rest of the Jax ecosystem. For more information take a look at the Documentation .","title":"Main Features"},{"location":"#installation","text":"Install Elegy using pip: pip install elegy For Windows users we recommend the Windows subsystem for linux 2 WSL2 since jax does not support it yet.","title":"Installation"},{"location":"#quick-start","text":"Elegy greatly simplifies the training of Deep Learning models compared to pure Jax where, due to Jax's functional nature, users have to do a lot of book keeping around the state of the model. In Elegy you just have to follow 3 basic steps: 1. Define the architecture inside an elegy.Module : class MLP ( elegy . Module ): def call ( self , x : jnp . ndarray ) -> jnp . ndarray : x = elegy . nn . Linear ( 300 )( x ) x = jax . nn . relu ( x ) x = elegy . nn . Linear ( 10 )( x ) return x Note that we can define sub-modules on-the-fly directly in the call (forward) method. 2. Create a Model from this module and specify additional things like losses, metrics, and optimizers: model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . rmsprop ( 1e-3 ), ) 3. Train the model using the fit method: model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . TensorBoard ( \"summaries\" )] ) And you are done! For more information check out: Our Getting Started tutorial. Elegy's Documentation . The examples directory. What is Jax?","title":"Quick Start"},{"location":"#why-jax-elegy","text":"Given all the well-stablished Deep Learning framework like TensorFlow + Keras or Pytorch + Pytorch-Lightning/Skorch, it is fair to ask why we need something like Jax + Elegy? Here are some of the reasons why this framework exists.","title":"Why Jax &amp; Elegy?"},{"location":"#why-jax","text":"Jax is a linear algebra library with the perfect recipe: * Numpy's familiar API * The speed and hardware support of XLA * Automatic Differentiation The awesome thing about Jax is that Deep Learning is just a use-case that it happens to excel at but you can use it for most task you would use NumPy for. Jax is so compatible with Numpy that is array type actually inherits from np.ndarray . In a sense, Jax takes the best of both TensorFlow and Pytorch in a principled manner: while both TF and Pytorch historically converged to the same set of features, their APIs still contain quirks they have to keep for compatibility.","title":"Why Jax?"},{"location":"#why-elegy","text":"We believe that Elegy can offer the best experience for coding Deep Learning applications by leveraging the power and familiarity of Jax API, an easy-to-use and succinct Module system, and packaging everything on top of a convenient Keras-like API. Elegy improves upon other Deep Learning frameworks in the following ways: Its hook-based Module System makes it easier (less verbose) to write model code compared to Keras & Pytorch since it lets you declare sub-modules, parameters, and states directly on your call (forward) method. Thanks to this you get shape inference for free so there is no need for a build method (Keras) or propagating shape information all over the place (Pytorch). A naive implementation of Linear could be as simple as: class Linear ( elegy . Module ): def __init__ ( self , units ): super () . __init__ () self . units = units def call ( self , x ): w = self . add_parameter ( \"w\" , [ x . shape [ - 1 ], self . units ], initializer = jnp . ones ) b = self . add_parameter ( \"b\" , [ self . units ], initializer = jnp . ones ) return jnp . dot ( x , w ) + b It has a very flexible system for defining the inputs for losses and metrics based on dependency injection in opposition to Keras rigid requirement to have matching (output, label) pairs, and being unable to use additional information like inputs, parameters, and states in the definition of losses and metrics. Its hook system preserve's reference information from a module to its sub-modules, parameters, and states while maintaining a functional API. This is crucial since most Jax-based frameworks like Flax and Haiku tend to loose this information which makes it very tricky to perform tasks like transfer learning where you need to mix a pre-trained models into a new model (easier to do if you keep references).","title":"Why Elegy?"},{"location":"#features","text":"Model estimator class losses module metrics module regularizers module callbacks module nn layers module For more information checkout the Reference API section in the Documentation .","title":"Features"},{"location":"#contributing","text":"Deep Learning is evolving at an incredible pace, there is so much to do and so few hands. If you wish to contibute anything from a loss or metric to a new awesome feature for Elegy just open an issue or send a PR! For more information check out our Contributing Guide .","title":"Contributing"},{"location":"#about-us","text":"We are some friends passionate about ML.","title":"About Us"},{"location":"#license","text":"Apache","title":"License"},{"location":"#citing-elegy","text":"To cite this project: BibTeX @software{elegy2020repository, author = {PoetsAI}, title = {Elegy: A Keras-like deep learning framework based on Jax}, url = {https://github.com/poets-ai/elegy}, version = {0.2.2}, year = {2020}, } Where the current version may be retrieved either from the Release tag or the file elegy/__init__.py and the year corresponds to the project's release year.","title":"Citing Elegy"},{"location":"getting-started/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); In this tutorial we will explore the basic features of Elegy . If you are a Keras user you should feel at home, if you are currently using Jax things will appear much more streamlined. To get started you will first need to install the following dependencies: In [ ]: ! pip install -- upgrade pip ! pip install elegy dataget matplotlib ! pip install jax == 0.1 . 75 jaxlib == 0.1 . 52 # for CPU only # For GPU install proper version of your CUDA, following will work in COLAB: # ! pip install --upgrade jax==0.1.75 jaxlib==0.1.52+cuda101 -f https://storage.googleapis.com/jax-releases/jax_releases.html Note that Elegy doesn't depend on jax since there are both cpu and gpu versions you can choose from, the previous block will install jax-cpu , if you want jax to run on gpu you will need to install it separately. If you are running this example on Colab you need to uncomment the part which installs the GPU version suitable for Colab. Loading the Data \u00b6 In this tutorial we will train a Neural Network on the MNIST dataset, for this we will first need to download and load the data into memory. Here we will use dataget for simplicity but you can use you favorite datasets library. In [1]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () print ( \"X_train:\" , X_train . shape , X_train . dtype ) print ( \"y_train:\" , y_train . shape , y_train . dtype ) print ( \"X_test:\" , X_test . shape , X_test . dtype ) print ( \"y_test:\" , y_test . shape , y_test . dtype ) X_train: (60000, 28, 28) uint8 y_train: (60000,) uint8 X_test: (10000, 28, 28) uint8 y_test: (10000,) uint8 In this case dataget loads the data from Yann LeCun's website. Defining the Architecture \u00b6 Now that we have the data we can define our model. In Elegy you can do this by inheriting from elegy.Module and defining a call method. This method should take in some inputs, perform a series of transformation using Jax, and returns the outputs of the network. In this example we will create a simple 2 layer MLP using Elegy modules: In [2]: import jax.numpy as jnp import jax import elegy class MLP ( elegy . Module ): \"\"\"Standard LeNet-300-100 MLP network.\"\"\" def __init__ ( self , n1 : int = 300 , n2 : int = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . n1 = n1 self . n2 = n2 def call ( self , image : jnp . ndarray ) -> jnp . ndarray : image = image . astype ( jnp . float32 ) / 255.0 mlp = elegy . nn . sequential ( elegy . nn . Flatten (), elegy . nn . Linear ( self . n1 ), jax . nn . relu , elegy . nn . Linear ( self . n2 ), jax . nn . relu , elegy . nn . Linear ( 10 ), ) return mlp ( image ) Here we are using sequential to stack two layers with relu activations and a final Linear layer with 10 units that represents the logits of the network. This code should feel familiar to most Keras / PyTorch users. The main difference here is that thanks to elegy's hooks system you can (uncoditionally) declare modules, parameters, and states right in your call (forward) function without having to explicitly assign them to properties. This tends to produce much more readable code and reduce boilerplate. Creating the Model \u00b6 Now that we have this module we can create an Elegy Model . In [3]: import optax model = elegy . Model ( module = MLP ( n1 = 300 , n2 = 100 ), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-4 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), ) Much like keras.Model , an Elegy Model is tasked with performing training, evaluation, and inference. The constructor of this class accepts most of the arguments accepted by keras.Model.compile as you might have seen but there are some notable differences: It requires you to pass a module as first argument. loss can be a list even if we don't have multiple corresponding outputs/labels, this is because Elegy exposes a more flexible system for defining losses and metrics based on Dependency Injection. As in Keras, you can get a rich description of the model by calling Model.summary with a sample input: In [4]: model . summary ( X_train [: 64 ]) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Layer \u2502 Outputs Shape \u2502 Trainable \u2502 Non-trainable \u2502 \u2502 \u2502 \u2502 Parameters \u2502 Parameters \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 Inputs \u2502 (64, 28, 28) uint8 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 flatten (Flatten) \u2502 (64, 784) float32 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear (Linear) \u2502 (64, 300) float32 \u2502 235,500 942.0 KB \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 relu \u2502 (64, 300) float32 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear_1 (Linear) \u2502 (64, 100) float32 \u2502 30,100 120.4 KB \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 relu_1 \u2502 (64, 100) float32 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear_2 (Linear) \u2502 (64, 10) float32 \u2502 1,010 4.0 KB \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Outputs (MLP) \u2502 (64, 10) float32 \u2502 0 \u2502 0 \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Total Parameters: 266,610 1.1 MB Trainable Parameters: 266,610 1.1 MB Non-trainable Parameters: 0 Training the Model \u00b6 Having our model instance we are now ready to pass it some data to start training. Like in Keras this is done via the fit method which contains more or less the same signature. We try to be as compatible with Keras as possible but also remove a lot of the Tensorflow specific stuff. The following code will train our model for 100 epochs while limiting each epoch to 200 steps and using a batch size of 64 : In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . ModelCheckpoint ( \"model\" , save_best_only = True )], ) ... Epoch 99/100 200/200 [==============================] - 1s 4ms/step - l2_regularization_loss: 0.0452 - loss: 0.0662 - sparse_categorical_accuracy: 0.9928 - sparse_categorical_crossentropy_loss: 0.0210 - val_l2_regularization_loss: 0.0451 - val_loss: 0.1259 - val_sparse_categorical_accuracy: 0.9766 - val_sparse_categorical_crossentropy_loss: 0.0808 Epoch 100/100 200/200 [==============================] - 1s 4ms/step - l2_regularization_loss: 0.0450 - loss: 0.0610 - sparse_categorical_accuracy: 0.9953 - sparse_categorical_crossentropy_loss: 0.0161 - val_l2_regularization_loss: 0.0447 - val_loss: 0.1093 - val_sparse_categorical_accuracy: 0.9795 - val_sparse_categorical_crossentropy_loss: 0.0646 As you see we've ported Keras progress bar and also implemented its Callback and History APIs. fit returns a history object which we will use next to visualize how the metrics and losses evolved during training. In [6]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history ) Generating Predictions \u00b6 Having our trained model we can now get some samples from the test set and generate some predictions. First we will just pick some random samples using numpy : In [7]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) x_sample = X_test [ idxs ] Here we selected 9 random images. Now we can use the predict method to get their labels: In [8]: y_pred = model . predict ( x = x_sample ) Easy right? Finally lets plot the results to see if they are accurate. In [9]: plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( x_sample [ k ], cmap = \"gray\" ) Perfect! Serialization \u00b6 To serialize the Model you can just use the model.save(...) method, this will create a folder with some files that contain the model's code plus all parameters and states. However, here we don't need to do that since we previously added the elegy.callbacks.ModelCheckpoint callback on fit which periodically does this for us during training. We configured ModelCheckpoint to save our model to a folder called \"model\" so we can just load it from there using elegy.model.load . Lets get a new model reference containing the same weights and call its evaluate method to verify everything loaded correctly: In [10]: # current model reference print ( \"current model id:\" , id ( model )) # load model from disk model = elegy . model . load ( \"model\" ) # new model reference print ( \"new model id: \" , id ( model )) # check that it works! model . evaluate ( x = X_test , y = y_test ) current model id: 140137340602160 new model id: 140136071352432 /data/cristian/elegy/.venv/lib/python3.8/site-packages/jax/numpy/lax_numpy.py:1531: FutureWarning: jax.numpy reductions won't accept lists and tuples in future versions, only scalars and ndarrays warnings.warn(msg, category=FutureWarning) 313/313 [==============================] - 1s 3ms/step - l2_regularization_loss: 0.0450 - loss: 0.1013 - sparse_categorical_accuracy: 0.9825 - sparse_categorical_crossentropy_loss: 0.0563 Out[10]: {'l2_regularization_loss': array(0.04499801, dtype=float32), 'loss': array(0.10126135, dtype=float32), 'sparse_categorical_accuracy': array(0.9825001, dtype=float32), 'sparse_categorical_crossentropy_loss': array(0.05626357, dtype=float32), 'size': 32} Excellent! We hope you've enjoyed this tutorial. Next Steps \u00b6 Elegy is still in a very early stage, there are probably tons of bugs and missing features but we will get there. If you have some ideas or feedback on the current design we are eager to hear from you, feel free to open an issue.","title":"Getting Started"},{"location":"getting-started/#loading-the-data","text":"In this tutorial we will train a Neural Network on the MNIST dataset, for this we will first need to download and load the data into memory. Here we will use dataget for simplicity but you can use you favorite datasets library. In [1]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () print ( \"X_train:\" , X_train . shape , X_train . dtype ) print ( \"y_train:\" , y_train . shape , y_train . dtype ) print ( \"X_test:\" , X_test . shape , X_test . dtype ) print ( \"y_test:\" , y_test . shape , y_test . dtype ) X_train: (60000, 28, 28) uint8 y_train: (60000,) uint8 X_test: (10000, 28, 28) uint8 y_test: (10000,) uint8 In this case dataget loads the data from Yann LeCun's website.","title":"Loading the Data"},{"location":"getting-started/#defining-the-architecture","text":"Now that we have the data we can define our model. In Elegy you can do this by inheriting from elegy.Module and defining a call method. This method should take in some inputs, perform a series of transformation using Jax, and returns the outputs of the network. In this example we will create a simple 2 layer MLP using Elegy modules: In [2]: import jax.numpy as jnp import jax import elegy class MLP ( elegy . Module ): \"\"\"Standard LeNet-300-100 MLP network.\"\"\" def __init__ ( self , n1 : int = 300 , n2 : int = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . n1 = n1 self . n2 = n2 def call ( self , image : jnp . ndarray ) -> jnp . ndarray : image = image . astype ( jnp . float32 ) / 255.0 mlp = elegy . nn . sequential ( elegy . nn . Flatten (), elegy . nn . Linear ( self . n1 ), jax . nn . relu , elegy . nn . Linear ( self . n2 ), jax . nn . relu , elegy . nn . Linear ( 10 ), ) return mlp ( image ) Here we are using sequential to stack two layers with relu activations and a final Linear layer with 10 units that represents the logits of the network. This code should feel familiar to most Keras / PyTorch users. The main difference here is that thanks to elegy's hooks system you can (uncoditionally) declare modules, parameters, and states right in your call (forward) function without having to explicitly assign them to properties. This tends to produce much more readable code and reduce boilerplate.","title":"Defining the Architecture"},{"location":"getting-started/#creating-the-model","text":"Now that we have this module we can create an Elegy Model . In [3]: import optax model = elegy . Model ( module = MLP ( n1 = 300 , n2 = 100 ), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-4 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), ) Much like keras.Model , an Elegy Model is tasked with performing training, evaluation, and inference. The constructor of this class accepts most of the arguments accepted by keras.Model.compile as you might have seen but there are some notable differences: It requires you to pass a module as first argument. loss can be a list even if we don't have multiple corresponding outputs/labels, this is because Elegy exposes a more flexible system for defining losses and metrics based on Dependency Injection. As in Keras, you can get a rich description of the model by calling Model.summary with a sample input: In [4]: model . summary ( X_train [: 64 ]) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Layer \u2502 Outputs Shape \u2502 Trainable \u2502 Non-trainable \u2502 \u2502 \u2502 \u2502 Parameters \u2502 Parameters \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 Inputs \u2502 (64, 28, 28) uint8 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 flatten (Flatten) \u2502 (64, 784) float32 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear (Linear) \u2502 (64, 300) float32 \u2502 235,500 942.0 KB \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 relu \u2502 (64, 300) float32 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear_1 (Linear) \u2502 (64, 100) float32 \u2502 30,100 120.4 KB \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 relu_1 \u2502 (64, 100) float32 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear_2 (Linear) \u2502 (64, 10) float32 \u2502 1,010 4.0 KB \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Outputs (MLP) \u2502 (64, 10) float32 \u2502 0 \u2502 0 \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Total Parameters: 266,610 1.1 MB Trainable Parameters: 266,610 1.1 MB Non-trainable Parameters: 0","title":"Creating the Model"},{"location":"getting-started/#training-the-model","text":"Having our model instance we are now ready to pass it some data to start training. Like in Keras this is done via the fit method which contains more or less the same signature. We try to be as compatible with Keras as possible but also remove a lot of the Tensorflow specific stuff. The following code will train our model for 100 epochs while limiting each epoch to 200 steps and using a batch size of 64 : In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . ModelCheckpoint ( \"model\" , save_best_only = True )], ) ... Epoch 99/100 200/200 [==============================] - 1s 4ms/step - l2_regularization_loss: 0.0452 - loss: 0.0662 - sparse_categorical_accuracy: 0.9928 - sparse_categorical_crossentropy_loss: 0.0210 - val_l2_regularization_loss: 0.0451 - val_loss: 0.1259 - val_sparse_categorical_accuracy: 0.9766 - val_sparse_categorical_crossentropy_loss: 0.0808 Epoch 100/100 200/200 [==============================] - 1s 4ms/step - l2_regularization_loss: 0.0450 - loss: 0.0610 - sparse_categorical_accuracy: 0.9953 - sparse_categorical_crossentropy_loss: 0.0161 - val_l2_regularization_loss: 0.0447 - val_loss: 0.1093 - val_sparse_categorical_accuracy: 0.9795 - val_sparse_categorical_crossentropy_loss: 0.0646 As you see we've ported Keras progress bar and also implemented its Callback and History APIs. fit returns a history object which we will use next to visualize how the metrics and losses evolved during training. In [6]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history )","title":"Training the Model"},{"location":"getting-started/#generating-predictions","text":"Having our trained model we can now get some samples from the test set and generate some predictions. First we will just pick some random samples using numpy : In [7]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) x_sample = X_test [ idxs ] Here we selected 9 random images. Now we can use the predict method to get their labels: In [8]: y_pred = model . predict ( x = x_sample ) Easy right? Finally lets plot the results to see if they are accurate. In [9]: plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( x_sample [ k ], cmap = \"gray\" ) Perfect!","title":"Generating Predictions"},{"location":"getting-started/#serialization","text":"To serialize the Model you can just use the model.save(...) method, this will create a folder with some files that contain the model's code plus all parameters and states. However, here we don't need to do that since we previously added the elegy.callbacks.ModelCheckpoint callback on fit which periodically does this for us during training. We configured ModelCheckpoint to save our model to a folder called \"model\" so we can just load it from there using elegy.model.load . Lets get a new model reference containing the same weights and call its evaluate method to verify everything loaded correctly: In [10]: # current model reference print ( \"current model id:\" , id ( model )) # load model from disk model = elegy . model . load ( \"model\" ) # new model reference print ( \"new model id: \" , id ( model )) # check that it works! model . evaluate ( x = X_test , y = y_test ) current model id: 140137340602160 new model id: 140136071352432 /data/cristian/elegy/.venv/lib/python3.8/site-packages/jax/numpy/lax_numpy.py:1531: FutureWarning: jax.numpy reductions won't accept lists and tuples in future versions, only scalars and ndarrays warnings.warn(msg, category=FutureWarning) 313/313 [==============================] - 1s 3ms/step - l2_regularization_loss: 0.0450 - loss: 0.1013 - sparse_categorical_accuracy: 0.9825 - sparse_categorical_crossentropy_loss: 0.0563 Out[10]: {'l2_regularization_loss': array(0.04499801, dtype=float32), 'loss': array(0.10126135, dtype=float32), 'sparse_categorical_accuracy': array(0.9825001, dtype=float32), 'sparse_categorical_crossentropy_loss': array(0.05626357, dtype=float32), 'size': 32} Excellent! We hope you've enjoyed this tutorial.","title":"Serialization"},{"location":"getting-started/#next-steps","text":"Elegy is still in a very early stage, there are probably tons of bugs and missing features but we will get there. If you have some ideas or feedback on the current design we are eager to hear from you, feel free to open an issue.","title":"Next Steps"},{"location":"api/Loss/","text":"elegy.Loss Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/loss.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"Loss"},{"location":"api/Loss/#elegyloss","text":"","title":"elegy.Loss"},{"location":"api/Loss/#elegy.losses.loss.Loss","text":"Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this.","title":"elegy.losses.loss.Loss"},{"location":"api/Loss/#elegy.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/loss.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"__init__()"},{"location":"api/Metric/","text":"elegy.Metric Encapsulates metric logic and state. Usage: m = SomeMetric ( ... ) for input in ... : result = m ( input ) print ( 'Final result: ' , result ) Usage with the Model API: class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), ], metrics = [ elegy . metrics . SparseCategoricalAccuracy () ], optimizer = optax . rmsprop ( 1e-3 ), ) To be implemented by subclasses: call() : All state variables should be created in this method by calling self.add_parameter(..., trainable=False) , update this state by calling self.update_parameter(...) , and return a result based on these states. Example subclass implementation: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = hk . add_parameter ( \"total\" , initializer = 0 , trainable = False ) count = hk . add_parameter ( \"count\" , initializer = 0 , trainable = False ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) hk . update_parameter ( \"total\" , total ) hk . update_parameter ( \"count\" , count ) return total / count __init__ ( self , on = None , ** kwargs ) special Base Metric constructor. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/metrics/metric.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Base Metric constructor. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"Metric"},{"location":"api/Metric/#elegymetric","text":"","title":"elegy.Metric"},{"location":"api/Metric/#elegy.metrics.metric.Metric","text":"Encapsulates metric logic and state. Usage: m = SomeMetric ( ... ) for input in ... : result = m ( input ) print ( 'Final result: ' , result ) Usage with the Model API: class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), ], metrics = [ elegy . metrics . SparseCategoricalAccuracy () ], optimizer = optax . rmsprop ( 1e-3 ), ) To be implemented by subclasses: call() : All state variables should be created in this method by calling self.add_parameter(..., trainable=False) , update this state by calling self.update_parameter(...) , and return a result based on these states. Example subclass implementation: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = hk . add_parameter ( \"total\" , initializer = 0 , trainable = False ) count = hk . add_parameter ( \"count\" , initializer = 0 , trainable = False ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) hk . update_parameter ( \"total\" , total ) hk . update_parameter ( \"count\" , count ) return total / count","title":"elegy.metrics.metric.Metric"},{"location":"api/Metric/#elegy.metrics.metric.Metric.__init__","text":"Base Metric constructor. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/metrics/metric.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Base Metric constructor. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"__init__()"},{"location":"api/Model/","text":"elegy.Model Model is tasked with performing training, evaluation, and inference for a given elegy.Module or haiku.Module . To create a Model you first have to define its architecture in a Module : class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) Then you can pass this Module to the Model 's constructor and specify additional things like losses, metrics, optimizer, and callbacks: model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . rmsprop ( 1e-3 ), ) Once the model is created, you can train the model with model.fit() , or use the model to do prediction with model.predict() . Checkout Getting Started for additional details. Attributes: Name Type Description parameters A haiku.Params structure with the weights of the model. states A haiku.State structure with non-trainable parameters of the model. optimizer_state A optax.OptState structure with states of the optimizer. metrics_states A haiku.State structure with the states of the metrics. initial_metrics_state A haiku.State structure with the initial states of the metrics. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code, instead of using Jax's jit to optimize the computation. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. evaluate ( self , x , y = None , verbose = 1 , batch_size = None , sample_weight = None , steps = None , callbacks = None ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required y Optional[Union[jax._src.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 1 batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None steps Optional[int] Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . This argument is not supported with array inputs. None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.model.Model.fit]. Returns: Type Description Dict[str, numpy.ndarray] A dictionary for mapping the losses and metrics names to the values obtained. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model/model.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 def evaluate ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], y : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. This argument is not supported with array inputs. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.model.Model.fit]. Returns: A dictionary for mapping the losses and metrics names to the values obtained. Raises: ValueError: in case of invalid arguments. \"\"\" data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , training = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . test_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 1 , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 ) Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable]] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for generator type is given below. None y Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator. 0.0 validation_data Optional[Union[Tuple, Iterable]] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy/Jax arrays, list of arrays or mappings tuple (x_val, y_val, val_sample_weights) of Numpy/Jax arrays, list of arrays or mappings generator For the first two cases, batch_size must be provided. For the last case, validation_steps should be provided, and should follow the same convention for yielding data as x . Note that validation_data does not support all the data types that are supported in x , eg, dict. None shuffle bool Boolean (whether to shuffle the training data before each epoch). This argument is ignored when x is a generator. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of generators (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description History A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model/model.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 def fit ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , None , ] = None , y : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ np . ndarray ] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , ) -> History : \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for generator type is given below. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a generator. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy/Jax arrays, list of arrays or mappings - tuple `(x_val, y_val, val_sample_weights)` of Numpy/Jax arrays, list of arrays or mappings - generator For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` should be provided, and should follow the same convention for yielding data as `x`. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict. shuffle: Boolean (whether to shuffle the training data before each epoch). This argument is ignored when `x` is a generator. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of generators (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if x is None : x = {} if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) # sample_weight = batch[2] if len(batch) == 3 else None x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . train_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) epoch_logs = copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) # print( # f\"epoch: {epoch} - \" # + \" - \".join(f\"{key}: {value:.3f}\" for key, value in epoch_logs.items()) # ) if self . stop_training : break callbacks . on_train_end () return self . history load ( self , path , include_optimizer = True ) Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the haiku.Params + haiku.State structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required include_optimizer bool If True, loads optimizer's state if available. True Source code in elegy/model/model.py 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 def load ( self , path : tp . Union [ str , Path ], include_optimizer : bool = True ) -> None : \"\"\" Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the `haiku.Params` + `haiku.State` structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Arguments: path: path to a saved model's directory. include_optimizer: If True, loads optimizer's state if available. \"\"\" if isinstance ( path , str ): path = Path ( path ) parameters : tp . Dict = deepdish . io . load ( path / \"parameters.h5\" ) optimizer_params_path = path / \"optimizer_parameters.pkl\" if include_optimizer and optimizer_params_path . exists (): with open ( optimizer_params_path , \"rb\" ) as f : parameters [ \"optimizer\" ] = pickle . load ( f ) self . set_parameters ( parameters ) predict ( self , x , verbose = 0 , batch_size = None , steps = None , callbacks = None ) Generates output predictions for the input samples. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generators (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.model.Model.fit]. Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description ndarray Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model/model.py 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 def predict ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> np . ndarray : \"\"\"Generates output predictions for the input samples. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generators (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.model.Model.fit]. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs , ) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs predict_on_batch ( self , x ) inherited Returns predictions for a single batch of samples. Parameters: Name Type Description Default x Union[jax._src.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. required Returns: Type Description Union[jax._src.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Jax array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between given number of inputs and expectations of the model. Source code in elegy/model/model.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def predict_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ] ) -> tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ]: \"\"\" Returns predictions for a single batch of samples. Arguments: x: Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. Returns: Jax array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. \"\"\" self . maybe_initialize ( mode = Mode . predict , x = x ) method = self . predict_step if self . run_eagerly else self . predict_step_jit return method ( x = x ) reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/model/model.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) save ( self , path , include_optimizer = True ) Saves the model to disk. It creates a directory that includes: The Model object instance serialized with pickle as as {path}/model.pkl , this allows you to re-instantiate the model later. The model parameters + states serialized into HDF5 as {path}/parameters.h5 . The states of the optimizer serialized with pickle as as {path}/optimizer_parameters.pkl , allowing to resume training exactly where you left off. We hope to use HDF5 in the future but optax states is incompatible with deepdish . This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via Model.load if the model is already instiated or elegy.model.load to load the model instance from its pickled version. import elegy model . save ( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load ( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path where model structure will be saved. required include_optimizer bool If True, save optimizer's states together. True Source code in elegy/model/model.py 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 def save ( self , path : tp . Union [ str , Path ], include_optimizer : bool = True ) -> None : \"\"\" Saves the model to disk. It creates a directory that includes: - The `Model` object instance serialized with `pickle` as as `{path}/model.pkl`, this allows you to re-instantiate the model later. - The model parameters + states serialized into HDF5 as `{path}/parameters.h5`. - The states of the optimizer serialized with `pickle` as as `{path}/optimizer_parameters.pkl`, allowing to resume training exactly where you left off. We hope to use HDF5 in the future but `optax` states is incompatible with `deepdish`. This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via `Model.load` if the model is already instiated or `elegy.model.load` to load the model instance from its pickled version. ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path where model structure will be saved. include_optimizer: If True, save optimizer's states together. \"\"\" if isinstance ( path , str ): path = Path ( path ) path . mkdir ( parents = True , exist_ok = True ) parameters = self . get_parameters () if \"optimizer\" in parameters : del parameters [ \"optimizer\" ] deepdish . io . save ( path / \"parameters.h5\" , parameters ) # optimizer parameters saved as a pickle because optax uses named tuples # which are converted to regular tuples by deepdish when loading. if include_optimizer and self . optimizer is not None : optimizer_params = self . optimizer . get_parameters () with open ( path / \"optimizer_parameters.pkl\" , \"wb\" ) as f : pickle . dump ( optimizer_params , f ) else : optimizer_params = None self . reset () try : path = path / \"model.pkl\" with open ( path , \"wb\" ) as f : cloudpickle . dump ( self , f ) except BaseException as e : print ( f \"Error occurred saving the model object at { path } \\n Continuing....\" ) if optimizer_params is not None : parameters [ \"optimizer\" ] = optimizer_params self . set_parameters ( parameters ) summary ( self , x , depth = 2 , tablefmt = 'fancy_grid' , ** tablulate_kwargs ) inherited Prints a summary of the network. Parameters: Name Type Description Default x A sample of inputs to the network. required depth int The level number of nested level which will be showed. Information about summaries from modules deeper than depth will be aggregated together. 2 tablefmt str A string represeting the style of the table generated by tabulate . See python-tabulate for more options. 'fancy_grid' tablulate_kwargs Additional keyword arguments passed to tabulate . See python-tabulate for more options. {} Source code in elegy/model/model.py 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 def summary ( self , x , depth : int = 2 , tablefmt : str = \"fancy_grid\" , ** tablulate_kwargs ): \"\"\" Prints a summary of the network. Arguments: x: A sample of inputs to the network. depth: The level number of nested level which will be showed. Information about summaries from modules deeper than `depth` will be aggregated together. tablefmt: A string represeting the style of the table generated by `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. tablulate_kwargs: Additional keyword arguments passed to `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. \"\"\" self . maybe_initialize ( mode = Mode . predict , x = x ) with hooks_context ( summaries = True ): self . predict_fn ( x ) summaries = get_summaries () assert summaries is not None def format_output ( outputs ) -> str : file = StringIO () outputs = jax . tree_map ( lambda x : f \" { x . shape } {{ pad }} { x . dtype } \" , outputs ) yaml . safe_dump ( outputs , file , default_flow_style = False , indent = 2 , explicit_end = False ) return file . getvalue () . replace ( \" \\n ...\" , \"\" ) def format_size ( size ): return ( f \" { size / 1e9 : ,.1f } GB\" if size > 1e9 else f \" { size / 1e6 : ,.1f } MB\" if size > 1e6 else f \" { size / 1e3 : ,.1f } KB\" if size > 1e3 else f \" { size : , } B\" ) table : tp . List = [[ \"Inputs\" , format_output ( x ), \"0\" , \"0\" ]] for module , base_name , value in summaries : base_name_parts = base_name . split ( \"/\" )[ 1 :] module_depth = len ( base_name_parts ) if module_depth > depth : continue include_submodules = module_depth == depth params_count = ( module . parameters_size ( include_submodules ) if module is not None else 0 ) params_size = ( module . parameters_bytes ( include_submodules ) if module is not None else 0 ) states_count = ( module . states_size ( include_submodules ) if module is not None else 0 ) states_size = ( module . states_bytes ( include_submodules ) if module is not None else 0 ) class_name = f \"( { module . __class__ . __name__ } )\" if module is not None else \"\" base_name = \"/\" . join ( base_name_parts ) if not base_name : base_name = \"Outputs\" table . append ( [ f \" { base_name } {{ pad }} { class_name } \" , format_output ( value ), f \" { params_count : , } {{ pad }} { format_size ( params_size ) } \" if params_count > 0 else \"0\" , f \" { states_count : , } {{ pad }} { format_size ( states_size ) } \" if states_count > 0 else \"0\" , ] ) # add papdding for col in range ( 4 ): max_length = max ( len ( line . split ( \" {pad} \" )[ 0 ]) for row in table for line in row [ col ] . split ( \" \\n \" ) ) for row in table : row [ col ] = \" \\n \" . join ( line . format ( pad = \" \" * ( max_length - len ( line . rstrip () . split ( \" {pad} \" )[ 0 ])) ) for line in row [ col ] . rstrip () . split ( \" \\n \" ) ) print ( \" \\n \" + tabulate ( table , headers = [ \"Layer\" , \"Outputs Shape\" , \"Trainable \\n Parameters\" , \"Non-trainable \\n Parameters\" , ], tablefmt = tablefmt , ** tablulate_kwargs , ) ) params_count = self . parameters_size () params_size = self . parameters_bytes () states_count = self . states_size () states_size = self . states_bytes () total_count = params_count + states_count total_size = params_size + states_size print ( tabulate ( [ [ f \"Total Parameters:\" , f \" { total_count : , } \" , f \" { format_size ( total_size ) } \" if total_count > 0 else \"\" , ], [ f \"Trainable Parameters:\" , f \" { params_count : , } \" , f \" { format_size ( params_size ) } \" if params_count > 0 else \"\" , ], [ f \"Non-trainable Parameters:\" , f \" { states_count : , } \" , f \" { format_size ( states_size ) } \" if states_count > 0 else \"\" , ], ], tablefmt = \"plain\" , ) + \" \\n \" ) test_on_batch ( self , x , y = None , sample_weight = None , class_weight = None ) inherited Test the model on a single batch of samples. Parameters: Name Type Description Default x Union[jax._src.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[jax._src.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). None sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model/model.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def test_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ jnp . ndarray ] = None , class_weight : tp . Optional [ jnp . ndarray ] = None , ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . maybe_initialize ( mode = Mode . test , x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , ) method = self . test_step if self . run_eagerly else self . test_step_jit loss , logs , grads = method ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight ) return logs train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None ) inherited Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). It should be consistent with x (you cannot have Numpy inputs and array targets, or inversely). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight Optional[numpy.ndarray] Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None Returns: Type Description Dict[str, numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model/model.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def train_on_batch ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ np . ndarray ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\" Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). It should be consistent with `x` (you cannot have Numpy inputs and array targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . maybe_initialize ( mode = Mode . train , x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , ) method = self . train_step if self . run_eagerly else self . train_step_jit return method ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight )","title":"Model"},{"location":"api/Model/#elegymodel","text":"","title":"elegy.Model"},{"location":"api/Model/#elegy.model.model.Model","text":"Model is tasked with performing training, evaluation, and inference for a given elegy.Module or haiku.Module . To create a Model you first have to define its architecture in a Module : class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) Then you can pass this Module to the Model 's constructor and specify additional things like losses, metrics, optimizer, and callbacks: model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . rmsprop ( 1e-3 ), ) Once the model is created, you can train the model with model.fit() , or use the model to do prediction with model.predict() . Checkout Getting Started for additional details. Attributes: Name Type Description parameters A haiku.Params structure with the weights of the model. states A haiku.State structure with non-trainable parameters of the model. optimizer_state A optax.OptState structure with states of the optimizer. metrics_states A haiku.State structure with the states of the metrics. initial_metrics_state A haiku.State structure with the initial states of the metrics. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code, instead of using Jax's jit to optimize the computation. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls.","title":"elegy.model.model.Model"},{"location":"api/Model/#elegy.model.model.Model.evaluate","text":"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required y Optional[Union[jax._src.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 1 batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None steps Optional[int] Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . This argument is not supported with array inputs. None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.model.Model.fit]. Returns: Type Description Dict[str, numpy.ndarray] A dictionary for mapping the losses and metrics names to the values obtained. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model/model.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 def evaluate ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], y : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. This argument is not supported with array inputs. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.model.Model.fit]. Returns: A dictionary for mapping the losses and metrics names to the values obtained. Raises: ValueError: in case of invalid arguments. \"\"\" data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , training = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . test_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs","title":"evaluate()"},{"location":"api/Model/#elegy.model.model.Model.fit","text":"Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable]] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for generator type is given below. None y Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator. 0.0 validation_data Optional[Union[Tuple, Iterable]] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy/Jax arrays, list of arrays or mappings tuple (x_val, y_val, val_sample_weights) of Numpy/Jax arrays, list of arrays or mappings generator For the first two cases, batch_size must be provided. For the last case, validation_steps should be provided, and should follow the same convention for yielding data as x . Note that validation_data does not support all the data types that are supported in x , eg, dict. None shuffle bool Boolean (whether to shuffle the training data before each epoch). This argument is ignored when x is a generator. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of generators (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description History A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model/model.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 def fit ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , None , ] = None , y : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ np . ndarray ] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , ) -> History : \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for generator type is given below. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a generator. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy/Jax arrays, list of arrays or mappings - tuple `(x_val, y_val, val_sample_weights)` of Numpy/Jax arrays, list of arrays or mappings - generator For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` should be provided, and should follow the same convention for yielding data as `x`. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict. shuffle: Boolean (whether to shuffle the training data before each epoch). This argument is ignored when `x` is a generator. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of generators (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if x is None : x = {} if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) # sample_weight = batch[2] if len(batch) == 3 else None x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . train_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) epoch_logs = copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) # print( # f\"epoch: {epoch} - \" # + \" - \".join(f\"{key}: {value:.3f}\" for key, value in epoch_logs.items()) # ) if self . stop_training : break callbacks . on_train_end () return self . history","title":"fit()"},{"location":"api/Model/#elegy.model.model.Model.load","text":"Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the haiku.Params + haiku.State structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required include_optimizer bool If True, loads optimizer's state if available. True Source code in elegy/model/model.py 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 def load ( self , path : tp . Union [ str , Path ], include_optimizer : bool = True ) -> None : \"\"\" Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the `haiku.Params` + `haiku.State` structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Arguments: path: path to a saved model's directory. include_optimizer: If True, loads optimizer's state if available. \"\"\" if isinstance ( path , str ): path = Path ( path ) parameters : tp . Dict = deepdish . io . load ( path / \"parameters.h5\" ) optimizer_params_path = path / \"optimizer_parameters.pkl\" if include_optimizer and optimizer_params_path . exists (): with open ( optimizer_params_path , \"rb\" ) as f : parameters [ \"optimizer\" ] = pickle . load ( f ) self . set_parameters ( parameters )","title":"load()"},{"location":"api/Model/#elegy.model.model.Model.predict","text":"Generates output predictions for the input samples. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generators (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.model.Model.fit]. Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description ndarray Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model/model.py 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 def predict ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> np . ndarray : \"\"\"Generates output predictions for the input samples. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generators (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.model.Model.fit]. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs , ) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs","title":"predict()"},{"location":"api/Model/#elegy.model.model.Model.predict_on_batch","text":"Returns predictions for a single batch of samples. Parameters: Name Type Description Default x Union[jax._src.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. required Returns: Type Description Union[jax._src.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Jax array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between given number of inputs and expectations of the model. Source code in elegy/model/model.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def predict_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ] ) -> tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ]: \"\"\" Returns predictions for a single batch of samples. Arguments: x: Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. Returns: Jax array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. \"\"\" self . maybe_initialize ( mode = Mode . predict , x = x ) method = self . predict_step if self . run_eagerly else self . predict_step_jit return method ( x = x )","title":"predict_on_batch()"},{"location":"api/Model/#elegy.model.model.Model.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/model/model.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/Model/#elegy.model.model.Model.save","text":"Saves the model to disk. It creates a directory that includes: The Model object instance serialized with pickle as as {path}/model.pkl , this allows you to re-instantiate the model later. The model parameters + states serialized into HDF5 as {path}/parameters.h5 . The states of the optimizer serialized with pickle as as {path}/optimizer_parameters.pkl , allowing to resume training exactly where you left off. We hope to use HDF5 in the future but optax states is incompatible with deepdish . This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via Model.load if the model is already instiated or elegy.model.load to load the model instance from its pickled version. import elegy model . save ( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load ( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path where model structure will be saved. required include_optimizer bool If True, save optimizer's states together. True Source code in elegy/model/model.py 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 def save ( self , path : tp . Union [ str , Path ], include_optimizer : bool = True ) -> None : \"\"\" Saves the model to disk. It creates a directory that includes: - The `Model` object instance serialized with `pickle` as as `{path}/model.pkl`, this allows you to re-instantiate the model later. - The model parameters + states serialized into HDF5 as `{path}/parameters.h5`. - The states of the optimizer serialized with `pickle` as as `{path}/optimizer_parameters.pkl`, allowing to resume training exactly where you left off. We hope to use HDF5 in the future but `optax` states is incompatible with `deepdish`. This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via `Model.load` if the model is already instiated or `elegy.model.load` to load the model instance from its pickled version. ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path where model structure will be saved. include_optimizer: If True, save optimizer's states together. \"\"\" if isinstance ( path , str ): path = Path ( path ) path . mkdir ( parents = True , exist_ok = True ) parameters = self . get_parameters () if \"optimizer\" in parameters : del parameters [ \"optimizer\" ] deepdish . io . save ( path / \"parameters.h5\" , parameters ) # optimizer parameters saved as a pickle because optax uses named tuples # which are converted to regular tuples by deepdish when loading. if include_optimizer and self . optimizer is not None : optimizer_params = self . optimizer . get_parameters () with open ( path / \"optimizer_parameters.pkl\" , \"wb\" ) as f : pickle . dump ( optimizer_params , f ) else : optimizer_params = None self . reset () try : path = path / \"model.pkl\" with open ( path , \"wb\" ) as f : cloudpickle . dump ( self , f ) except BaseException as e : print ( f \"Error occurred saving the model object at { path } \\n Continuing....\" ) if optimizer_params is not None : parameters [ \"optimizer\" ] = optimizer_params self . set_parameters ( parameters )","title":"save()"},{"location":"api/Model/#elegy.model.model.Model.summary","text":"Prints a summary of the network. Parameters: Name Type Description Default x A sample of inputs to the network. required depth int The level number of nested level which will be showed. Information about summaries from modules deeper than depth will be aggregated together. 2 tablefmt str A string represeting the style of the table generated by tabulate . See python-tabulate for more options. 'fancy_grid' tablulate_kwargs Additional keyword arguments passed to tabulate . See python-tabulate for more options. {} Source code in elegy/model/model.py 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 def summary ( self , x , depth : int = 2 , tablefmt : str = \"fancy_grid\" , ** tablulate_kwargs ): \"\"\" Prints a summary of the network. Arguments: x: A sample of inputs to the network. depth: The level number of nested level which will be showed. Information about summaries from modules deeper than `depth` will be aggregated together. tablefmt: A string represeting the style of the table generated by `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. tablulate_kwargs: Additional keyword arguments passed to `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. \"\"\" self . maybe_initialize ( mode = Mode . predict , x = x ) with hooks_context ( summaries = True ): self . predict_fn ( x ) summaries = get_summaries () assert summaries is not None def format_output ( outputs ) -> str : file = StringIO () outputs = jax . tree_map ( lambda x : f \" { x . shape } {{ pad }} { x . dtype } \" , outputs ) yaml . safe_dump ( outputs , file , default_flow_style = False , indent = 2 , explicit_end = False ) return file . getvalue () . replace ( \" \\n ...\" , \"\" ) def format_size ( size ): return ( f \" { size / 1e9 : ,.1f } GB\" if size > 1e9 else f \" { size / 1e6 : ,.1f } MB\" if size > 1e6 else f \" { size / 1e3 : ,.1f } KB\" if size > 1e3 else f \" { size : , } B\" ) table : tp . List = [[ \"Inputs\" , format_output ( x ), \"0\" , \"0\" ]] for module , base_name , value in summaries : base_name_parts = base_name . split ( \"/\" )[ 1 :] module_depth = len ( base_name_parts ) if module_depth > depth : continue include_submodules = module_depth == depth params_count = ( module . parameters_size ( include_submodules ) if module is not None else 0 ) params_size = ( module . parameters_bytes ( include_submodules ) if module is not None else 0 ) states_count = ( module . states_size ( include_submodules ) if module is not None else 0 ) states_size = ( module . states_bytes ( include_submodules ) if module is not None else 0 ) class_name = f \"( { module . __class__ . __name__ } )\" if module is not None else \"\" base_name = \"/\" . join ( base_name_parts ) if not base_name : base_name = \"Outputs\" table . append ( [ f \" { base_name } {{ pad }} { class_name } \" , format_output ( value ), f \" { params_count : , } {{ pad }} { format_size ( params_size ) } \" if params_count > 0 else \"0\" , f \" { states_count : , } {{ pad }} { format_size ( states_size ) } \" if states_count > 0 else \"0\" , ] ) # add papdding for col in range ( 4 ): max_length = max ( len ( line . split ( \" {pad} \" )[ 0 ]) for row in table for line in row [ col ] . split ( \" \\n \" ) ) for row in table : row [ col ] = \" \\n \" . join ( line . format ( pad = \" \" * ( max_length - len ( line . rstrip () . split ( \" {pad} \" )[ 0 ])) ) for line in row [ col ] . rstrip () . split ( \" \\n \" ) ) print ( \" \\n \" + tabulate ( table , headers = [ \"Layer\" , \"Outputs Shape\" , \"Trainable \\n Parameters\" , \"Non-trainable \\n Parameters\" , ], tablefmt = tablefmt , ** tablulate_kwargs , ) ) params_count = self . parameters_size () params_size = self . parameters_bytes () states_count = self . states_size () states_size = self . states_bytes () total_count = params_count + states_count total_size = params_size + states_size print ( tabulate ( [ [ f \"Total Parameters:\" , f \" { total_count : , } \" , f \" { format_size ( total_size ) } \" if total_count > 0 else \"\" , ], [ f \"Trainable Parameters:\" , f \" { params_count : , } \" , f \" { format_size ( params_size ) } \" if params_count > 0 else \"\" , ], [ f \"Non-trainable Parameters:\" , f \" { states_count : , } \" , f \" { format_size ( states_size ) } \" if states_count > 0 else \"\" , ], ], tablefmt = \"plain\" , ) + \" \\n \" )","title":"summary()"},{"location":"api/Model/#elegy.model.model.Model.test_on_batch","text":"Test the model on a single batch of samples. Parameters: Name Type Description Default x Union[jax._src.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[jax._src.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). None sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model/model.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def test_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ jnp . ndarray ] = None , class_weight : tp . Optional [ jnp . ndarray ] = None , ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . maybe_initialize ( mode = Mode . test , x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , ) method = self . test_step if self . run_eagerly else self . test_step_jit loss , logs , grads = method ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight ) return logs","title":"test_on_batch()"},{"location":"api/Model/#elegy.model.model.Model.train_on_batch","text":"Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). It should be consistent with x (you cannot have Numpy inputs and array targets, or inversely). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight Optional[numpy.ndarray] Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None Returns: Type Description Dict[str, numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model/model.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def train_on_batch ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ np . ndarray ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\" Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). It should be consistent with `x` (you cannot have Numpy inputs and array targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . maybe_initialize ( mode = Mode . train , x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , ) method = self . train_step if self . run_eagerly else self . train_step_jit return method ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight )","title":"train_on_batch()"},{"location":"api/Module/","text":"elegy.Module Basic Elegy Module. For more information check out the Module System guide . Attributes: Name Type Description initialized Whether or not the module is initialized. __init__ ( self , name = None , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>) special Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Parameters: Name Type Description Default name Optional[str] An optional string name for the class. Must be a valid elsePython identifier. If name is not provided then the class name for the current instance is converted to lower_snake_case and used instead. None Source code in elegy/module.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : np . dtype = jnp . float32 ): \"\"\" Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Arguments: name: An optional string name for the class. Must be a valid elsePython identifier. If ``name`` is not provided then the class name for the current instance is converted to ``lower_snake_case`` and used instead. \"\"\" self . name = name if name else utils . lower_snake_case ( self . __class__ . __name__ ) self . dtype = dtype self . _params = {} self . _states = [] self . _submodules = [] self . _dynamic_submodules = [] self . _initialized = False self . _trainable = True _init = self . init def init ( * args , ** kwargs ): return _init ( * args , ** kwargs ) self . init = init utils . wraps ( self . call )( self . init ) utils . wraps ( self . call )( self ) self . _jit_functions () add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/module.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value get_parameters ( self , trainable = None ) Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/module.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) Initializes the module, Source code in elegy/module.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/module.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/module.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"Module"},{"location":"api/Module/#elegymodule","text":"","title":"elegy.Module"},{"location":"api/Module/#elegy.module.Module","text":"Basic Elegy Module. For more information check out the Module System guide . Attributes: Name Type Description initialized Whether or not the module is initialized.","title":"elegy.module.Module"},{"location":"api/Module/#elegy.module.Module.__init__","text":"Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Parameters: Name Type Description Default name Optional[str] An optional string name for the class. Must be a valid elsePython identifier. If name is not provided then the class name for the current instance is converted to lower_snake_case and used instead. None Source code in elegy/module.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : np . dtype = jnp . float32 ): \"\"\" Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Arguments: name: An optional string name for the class. Must be a valid elsePython identifier. If ``name`` is not provided then the class name for the current instance is converted to ``lower_snake_case`` and used instead. \"\"\" self . name = name if name else utils . lower_snake_case ( self . __class__ . __name__ ) self . dtype = dtype self . _params = {} self . _states = [] self . _submodules = [] self . _dynamic_submodules = [] self . _initialized = False self . _trainable = True _init = self . init def init ( * args , ** kwargs ): return _init ( * args , ** kwargs ) self . init = init utils . wraps ( self . call )( self . init ) utils . wraps ( self . call )( self ) self . _jit_functions ()","title":"__init__()"},{"location":"api/Module/#elegy.module.Module.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/module.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/Module/#elegy.module.Module.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/module.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/Module/#elegy.module.Module.init","text":"Initializes the module, Source code in elegy/module.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/Module/#elegy.module.Module.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/module.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/Module/#elegy.module.Module.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/module.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/RNG/","text":"elegy.RNG","title":"RNG"},{"location":"api/RNG/#elegyrng","text":"","title":"elegy.RNG"},{"location":"api/RNG/#elegy.random.RNG","text":"","title":"elegy.random.RNG"},{"location":"api/add_loss/","text":"elegy.add_loss A hook that lets you define a loss within a [ module ][elegy.module.Module]. w = self . add_parameter ( \"w\" , [ 3 , 5 ], initializer = jnp . ones ) # L2 regularization penalty elegy . add_loss ( \"l2_regularization\" , 0.01 * jnp . mean ( w ** 2 )) Parameters: Name Type Description Default name str The name of the loss. If a name is repeated on different calls values will be added together. required value ndarray The value for the loss. required Source code in elegy/module.py 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def add_loss ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a loss within a [`module`][elegy.module.Module]. ```python w = self.add_parameter(\"w\", [3, 5], initializer=jnp.ones) # L2 regularization penalty elegy.add_loss(\"l2_regularization\", 0.01 * jnp.mean(w ** 2)) ``` Arguments: name: The name of the loss. If a `name` is repeated on different calls values will be added together. value: The value for the loss. \"\"\" if LOCAL . losses is None : return if not name . endswith ( \"loss\" ): name += \"_loss\" if name in LOCAL . losses : LOCAL . losses [ name ] += value else : LOCAL . losses [ name ] = value","title":"add_loss"},{"location":"api/add_loss/#elegyadd_loss","text":"","title":"elegy.add_loss"},{"location":"api/add_loss/#elegy.module.add_loss","text":"A hook that lets you define a loss within a [ module ][elegy.module.Module]. w = self . add_parameter ( \"w\" , [ 3 , 5 ], initializer = jnp . ones ) # L2 regularization penalty elegy . add_loss ( \"l2_regularization\" , 0.01 * jnp . mean ( w ** 2 )) Parameters: Name Type Description Default name str The name of the loss. If a name is repeated on different calls values will be added together. required value ndarray The value for the loss. required Source code in elegy/module.py 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def add_loss ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a loss within a [`module`][elegy.module.Module]. ```python w = self.add_parameter(\"w\", [3, 5], initializer=jnp.ones) # L2 regularization penalty elegy.add_loss(\"l2_regularization\", 0.01 * jnp.mean(w ** 2)) ``` Arguments: name: The name of the loss. If a `name` is repeated on different calls values will be added together. value: The value for the loss. \"\"\" if LOCAL . losses is None : return if not name . endswith ( \"loss\" ): name += \"_loss\" if name in LOCAL . losses : LOCAL . losses [ name ] += value else : LOCAL . losses [ name ] = value","title":"elegy.module.add_loss"},{"location":"api/add_metric/","text":"elegy.add_metric A hook that lets you define a metric within a [ module ][elegy.module.Module]. y = jax . nn . relu ( x ) elegy . add_metric ( \"activation_mean\" , jnp . mean ( y )) Parameters: Name Type Description Default name str The name of the loss. If a metric with the same name already exists a unique identifier will be generated. required value ndarray The value for the metric. required Source code in elegy/module.py 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 def add_metric ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a metric within a [`module`][elegy.module.Module]. ```python y = jax.nn.relu(x) elegy.add_metric(\"activation_mean\", jnp.mean(y)) ``` Arguments: name: The name of the loss. If a metric with the same `name` already exists a unique identifier will be generated. value: The value for the metric. \"\"\" if LOCAL . metrics is None : return name = f \" { base_name () } / { name } \" name = get_unique_name ( set ( LOCAL . metrics ), name ) LOCAL . metrics [ name ] = value","title":"add_metric"},{"location":"api/add_metric/#elegyadd_metric","text":"","title":"elegy.add_metric"},{"location":"api/add_metric/#elegy.module.add_metric","text":"A hook that lets you define a metric within a [ module ][elegy.module.Module]. y = jax . nn . relu ( x ) elegy . add_metric ( \"activation_mean\" , jnp . mean ( y )) Parameters: Name Type Description Default name str The name of the loss. If a metric with the same name already exists a unique identifier will be generated. required value ndarray The value for the metric. required Source code in elegy/module.py 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 def add_metric ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a metric within a [`module`][elegy.module.Module]. ```python y = jax.nn.relu(x) elegy.add_metric(\"activation_mean\", jnp.mean(y)) ``` Arguments: name: The name of the loss. If a metric with the same `name` already exists a unique identifier will be generated. value: The value for the metric. \"\"\" if LOCAL . metrics is None : return name = f \" { base_name () } / { name } \" name = get_unique_name ( set ( LOCAL . metrics ), name ) LOCAL . metrics [ name ] = value","title":"elegy.module.add_metric"},{"location":"api/add_summary/","text":"elegy.add_summary A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so [ Model.summary ][elegy.model.model.Model.summary] can show a representation of architecture. def call ( self , x ): ... y = jax . nn . relu ( x ) elegy . add_summary ( \"relu\" , y ) ... Parameters: Name Type Description Default module_or_name Union[elegy.module.Module, str] The name of the summary or alternatively the module that this summary will represent. If a summary with the same name already exists a unique identifier will be generated. required value ndarray The value for the summary. required Source code in elegy/module.py 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 def add_summary ( module_or_name : tp . Union [ Module , str ], value : np . ndarray ) -> None : \"\"\" A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so [`Model.summary`][elegy.model.model.Model.summary] can show a representation of architecture. ```python def call(self, x): ... y = jax.nn.relu(x) elegy.add_summary(\"relu\", y) ... ``` Arguments: module_or_name: The name of the summary or alternatively the module that this summary will represent. If a summary with the same name already exists a unique identifier will be generated. value: The value for the summary. \"\"\" if LOCAL . summaries is None : return name = base_name () if isinstance ( module_or_name , str ): name = f \" { name } / { module_or_name } \" if name else module_or_name name = get_unique_name ({ t [ 1 ] for t in LOCAL . summaries }, name ) module = None else : module = module_or_name LOCAL . summaries . append (( module , name , value ))","title":"add_summary"},{"location":"api/add_summary/#elegyadd_summary","text":"","title":"elegy.add_summary"},{"location":"api/add_summary/#elegy.module.add_summary","text":"A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so [ Model.summary ][elegy.model.model.Model.summary] can show a representation of architecture. def call ( self , x ): ... y = jax . nn . relu ( x ) elegy . add_summary ( \"relu\" , y ) ... Parameters: Name Type Description Default module_or_name Union[elegy.module.Module, str] The name of the summary or alternatively the module that this summary will represent. If a summary with the same name already exists a unique identifier will be generated. required value ndarray The value for the summary. required Source code in elegy/module.py 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 def add_summary ( module_or_name : tp . Union [ Module , str ], value : np . ndarray ) -> None : \"\"\" A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so [`Model.summary`][elegy.model.model.Model.summary] can show a representation of architecture. ```python def call(self, x): ... y = jax.nn.relu(x) elegy.add_summary(\"relu\", y) ... ``` Arguments: module_or_name: The name of the summary or alternatively the module that this summary will represent. If a summary with the same name already exists a unique identifier will be generated. value: The value for the summary. \"\"\" if LOCAL . summaries is None : return name = base_name () if isinstance ( module_or_name , str ): name = f \" { name } / { module_or_name } \" if name else module_or_name name = get_unique_name ({ t [ 1 ] for t in LOCAL . summaries }, name ) module = None else : module = module_or_name LOCAL . summaries . append (( module , name , value ))","title":"elegy.module.add_summary"},{"location":"api/get_dynamic_context/","text":"elegy.get_dynamic_context Source code in elegy/module.py 882 883 def get_dynamic_context () -> \"DynamicContext\" : return LOCAL . dynamic_context ()","title":"get_dynamic_context"},{"location":"api/get_dynamic_context/#elegyget_dynamic_context","text":"","title":"elegy.get_dynamic_context"},{"location":"api/get_dynamic_context/#elegy.module.get_dynamic_context","text":"Source code in elegy/module.py 882 883 def get_dynamic_context () -> \"DynamicContext\" : return LOCAL . dynamic_context ()","title":"elegy.module.get_dynamic_context"},{"location":"api/get_losses/","text":"elegy.get_losses Source code in elegy/module.py 702 703 def get_losses () -> tp . Optional [ tp . Dict [ str , tp . Any ]]: return LOCAL . losses","title":"get_losses"},{"location":"api/get_losses/#elegyget_losses","text":"","title":"elegy.get_losses"},{"location":"api/get_losses/#elegy.module.get_losses","text":"Source code in elegy/module.py 702 703 def get_losses () -> tp . Optional [ tp . Dict [ str , tp . Any ]]: return LOCAL . losses","title":"elegy.module.get_losses"},{"location":"api/get_metrics/","text":"elegy.get_metrics Source code in elegy/module.py 706 707 def get_metrics () -> tp . Optional [ tp . Dict [ str , tp . Any ]]: return LOCAL . metrics","title":"get_metrics"},{"location":"api/get_metrics/#elegyget_metrics","text":"","title":"elegy.get_metrics"},{"location":"api/get_metrics/#elegy.module.get_metrics","text":"Source code in elegy/module.py 706 707 def get_metrics () -> tp . Optional [ tp . Dict [ str , tp . Any ]]: return LOCAL . metrics","title":"elegy.module.get_metrics"},{"location":"api/get_rng/","text":"elegy.get_rng Source code in elegy/module.py 670 671 672 def get_rng () -> RNG : \"\"\"\"\"\" return LOCAL . rng","title":"get_rng"},{"location":"api/get_rng/#elegyget_rng","text":"","title":"elegy.get_rng"},{"location":"api/get_rng/#elegy.module.get_rng","text":"Source code in elegy/module.py 670 671 672 def get_rng () -> RNG : \"\"\"\"\"\" return LOCAL . rng","title":"elegy.module.get_rng"},{"location":"api/get_static_context/","text":"elegy.get_static_context Source code in elegy/module.py 886 887 def get_static_context () -> \"StaticContext\" : return LOCAL . static_context ()","title":"get_static_context"},{"location":"api/get_static_context/#elegyget_static_context","text":"","title":"elegy.get_static_context"},{"location":"api/get_static_context/#elegy.module.get_static_context","text":"Source code in elegy/module.py 886 887 def get_static_context () -> \"StaticContext\" : return LOCAL . static_context ()","title":"elegy.module.get_static_context"},{"location":"api/get_summaries/","text":"elegy.get_summaries Source code in elegy/module.py 710 711 712 713 def get_summaries () -> tp . Optional [ tp . List [ tp . Tuple [ tp . Optional [ \"Module\" ], str , np . ndarray ]] ]: return LOCAL . summaries","title":"get_summaries"},{"location":"api/get_summaries/#elegyget_summaries","text":"","title":"elegy.get_summaries"},{"location":"api/get_summaries/#elegy.module.get_summaries","text":"Source code in elegy/module.py 710 711 712 713 def get_summaries () -> tp . Optional [ tp . List [ tp . Tuple [ tp . Optional [ \"Module\" ], str , np . ndarray ]] ]: return LOCAL . summaries","title":"elegy.module.get_summaries"},{"location":"api/hooks_context/","text":"elegy.hooks_context Source code in elegy/module.py 725 726 727 728 def hooks_context ( summaries : bool = False , ) -> tp . ContextManager [ None ]: return _hooks_context ( summaries = summaries )","title":"hooks_context"},{"location":"api/hooks_context/#elegyhooks_context","text":"","title":"elegy.hooks_context"},{"location":"api/hooks_context/#elegy.module.hooks_context","text":"Source code in elegy/module.py 725 726 727 728 def hooks_context ( summaries : bool = False , ) -> tp . ContextManager [ None ]: return _hooks_context ( summaries = summaries )","title":"elegy.module.hooks_context"},{"location":"api/is_training/","text":"elegy.is_training Source code in elegy/module.py 698 699 def is_training () -> bool : return LOCAL . training","title":"is_training"},{"location":"api/is_training/#elegyis_training","text":"","title":"elegy.is_training"},{"location":"api/is_training/#elegy.module.is_training","text":"Source code in elegy/module.py 698 699 def is_training () -> bool : return LOCAL . training","title":"elegy.module.is_training"},{"location":"api/jit/","text":"elegy.jit Source code in elegy/module.py 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 def jit ( f : tp . Union [ tp . Callable , Module ], modules : tp . Optional [ tp . Union [ Module , tp . List [ Module ]]] = None , ** kwargs , ): static_argnums = tuple ( kwargs . pop ( \"static_argnums\" , ())) if modules is None : modules = [] elif isinstance ( modules , Module ): modules = [ modules ] if isinstance ( f , Module ): if modules is not None and f not in modules : modules . append ( f ) elif modules is None : modules = [ f ] if len ( modules ) < 1 : raise ValueError ( \"No module specified\" ) static_argnums = ( 0 , 1 ) + tuple ( i + 4 for i in static_argnums ) def _jit_fn ( states_tuple : tp . Tuple [ FrozenDict [ str , tp . Any ], ... ], statics : StaticContext , dynamics : DynamicContext , parameters_tuple : tp . Tuple [ tp . Dict , ... ], * args , ) -> tp . Tuple [ tp . Any , DynamicContext , tp . Tuple ]: assert isinstance ( modules , list ) # states_tuple is not set because its static, therefore no need to propagate down # set global state set_context ( statics , dynamics ) # set params to modules for module , parameters in zip ( modules , parameters_tuple ): module . set_parameters ( parameters ) outputs = f ( * args ) parameters_tuple = tuple ( module . get_parameters () for module in modules ) return ( outputs , get_dynamic_context (), parameters_tuple , ) jit_fn = jax . jit ( _jit_fn , static_argnums , ** kwargs ) @functools . wraps ( f ) def wrapper ( * args ): assert isinstance ( modules , list ) states_tuple = utils . to_static ( tuple ( FrozenDict ( module . _get_module_states ()) for module in modules ) ) statics = get_static_context () dynamics = get_dynamic_context () parameters_tuple = tuple ( module . get_parameters () for module in modules ) # static_argnums outputs , dynamics , parameters_tuple = jit_fn ( states_tuple , statics , dynamics , parameters_tuple , * args , ) statics = get_static_context () # set global state set_context ( statics , dynamics ) # set params to modules for module , parameters in zip ( modules , parameters_tuple ): module . set_parameters ( parameters ) return outputs return wrapper","title":"jit"},{"location":"api/jit/#elegyjit","text":"","title":"elegy.jit"},{"location":"api/jit/#elegy.module.jit","text":"Source code in elegy/module.py 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 def jit ( f : tp . Union [ tp . Callable , Module ], modules : tp . Optional [ tp . Union [ Module , tp . List [ Module ]]] = None , ** kwargs , ): static_argnums = tuple ( kwargs . pop ( \"static_argnums\" , ())) if modules is None : modules = [] elif isinstance ( modules , Module ): modules = [ modules ] if isinstance ( f , Module ): if modules is not None and f not in modules : modules . append ( f ) elif modules is None : modules = [ f ] if len ( modules ) < 1 : raise ValueError ( \"No module specified\" ) static_argnums = ( 0 , 1 ) + tuple ( i + 4 for i in static_argnums ) def _jit_fn ( states_tuple : tp . Tuple [ FrozenDict [ str , tp . Any ], ... ], statics : StaticContext , dynamics : DynamicContext , parameters_tuple : tp . Tuple [ tp . Dict , ... ], * args , ) -> tp . Tuple [ tp . Any , DynamicContext , tp . Tuple ]: assert isinstance ( modules , list ) # states_tuple is not set because its static, therefore no need to propagate down # set global state set_context ( statics , dynamics ) # set params to modules for module , parameters in zip ( modules , parameters_tuple ): module . set_parameters ( parameters ) outputs = f ( * args ) parameters_tuple = tuple ( module . get_parameters () for module in modules ) return ( outputs , get_dynamic_context (), parameters_tuple , ) jit_fn = jax . jit ( _jit_fn , static_argnums , ** kwargs ) @functools . wraps ( f ) def wrapper ( * args ): assert isinstance ( modules , list ) states_tuple = utils . to_static ( tuple ( FrozenDict ( module . _get_module_states ()) for module in modules ) ) statics = get_static_context () dynamics = get_dynamic_context () parameters_tuple = tuple ( module . get_parameters () for module in modules ) # static_argnums outputs , dynamics , parameters_tuple = jit_fn ( states_tuple , statics , dynamics , parameters_tuple , * args , ) statics = get_static_context () # set global state set_context ( statics , dynamics ) # set params to modules for module , parameters in zip ( modules , parameters_tuple ): module . set_parameters ( parameters ) return outputs return wrapper","title":"elegy.module.jit"},{"location":"api/name_context/","text":"elegy.name_context Source code in elegy/module.py 793 794 def name_context ( name : str ) -> tp . ContextManager [ str ]: return _name_context ( name )","title":"name_context"},{"location":"api/name_context/#elegyname_context","text":"","title":"elegy.name_context"},{"location":"api/name_context/#elegy.module.name_context","text":"Source code in elegy/module.py 793 794 def name_context ( name : str ) -> tp . ContextManager [ str ]: return _name_context ( name )","title":"elegy.module.name_context"},{"location":"api/next_rng_key/","text":"elegy.next_rng_key Returns a key usable with jax.random.* functions. Source code in elegy/module.py 679 680 681 682 683 def next_rng_key () -> jnp . ndarray : \"\"\" Returns a key usable with `jax.random.*` functions. \"\"\" return LOCAL . rng ()","title":"next_rng_key"},{"location":"api/next_rng_key/#elegynext_rng_key","text":"","title":"elegy.next_rng_key"},{"location":"api/next_rng_key/#elegy.module.next_rng_key","text":"Returns a key usable with jax.random.* functions. Source code in elegy/module.py 679 680 681 682 683 def next_rng_key () -> jnp . ndarray : \"\"\" Returns a key usable with `jax.random.*` functions. \"\"\" return LOCAL . rng ()","title":"elegy.module.next_rng_key"},{"location":"api/set_context/","text":"elegy.set_context Source code in elegy/module.py 890 891 892 893 894 895 def set_context ( static : \"StaticContext\" , dynamic : \"DynamicContext\" ): LOCAL . set_from ( static , dynamic ) def _grad_fn ( parameters_tuple : tp . Tuple [ tp . Dict , ... ], * args , ** kwargs ): assert isinstance ( parameters_tuple , tuple ) assert isinstance ( modules , list )","title":"set_context"},{"location":"api/set_context/#elegyset_context","text":"","title":"elegy.set_context"},{"location":"api/set_context/#elegy.module.set_context","text":"Source code in elegy/module.py 890 891 892 893 894 895 def set_context ( static : \"StaticContext\" , dynamic : \"DynamicContext\" ): LOCAL . set_from ( static , dynamic ) def _grad_fn ( parameters_tuple : tp . Tuple [ tp . Dict , ... ], * args , ** kwargs ): assert isinstance ( parameters_tuple , tuple ) assert isinstance ( modules , list )","title":"elegy.module.set_context"},{"location":"api/set_rng/","text":"elegy.set_rng Source code in elegy/module.py 675 676 def set_rng ( rng : RNG ) -> None : LOCAL . rng = rng","title":"set_rng"},{"location":"api/set_rng/#elegyset_rng","text":"","title":"elegy.set_rng"},{"location":"api/set_rng/#elegy.module.set_rng","text":"Source code in elegy/module.py 675 676 def set_rng ( rng : RNG ) -> None : LOCAL . rng = rng","title":"elegy.module.set_rng"},{"location":"api/set_training/","text":"elegy.set_training Source code in elegy/module.py 694 695 def set_training ( training : bool ) -> None : LOCAL . training = training","title":"set_training"},{"location":"api/set_training/#elegyset_training","text":"","title":"elegy.set_training"},{"location":"api/set_training/#elegy.module.set_training","text":"Source code in elegy/module.py 694 695 def set_training ( training : bool ) -> None : LOCAL . training = training","title":"elegy.module.set_training"},{"location":"api/to_module/","text":"elegy.to_module Source code in elegy/module.py 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 def to_module ( f ): class ToModule ( Module ): def __init__ ( self , name : tp . Optional [ str ] = None ): super () . __init__ ( name = utils . lower_snake_case ( f . __name__ ) if name is None else name ) self . call = f def call ( self , * args , ** kwargs ): ... ToModule . __name__ = f . __name__ return ToModule","title":"to_module"},{"location":"api/to_module/#elegyto_module","text":"","title":"elegy.to_module"},{"location":"api/to_module/#elegy.module.to_module","text":"Source code in elegy/module.py 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 def to_module ( f ): class ToModule ( Module ): def __init__ ( self , name : tp . Optional [ str ] = None ): super () . __init__ ( name = utils . lower_snake_case ( f . __name__ ) if name is None else name ) self . call = f def call ( self , * args , ** kwargs ): ... ToModule . __name__ = f . __name__ return ToModule","title":"elegy.module.to_module"},{"location":"api/training_context/","text":"elegy.training_context Source code in elegy/module.py 778 779 def training_context ( training : bool ) -> tp . ContextManager [ None ]: return _training_context ( training )","title":"training_context"},{"location":"api/training_context/#elegytraining_context","text":"","title":"elegy.training_context"},{"location":"api/training_context/#elegy.module.training_context","text":"Source code in elegy/module.py 778 779 def training_context ( training : bool ) -> tp . ContextManager [ None ]: return _training_context ( training )","title":"elegy.module.training_context"},{"location":"api/value_and_grad/","text":"elegy.value_and_grad Source code in elegy/module.py 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 def value_and_grad ( f : tp . Union [ tp . Callable , Module ], modules : tp . Optional [ tp . Union [ Module , tp . List [ Module ]]] = None , parameters_fn : tp . Callable [ [ tp . List [ Module ]], tp . List [ tp . Any ] ] = get_trainable_parameters , ** kwargs , ): is_list = isinstance ( modules , tp . List ) if modules is None : modules = [] elif isinstance ( modules , Module ): modules = [ modules ] if isinstance ( f , Module ): if modules is not None and f not in modules : modules . append ( f ) elif modules is None : modules = [ f ] assert len ( modules ) > 0 def _grad_fn ( parameters_tuple : tp . Tuple [ tp . Dict , ... ], * args , ** kwargs ): assert isinstance ( parameters_tuple , tuple ) assert isinstance ( modules , list ) # set traced parameters for module , parameters in zip ( modules , parameters_tuple ): module . set_parameters ( parameters ) outputs = f ( * args , ** kwargs ) loss = outputs [ 0 ] if isinstance ( outputs , tuple ) else outputs parameters_tuple = tuple ( module . get_parameters () for module in modules ) return loss , ( outputs , get_static_context (), get_dynamic_context (), parameters_tuple , ) kwargs [ \"has_aux\" ] = True grad_fn = jax . value_and_grad ( _grad_fn , ** kwargs ) @functools . wraps ( f ) def wrapper ( * args , ** kwargs ): assert isinstance ( modules , list ) parameters_tuple = tuple ( parameters_fn ( modules )) ( _loss , ( outputs , statics , dynamics , parameters_tuple )), grads = grad_fn ( parameters_tuple , * args , ** kwargs ) parameters_tuple # set global state set_context ( statics , dynamics ) # set original untraced parameters for module , parameters in zip ( modules , parameters_tuple ): module . set_parameters ( parameters ) if not is_list : grads = grads [ 0 ] return outputs , grads return wrapper","title":"value_and_grad"},{"location":"api/value_and_grad/#elegyvalue_and_grad","text":"","title":"elegy.value_and_grad"},{"location":"api/value_and_grad/#elegy.module.value_and_grad","text":"Source code in elegy/module.py 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 def value_and_grad ( f : tp . Union [ tp . Callable , Module ], modules : tp . Optional [ tp . Union [ Module , tp . List [ Module ]]] = None , parameters_fn : tp . Callable [ [ tp . List [ Module ]], tp . List [ tp . Any ] ] = get_trainable_parameters , ** kwargs , ): is_list = isinstance ( modules , tp . List ) if modules is None : modules = [] elif isinstance ( modules , Module ): modules = [ modules ] if isinstance ( f , Module ): if modules is not None and f not in modules : modules . append ( f ) elif modules is None : modules = [ f ] assert len ( modules ) > 0 def _grad_fn ( parameters_tuple : tp . Tuple [ tp . Dict , ... ], * args , ** kwargs ): assert isinstance ( parameters_tuple , tuple ) assert isinstance ( modules , list ) # set traced parameters for module , parameters in zip ( modules , parameters_tuple ): module . set_parameters ( parameters ) outputs = f ( * args , ** kwargs ) loss = outputs [ 0 ] if isinstance ( outputs , tuple ) else outputs parameters_tuple = tuple ( module . get_parameters () for module in modules ) return loss , ( outputs , get_static_context (), get_dynamic_context (), parameters_tuple , ) kwargs [ \"has_aux\" ] = True grad_fn = jax . value_and_grad ( _grad_fn , ** kwargs ) @functools . wraps ( f ) def wrapper ( * args , ** kwargs ): assert isinstance ( modules , list ) parameters_tuple = tuple ( parameters_fn ( modules )) ( _loss , ( outputs , statics , dynamics , parameters_tuple )), grads = grad_fn ( parameters_tuple , * args , ** kwargs ) parameters_tuple # set global state set_context ( statics , dynamics ) # set original untraced parameters for module , parameters in zip ( modules , parameters_tuple ): module . set_parameters ( parameters ) if not is_list : grads = grads [ 0 ] return outputs , grads return wrapper","title":"elegy.module.value_and_grad"},{"location":"api/callbacks/CSVLogger/","text":"elegy.callbacks.CSVLogger Callback that streams epoch results to a csv file. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . Examples: csv_logger = CSVLogger ( 'training.log' ) model . fit ( X_train , Y_train , callbacks = [ csv_logger ]) on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/csv_logger.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} def handle_value ( k ): is_zero_dim_ndarray = isinstance ( k , np . ndarray ) and k . ndim == 0 if isinstance ( k , six . string_types ): return k elif isinstance ( k , tp . Iterable ) and not is_zero_dim_ndarray : return '\"[ %s ]\"' % ( \", \" . join ( map ( str , k ))) else : return k if self . keys is None : self . keys = sorted ( logs . keys ()) if self . model . stop_training : # We set NA so that csv parsers do not fail for this last epoch. logs = dict ([( k , logs [ k ]) if k in logs else ( k , \"NA\" ) for k in self . keys ]) if not self . writer : class CustomDialect ( csv . excel ): delimiter = self . sep fieldnames = [ \"epoch\" ] + self . keys self . writer = csv . DictWriter ( self . csv_file , fieldnames = fieldnames , dialect = CustomDialect ) if self . append_header : self . writer . writeheader () row_dict = collections . OrderedDict ({ \"epoch\" : epoch }) row_dict . update (( key , handle_value ( logs [ key ])) for key in self . keys ) self . writer . writerow ( row_dict ) self . csv_file . flush () on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 48 49 50 51 52 53 54 55 56 57 58 def on_train_begin ( self , logs = None ): if self . append : if os . path . exists ( self . filename ): with open ( self . filename , \"r\" + self . file_flags ) as f : self . append_header = not bool ( len ( f . readline ())) mode = \"a\" else : mode = \"w\" self . csv_file = io . open ( self . filename , mode + self . file_flags , ** self . _open_args ) on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 97 98 99 def on_train_end ( self , logs = None ): self . csv_file . close () self . writer = None","title":"CSVLogger"},{"location":"api/callbacks/CSVLogger/#elegycallbackscsvlogger","text":"","title":"elegy.callbacks.CSVLogger"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger","text":"Callback that streams epoch results to a csv file. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . Examples: csv_logger = CSVLogger ( 'training.log' ) model . fit ( X_train , Y_train , callbacks = [ csv_logger ])","title":"elegy.callbacks.csv_logger.CSVLogger"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/csv_logger.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} def handle_value ( k ): is_zero_dim_ndarray = isinstance ( k , np . ndarray ) and k . ndim == 0 if isinstance ( k , six . string_types ): return k elif isinstance ( k , tp . Iterable ) and not is_zero_dim_ndarray : return '\"[ %s ]\"' % ( \", \" . join ( map ( str , k ))) else : return k if self . keys is None : self . keys = sorted ( logs . keys ()) if self . model . stop_training : # We set NA so that csv parsers do not fail for this last epoch. logs = dict ([( k , logs [ k ]) if k in logs else ( k , \"NA\" ) for k in self . keys ]) if not self . writer : class CustomDialect ( csv . excel ): delimiter = self . sep fieldnames = [ \"epoch\" ] + self . keys self . writer = csv . DictWriter ( self . csv_file , fieldnames = fieldnames , dialect = CustomDialect ) if self . append_header : self . writer . writeheader () row_dict = collections . OrderedDict ({ \"epoch\" : epoch }) row_dict . update (( key , handle_value ( logs [ key ])) for key in self . keys ) self . writer . writerow ( row_dict ) self . csv_file . flush ()","title":"on_epoch_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 48 49 50 51 52 53 54 55 56 57 58 def on_train_begin ( self , logs = None ): if self . append : if os . path . exists ( self . filename ): with open ( self . filename , \"r\" + self . file_flags ) as f : self . append_header = not bool ( len ( f . readline ())) mode = \"a\" else : mode = \"w\" self . csv_file = io . open ( self . filename , mode + self . file_flags , ** self . _open_args )","title":"on_train_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 97 98 99 def on_train_end ( self , logs = None ): self . csv_file . close () self . writer = None","title":"on_train_end()"},{"location":"api/callbacks/Callback/","text":"elegy.callbacks.Callback Abstract base class used to build new callbacks. The logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch. Currently, the .fit() method of the Model class will include the following quantities in the logs that it passes to its callbacks: on_epoch_end : logs include ` acc ` and ` loss ` , and optionally include ` val_loss ` ( if validation is enabled in ` fit ` ), and ` val_acc ` ( if validation and accuracy monitoring are enabled ) . on_train_batch_begin : logs include ` size ` , the number of samples in the current batch . on_train_batch_end : logs include ` loss ` , and optionally ` acc ` ( if accuracy monitoring is enabled ) . Attributes: Name Type Description params dict Training parameters (eg. verbosity, batch size, number of epochs...). model elegy.model.Model Reference of the model being trained. on_epoch_begin ( self , epoch , logs = None ) Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/callback.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass on_predict_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"Callback"},{"location":"api/callbacks/Callback/#elegycallbackscallback","text":"","title":"elegy.callbacks.Callback"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback","text":"Abstract base class used to build new callbacks. The logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch. Currently, the .fit() method of the Model class will include the following quantities in the logs that it passes to its callbacks: on_epoch_end : logs include ` acc ` and ` loss ` , and optionally include ` val_loss ` ( if validation is enabled in ` fit ` ), and ` val_acc ` ( if validation and accuracy monitoring are enabled ) . on_train_batch_begin : logs include ` size ` , the number of samples in the current batch . on_train_batch_end : logs include ` loss ` , and optionally ` acc ` ( if accuracy monitoring is enabled ) . Attributes: Name Type Description params dict Training parameters (eg. verbosity, batch size, number of epochs...). model elegy.model.Model Reference of the model being trained.","title":"elegy.callbacks.callback.Callback"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/callback.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass","title":"on_epoch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/CallbackList/","text":"elegy.callbacks.CallbackList Container abstracting a list of callbacks. __init__ ( self , callbacks = None , add_history = False , add_progbar = False , model = None , ** params ) special Creates a container for Callbacks . Parameters: Name Type Description Default callbacks Optional[List[elegy.callbacks.callback.Callback]] List of Callback instances. None add_history bool Whether a History callback should be added, if one does not already exist in callback s. False add_progbar bool Whether a ProgbarLogger callback should be added, if one does not already exist in callback s. False model Optional[Any] The Model these Callback s are used with.` None **params If provided, parameters will be passed to each Callback via Callback.set_params . {} Source code in elegy/callbacks/callback_list.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , callbacks : tp . Optional [ tp . List [ Callback ]] = None , add_history : bool = False , add_progbar : bool = False , model : tp . Optional [ tp . Any ] = None , ** params ): \"\"\"Creates a container for `Callbacks`. Arguments: callbacks: List of `Callback` instances. add_history: Whether a `History` callback should be added, if one does not already exist in `callback`s. add_progbar: Whether a `ProgbarLogger` callback should be added, if one does not already exist in `callback`s. model: The `Model` these `Callback`s are used with.` **params: If provided, parameters will be passed to each `Callback` via `Callback.set_params`. \"\"\" self . callbacks = callbacks if callbacks else [] self . _add_default_callbacks ( add_history , add_progbar ) if model : self . set_model ( model ) if params : self . set_params ( params ) self . _queue_length = 10 self . _reset_batch_timing () # Determines if batch-level hooks need to be called. # This is important for performance, because processing batch-level logs # will cause async eager to block on each batch. # pylint: disable=protected-access self . _should_call_train_batch_hooks = any ( cb . _implements_train_batch_hooks () for cb in self . callbacks ) self . _should_call_test_batch_hooks = any ( cb . _implements_test_batch_hooks () for cb in self . callbacks ) self . _should_call_predict_batch_hooks = any ( cb . _implements_predict_batch_hooks () for cb in self . callbacks ) on_epoch_begin ( self , epoch , logs = None ) Calls the on_epoch_begin methods of its callbacks. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def on_epoch_begin ( self , epoch , logs = None ): \"\"\"Calls the `on_epoch_begin` methods of its callbacks. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_epoch_begin ( epoch , logs ) self . _reset_batch_timing () on_epoch_end ( self , epoch , logs = None ) Calls the on_epoch_end methods of its callbacks. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/callback_list.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def on_epoch_end ( self , epoch , logs = None ): \"\"\"Calls the `on_epoch_end` methods of its callbacks. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_epoch_end ( epoch , logs ) on_predict_batch_begin ( self , batch , logs = None ) Calls the on_predict_batch_begin methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback_list.py 263 264 265 266 267 268 269 270 271 272 273 def on_predict_batch_begin ( self , batch , logs = None ): \"\"\"Calls the `on_predict_batch_begin` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" if self . _should_call_predict_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . PREDICT , \"begin\" , batch , logs = logs ) on_predict_batch_end ( self , batch , logs = None ) Calls the on_predict_batch_end methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/callback_list.py 275 276 277 278 279 280 281 282 283 284 def on_predict_batch_end ( self , batch , logs = None ): \"\"\"Calls the `on_predict_batch_end` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" if self . _should_call_predict_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . PREDICT , \"end\" , batch , logs = logs ) on_predict_begin ( self , logs = None ) Calls the 'on_predict_begin` methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 330 331 332 333 334 335 336 337 338 339 def on_predict_begin ( self , logs = None ): \"\"\"Calls the 'on_predict_begin` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_predict_begin ( logs ) on_predict_end ( self , logs = None ) Calls the on_predict_end methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 341 342 343 344 345 346 347 348 349 350 def on_predict_end ( self , logs = None ): \"\"\"Calls the `on_predict_end` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_predict_end ( logs ) on_test_batch_begin ( self , batch , logs = None ) Calls the on_test_batch_begin methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback_list.py 240 241 242 243 244 245 246 247 248 249 250 def on_test_batch_begin ( self , batch , logs = None ): \"\"\"Calls the `on_test_batch_begin` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" if self . _should_call_test_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TEST , \"begin\" , batch , logs = logs ) on_test_batch_end ( self , batch , logs = None ) Calls the on_test_batch_end methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/callback_list.py 252 253 254 255 256 257 258 259 260 261 def on_test_batch_end ( self , batch , logs = None ): \"\"\"Calls the `on_test_batch_end` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" if self . _should_call_test_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TEST , \"end\" , batch , logs = logs ) on_test_begin ( self , logs = None ) Calls the on_test_begin methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 308 309 310 311 312 313 314 315 316 317 def on_test_begin ( self , logs = None ): \"\"\"Calls the `on_test_begin` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_test_begin ( logs ) on_test_end ( self , logs = None ) Calls the on_test_end methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 319 320 321 322 323 324 325 326 327 328 def on_test_end ( self , logs = None ): \"\"\"Calls the `on_test_end` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_test_end ( logs ) on_train_batch_begin ( self , batch , logs = None ) Calls the on_train_batch_begin methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback_list.py 215 216 217 218 219 220 221 222 223 224 225 226 227 def on_train_batch_begin ( self , batch , logs = None ): \"\"\"Calls the `on_train_batch_begin` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" # TODO(b/150629188): Make ProgBarLogger callback not use batch hooks # when verbose != 1 if self . _should_call_train_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TRAIN , \"begin\" , batch , logs = logs ) on_train_batch_end ( self , batch , logs = None ) Calls the on_train_batch_end methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/callback_list.py 229 230 231 232 233 234 235 236 237 238 def on_train_batch_end ( self , batch , logs = None ): \"\"\"Calls the `on_train_batch_end` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" if self . _should_call_train_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TRAIN , \"end\" , batch , logs = logs ) on_train_begin ( self , logs = None ) Calls the on_train_begin methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 286 287 288 289 290 291 292 293 294 295 def on_train_begin ( self , logs = None ): \"\"\"Calls the `on_train_begin` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_train_begin ( logs ) on_train_end ( self , logs = None ) Calls the on_train_end methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 297 298 299 300 301 302 303 304 305 306 def on_train_end ( self , logs = None ): \"\"\"Calls the `on_train_end` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_train_end ( logs )","title":"CallbackList"},{"location":"api/callbacks/CallbackList/#elegycallbackscallbacklist","text":"","title":"elegy.callbacks.CallbackList"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList","text":"Container abstracting a list of callbacks.","title":"elegy.callbacks.callback_list.CallbackList"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.__init__","text":"Creates a container for Callbacks . Parameters: Name Type Description Default callbacks Optional[List[elegy.callbacks.callback.Callback]] List of Callback instances. None add_history bool Whether a History callback should be added, if one does not already exist in callback s. False add_progbar bool Whether a ProgbarLogger callback should be added, if one does not already exist in callback s. False model Optional[Any] The Model these Callback s are used with.` None **params If provided, parameters will be passed to each Callback via Callback.set_params . {} Source code in elegy/callbacks/callback_list.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , callbacks : tp . Optional [ tp . List [ Callback ]] = None , add_history : bool = False , add_progbar : bool = False , model : tp . Optional [ tp . Any ] = None , ** params ): \"\"\"Creates a container for `Callbacks`. Arguments: callbacks: List of `Callback` instances. add_history: Whether a `History` callback should be added, if one does not already exist in `callback`s. add_progbar: Whether a `ProgbarLogger` callback should be added, if one does not already exist in `callback`s. model: The `Model` these `Callback`s are used with.` **params: If provided, parameters will be passed to each `Callback` via `Callback.set_params`. \"\"\" self . callbacks = callbacks if callbacks else [] self . _add_default_callbacks ( add_history , add_progbar ) if model : self . set_model ( model ) if params : self . set_params ( params ) self . _queue_length = 10 self . _reset_batch_timing () # Determines if batch-level hooks need to be called. # This is important for performance, because processing batch-level logs # will cause async eager to block on each batch. # pylint: disable=protected-access self . _should_call_train_batch_hooks = any ( cb . _implements_train_batch_hooks () for cb in self . callbacks ) self . _should_call_test_batch_hooks = any ( cb . _implements_test_batch_hooks () for cb in self . callbacks ) self . _should_call_predict_batch_hooks = any ( cb . _implements_predict_batch_hooks () for cb in self . callbacks )","title":"__init__()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_epoch_begin","text":"Calls the on_epoch_begin methods of its callbacks. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def on_epoch_begin ( self , epoch , logs = None ): \"\"\"Calls the `on_epoch_begin` methods of its callbacks. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_epoch_begin ( epoch , logs ) self . _reset_batch_timing ()","title":"on_epoch_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_epoch_end","text":"Calls the on_epoch_end methods of its callbacks. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/callback_list.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def on_epoch_end ( self , epoch , logs = None ): \"\"\"Calls the `on_epoch_end` methods of its callbacks. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_epoch_end ( epoch , logs )","title":"on_epoch_end()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_predict_batch_begin","text":"Calls the on_predict_batch_begin methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback_list.py 263 264 265 266 267 268 269 270 271 272 273 def on_predict_batch_begin ( self , batch , logs = None ): \"\"\"Calls the `on_predict_batch_begin` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" if self . _should_call_predict_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . PREDICT , \"begin\" , batch , logs = logs )","title":"on_predict_batch_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_predict_batch_end","text":"Calls the on_predict_batch_end methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/callback_list.py 275 276 277 278 279 280 281 282 283 284 def on_predict_batch_end ( self , batch , logs = None ): \"\"\"Calls the `on_predict_batch_end` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" if self . _should_call_predict_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . PREDICT , \"end\" , batch , logs = logs )","title":"on_predict_batch_end()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_predict_begin","text":"Calls the 'on_predict_begin` methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 330 331 332 333 334 335 336 337 338 339 def on_predict_begin ( self , logs = None ): \"\"\"Calls the 'on_predict_begin` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_predict_begin ( logs )","title":"on_predict_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_predict_end","text":"Calls the on_predict_end methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 341 342 343 344 345 346 347 348 349 350 def on_predict_end ( self , logs = None ): \"\"\"Calls the `on_predict_end` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_predict_end ( logs )","title":"on_predict_end()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_test_batch_begin","text":"Calls the on_test_batch_begin methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback_list.py 240 241 242 243 244 245 246 247 248 249 250 def on_test_batch_begin ( self , batch , logs = None ): \"\"\"Calls the `on_test_batch_begin` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" if self . _should_call_test_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TEST , \"begin\" , batch , logs = logs )","title":"on_test_batch_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_test_batch_end","text":"Calls the on_test_batch_end methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/callback_list.py 252 253 254 255 256 257 258 259 260 261 def on_test_batch_end ( self , batch , logs = None ): \"\"\"Calls the `on_test_batch_end` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" if self . _should_call_test_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TEST , \"end\" , batch , logs = logs )","title":"on_test_batch_end()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_test_begin","text":"Calls the on_test_begin methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 308 309 310 311 312 313 314 315 316 317 def on_test_begin ( self , logs = None ): \"\"\"Calls the `on_test_begin` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_test_begin ( logs )","title":"on_test_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_test_end","text":"Calls the on_test_end methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 319 320 321 322 323 324 325 326 327 328 def on_test_end ( self , logs = None ): \"\"\"Calls the `on_test_end` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_test_end ( logs )","title":"on_test_end()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_train_batch_begin","text":"Calls the on_train_batch_begin methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback_list.py 215 216 217 218 219 220 221 222 223 224 225 226 227 def on_train_batch_begin ( self , batch , logs = None ): \"\"\"Calls the `on_train_batch_begin` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" # TODO(b/150629188): Make ProgBarLogger callback not use batch hooks # when verbose != 1 if self . _should_call_train_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TRAIN , \"begin\" , batch , logs = logs )","title":"on_train_batch_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_train_batch_end","text":"Calls the on_train_batch_end methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/callback_list.py 229 230 231 232 233 234 235 236 237 238 def on_train_batch_end ( self , batch , logs = None ): \"\"\"Calls the `on_train_batch_end` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" if self . _should_call_train_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TRAIN , \"end\" , batch , logs = logs )","title":"on_train_batch_end()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_train_begin","text":"Calls the on_train_begin methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 286 287 288 289 290 291 292 293 294 295 def on_train_begin ( self , logs = None ): \"\"\"Calls the `on_train_begin` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_train_begin ( logs )","title":"on_train_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_train_end","text":"Calls the on_train_end methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py 297 298 299 300 301 302 303 304 305 306 def on_train_end ( self , logs = None ): \"\"\"Calls the `on_train_end` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_train_end ( logs )","title":"on_train_end()"},{"location":"api/callbacks/EarlyStopping/","text":"elegy.callbacks.EarlyStopping Stop training when a monitored metric has stopped improving. Assuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates. The quantity to be monitored needs to be available in logs dict. To make it so, pass the loss or metrics at model.__init__() . Examples: np . random . seed ( 42 ) class MLP ( elegy . Module ): def call ( self , input ): mlp = hk . Sequential ([ hk . Linear ( 10 ),]) return mlp ( input ) callback = elegy . callbacks . EarlyStopping ( monitor = \"loss\" , patience = 3 ) # This callback will stop the training when there is no improvement in # the for three consecutive epochs. model = elegy . Model ( module = MLP (), loss = elegy . losses . MeanSquaredError (), optimizer = optax . rmsprop ( 0.01 ), ) history = model . fit ( np . arange ( 100 ) . reshape ( 5 , 20 ) . astype ( np . float32 ), np . zeros ( 5 ), epochs = 10 , batch_size = 1 , callbacks = [ callback ], verbose = 0 , ) assert len ( history . history [ \"loss\" ]) == 7 # Only 7 epochs are run. on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/early_stopping.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def on_epoch_end ( self , epoch , logs = None ): current = self . get_monitor_value ( logs ) if current is None : return if self . monitor_op ( current - self . min_delta , self . best ): self . best = current self . wait = 0 if self . restore_best_weights : # This will also save optimizer state self . best_state = self . model . full_state else : self . wait += 1 if self . wait >= self . patience : self . stopped_epoch = epoch self . model . stop_training = True if self . restore_best_weights : if self . verbose > 0 : print ( \"Restoring model weights from the end of the best epoch.\" ) self . model . full_state = self . best_state on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 122 123 124 125 126 127 128 129 def on_train_begin ( self , logs = None ): # Allow instances to be re-used self . wait = 0 self . stopped_epoch = 0 if self . baseline is not None : self . best = self . baseline else : self . best = np . Inf if self . monitor_op == np . less else - np . Inf on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 151 152 153 def on_train_end ( self , logs = None ): if self . stopped_epoch > 0 and self . verbose > 0 : print ( \"Epoch %05d : early stopping\" % ( self . stopped_epoch + 1 ))","title":"EarlyStopping"},{"location":"api/callbacks/EarlyStopping/#elegycallbacksearlystopping","text":"","title":"elegy.callbacks.EarlyStopping"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping","text":"Stop training when a monitored metric has stopped improving. Assuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates. The quantity to be monitored needs to be available in logs dict. To make it so, pass the loss or metrics at model.__init__() . Examples: np . random . seed ( 42 ) class MLP ( elegy . Module ): def call ( self , input ): mlp = hk . Sequential ([ hk . Linear ( 10 ),]) return mlp ( input ) callback = elegy . callbacks . EarlyStopping ( monitor = \"loss\" , patience = 3 ) # This callback will stop the training when there is no improvement in # the for three consecutive epochs. model = elegy . Model ( module = MLP (), loss = elegy . losses . MeanSquaredError (), optimizer = optax . rmsprop ( 0.01 ), ) history = model . fit ( np . arange ( 100 ) . reshape ( 5 , 20 ) . astype ( np . float32 ), np . zeros ( 5 ), epochs = 10 , batch_size = 1 , callbacks = [ callback ], verbose = 0 , ) assert len ( history . history [ \"loss\" ]) == 7 # Only 7 epochs are run.","title":"elegy.callbacks.early_stopping.EarlyStopping"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/early_stopping.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def on_epoch_end ( self , epoch , logs = None ): current = self . get_monitor_value ( logs ) if current is None : return if self . monitor_op ( current - self . min_delta , self . best ): self . best = current self . wait = 0 if self . restore_best_weights : # This will also save optimizer state self . best_state = self . model . full_state else : self . wait += 1 if self . wait >= self . patience : self . stopped_epoch = epoch self . model . stop_training = True if self . restore_best_weights : if self . verbose > 0 : print ( \"Restoring model weights from the end of the best epoch.\" ) self . model . full_state = self . best_state","title":"on_epoch_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 122 123 124 125 126 127 128 129 def on_train_begin ( self , logs = None ): # Allow instances to be re-used self . wait = 0 self . stopped_epoch = 0 if self . baseline is not None : self . best = self . baseline else : self . best = np . Inf if self . monitor_op == np . less else - np . Inf","title":"on_train_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 151 152 153 def on_train_end ( self , logs = None ): if self . stopped_epoch > 0 and self . verbose > 0 : print ( \"Epoch %05d : early stopping\" % ( self . stopped_epoch + 1 ))","title":"on_train_end()"},{"location":"api/callbacks/History/","text":"elegy.callbacks.History Callback that records events into a History object. This callback is automatically applied to every Keras model. The History object gets returned by the fit method of models. on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/history.py 21 22 23 24 25 26 27 28 29 def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} self . epoch . append ( epoch ) for k , v in logs . items (): self . history . setdefault ( k , []) . append ( v ) # Set the history attribute on the model after the epoch ends. This will # make sure that the state which is set is the latest one. self . model . history = self on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 18 19 def on_train_begin ( self , logs = None ): self . epoch = [] on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"History"},{"location":"api/callbacks/History/#elegycallbackshistory","text":"","title":"elegy.callbacks.History"},{"location":"api/callbacks/History/#elegy.callbacks.history.History","text":"Callback that records events into a History object. This callback is automatically applied to every Keras model. The History object gets returned by the fit method of models.","title":"elegy.callbacks.history.History"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/history.py 21 22 23 24 25 26 27 28 29 def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} self . epoch . append ( epoch ) for k , v in logs . items (): self . history . setdefault ( k , []) . append ( v ) # Set the history attribute on the model after the epoch ends. This will # make sure that the state which is set is the latest one. self . model . history = self","title":"on_epoch_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 18 19 def on_train_begin ( self , logs = None ): self . epoch = []","title":"on_train_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/LambdaCallback/","text":"elegy.callbacks.LambdaCallback Callback for creating simple, custom callbacks on-the-fly. This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as: on_epoch_begin and on_epoch_end expect two positional arguments: epoch , logs on_train_batch_begin and on_train_batch_end expect two positional arguments: batch , logs on_train_begin and on_train_end expect one positional argument: logs Examples: # Print the batch number at the beginning of every batch. batch_print_callback = LambdaCallback ( on_train_batch_begin = lambda batch , logs : print ( batch )) # Stream the epoch loss to a file in JSON format. The file content # is not well-formed JSON but rather has a JSON object per line. import json json_log = open ( 'loss_log.json' , mode = 'wt' , buffering = 1 ) json_logging_callback = LambdaCallback ( on_epoch_end = lambda epoch , logs : json_log . write ( json . dumps ({ 'epoch' : epoch , 'loss' : logs [ 'loss' ]}) + ' \\n ' ), on_train_end = lambda logs : json_log . close () ) # Terminate some processes after having finished model training. processes = ... cleanup_callback = LambdaCallback ( on_train_end = lambda logs : [ p . terminate () for p in processes if p . is_alive ()]) model . fit ( ... , callbacks = [ batch_print_callback , json_logging_callback , cleanup_callback ]) on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) inherited Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/lambda_callback.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) inherited Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"LambdaCallback"},{"location":"api/callbacks/LambdaCallback/#elegycallbackslambdacallback","text":"","title":"elegy.callbacks.LambdaCallback"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback","text":"Callback for creating simple, custom callbacks on-the-fly. This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as: on_epoch_begin and on_epoch_end expect two positional arguments: epoch , logs on_train_batch_begin and on_train_batch_end expect two positional arguments: batch , logs on_train_begin and on_train_end expect one positional argument: logs Examples: # Print the batch number at the beginning of every batch. batch_print_callback = LambdaCallback ( on_train_batch_begin = lambda batch , logs : print ( batch )) # Stream the epoch loss to a file in JSON format. The file content # is not well-formed JSON but rather has a JSON object per line. import json json_log = open ( 'loss_log.json' , mode = 'wt' , buffering = 1 ) json_logging_callback = LambdaCallback ( on_epoch_end = lambda epoch , logs : json_log . write ( json . dumps ({ 'epoch' : epoch , 'loss' : logs [ 'loss' ]}) + ' \\n ' ), on_train_end = lambda logs : json_log . close () ) # Terminate some processes after having finished model training. processes = ... cleanup_callback = LambdaCallback ( on_train_end = lambda logs : [ p . terminate () for p in processes if p . is_alive ()]) model . fit ( ... , callbacks = [ batch_print_callback , json_logging_callback , cleanup_callback ])","title":"elegy.callbacks.lambda_callback.LambdaCallback"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/lambda_callback.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass","title":"on_epoch_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/ModelCheckpoint/","text":"elegy.callbacks.ModelCheckpoint Callback to save the Elegy model or model weights at some frequency. ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights at some interval, so the model or weights can be loaded later to continue the training from the state saved. A few options this callback provides include: Whether to only keep the model that has achieved the \"best performance\" so far, or whether to save the model at the end of every epoch regardless of performance. Definition of 'best'; which quantity to monitor and whether it should be maximized or minimized. The frequency it should save at. Currently, the callback supports saving at the end of every epoch, or after a fixed number of training batches. Examples: EPOCHS = 10 checkpoint_path = '/tmp/checkpoint' model_checkpoint_callback = elegy . callbacks . ModelCheckpoint ( path = checkpoint_path , monitor = 'val_acc' , mode = 'max' , save_best_only = True ) # Model is saved at the end of every epoch, if it's the best seen # so far. model . fit ( epochs = EPOCHS , callbacks = [ model_checkpoint_callback ]) # The model status (that are considered the best) are loaded into the model. model . load ( checkpoint_path ) on_epoch_begin ( self , epoch , logs = None ) Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 134 135 def on_epoch_begin ( self , epoch , logs = None ): self . _current_epoch = epoch on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/model_checkpoint.py 137 138 139 140 141 def on_epoch_end ( self , epoch , logs = None ): self . epochs_since_last_save += 1 # pylint: disable=protected-access if self . save_freq == \"epoch\" : self . _save_model ( epoch = epoch , logs = logs ) on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) inherited Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"ModelCheckpoint"},{"location":"api/callbacks/ModelCheckpoint/#elegycallbacksmodelcheckpoint","text":"","title":"elegy.callbacks.ModelCheckpoint"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint","text":"Callback to save the Elegy model or model weights at some frequency. ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights at some interval, so the model or weights can be loaded later to continue the training from the state saved. A few options this callback provides include: Whether to only keep the model that has achieved the \"best performance\" so far, or whether to save the model at the end of every epoch regardless of performance. Definition of 'best'; which quantity to monitor and whether it should be maximized or minimized. The frequency it should save at. Currently, the callback supports saving at the end of every epoch, or after a fixed number of training batches. Examples: EPOCHS = 10 checkpoint_path = '/tmp/checkpoint' model_checkpoint_callback = elegy . callbacks . ModelCheckpoint ( path = checkpoint_path , monitor = 'val_acc' , mode = 'max' , save_best_only = True ) # Model is saved at the end of every epoch, if it's the best seen # so far. model . fit ( epochs = EPOCHS , callbacks = [ model_checkpoint_callback ]) # The model status (that are considered the best) are loaded into the model. model . load ( checkpoint_path )","title":"elegy.callbacks.model_checkpoint.ModelCheckpoint"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 134 135 def on_epoch_begin ( self , epoch , logs = None ): self . _current_epoch = epoch","title":"on_epoch_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/model_checkpoint.py 137 138 139 140 141 def on_epoch_end ( self , epoch , logs = None ): self . epochs_since_last_save += 1 # pylint: disable=protected-access if self . save_freq == \"epoch\" : self . _save_model ( epoch = epoch , logs = logs )","title":"on_epoch_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/RemoteMonitor/","text":"elegy.callbacks.RemoteMonitor Callback used to stream events to a server. Requires the requests library. Events are sent to root + '/publish/epoch/end/' by default. Calls are HTTP POST, with a data argument which is a JSON-encoded dictionary of event data. If send_as_json is set to True, the content type of the request will be application/json. Otherwise the serialized JSON will be sent within a form. on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/remote_monitor.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def on_epoch_end ( self , epoch , logs = None ): if requests is None : raise ImportError ( \"RemoteMonitor requires the `requests` library.\" ) logs = logs or {} send = {} send [ \"epoch\" ] = epoch for k , v in logs . items (): # np.ndarray and np.generic are not scalar types # therefore we must unwrap their scalar values and # pass to the json-serializable dict 'send' if isinstance ( v , ( np . ndarray , np . generic )): send [ k ] = v . item () else : send [ k ] = v try : if self . send_as_json : requests . post ( self . root + self . path , json = send , headers = self . headers ) else : requests . post ( self . root + self . path , { self . field : json . dumps ( send )}, headers = self . headers , ) except requests . exceptions . RequestException : logging . warning ( \"Warning: could not reach RemoteMonitor \" \"root server at \" + str ( self . root ) ) on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) inherited Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"RemoteMonitor"},{"location":"api/callbacks/RemoteMonitor/#elegycallbacksremotemonitor","text":"","title":"elegy.callbacks.RemoteMonitor"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor","text":"Callback used to stream events to a server. Requires the requests library. Events are sent to root + '/publish/epoch/end/' by default. Calls are HTTP POST, with a data argument which is a JSON-encoded dictionary of event data. If send_as_json is set to True, the content type of the request will be application/json. Otherwise the serialized JSON will be sent within a form.","title":"elegy.callbacks.remote_monitor.RemoteMonitor"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/remote_monitor.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def on_epoch_end ( self , epoch , logs = None ): if requests is None : raise ImportError ( \"RemoteMonitor requires the `requests` library.\" ) logs = logs or {} send = {} send [ \"epoch\" ] = epoch for k , v in logs . items (): # np.ndarray and np.generic are not scalar types # therefore we must unwrap their scalar values and # pass to the json-serializable dict 'send' if isinstance ( v , ( np . ndarray , np . generic )): send [ k ] = v . item () else : send [ k ] = v try : if self . send_as_json : requests . post ( self . root + self . path , json = send , headers = self . headers ) else : requests . post ( self . root + self . path , { self . field : json . dumps ( send )}, headers = self . headers , ) except requests . exceptions . RequestException : logging . warning ( \"Warning: could not reach RemoteMonitor \" \"root server at \" + str ( self . root ) )","title":"on_epoch_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/TensorBoard/","text":"elegy.callbacks.TensorBoard Callback that streams epoch results to tensorboard events folder. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . tensorboard_logger = TensorBoard ( 'runs' ) model . fit ( X_train , Y_train , callbacks = [ tensorboard_logger ]) on_epoch_begin ( self , epoch , logs = None ) Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 105 106 def on_epoch_begin ( self , epoch : int , logs = None ): self . current_epoch = epoch on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/tensorboard.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} if self . keys is None : self . keys = logs . keys () # logs on on_{train, test}_batch_end do not have val metrics if self . write_per_batch : for key in logs : if \"val\" in key : self . val_writer . add_scalar ( key . replace ( \"val_\" , \"\" ), logs [ key ], self . global_step ) return elif epoch % self . update_freq == 0 : for key in self . keys : if \"val\" in key : self . val_writer . add_scalar ( key . replace ( \"val_\" , \"\" ), logs [ key ], epoch ) else : self . train_writer . add_scalar ( key , logs [ key ], epoch ) on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py 94 95 96 97 98 99 100 101 102 103 def on_train_batch_end ( self , batch : int , logs = None ): if not self . write_per_batch : return logs = logs or {} self . global_step = batch + self . current_epoch * ( self . steps ) if self . global_step % self . update_freq == 0 : if self . keys is None : self . keys = logs . keys () for key in self . keys : self . train_writer . add_scalar ( key , logs [ key ], self . global_step ) on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 84 85 86 87 88 89 90 91 92 def on_train_begin ( self , logs = None ): self . train_writer = SummaryWriter ( os . path . join ( self . logdir , \"train\" ), purge_step = self . purge_step ) self . val_writer = SummaryWriter ( os . path . join ( self . logdir , \"val\" ), purge_step = self . purge_step ) self . steps = self . params [ \"steps\" ] self . global_step = 0 on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 133 134 135 def on_train_end ( self , logs = None ): self . train_writer . close () self . val_writer . close ()","title":"TensorBoard"},{"location":"api/callbacks/TensorBoard/#elegycallbackstensorboard","text":"","title":"elegy.callbacks.TensorBoard"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard","text":"Callback that streams epoch results to tensorboard events folder. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . tensorboard_logger = TensorBoard ( 'runs' ) model . fit ( X_train , Y_train , callbacks = [ tensorboard_logger ])","title":"elegy.callbacks.tensorboard.TensorBoard"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 105 106 def on_epoch_begin ( self , epoch : int , logs = None ): self . current_epoch = epoch","title":"on_epoch_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/tensorboard.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} if self . keys is None : self . keys = logs . keys () # logs on on_{train, test}_batch_end do not have val metrics if self . write_per_batch : for key in logs : if \"val\" in key : self . val_writer . add_scalar ( key . replace ( \"val_\" , \"\" ), logs [ key ], self . global_step ) return elif epoch % self . update_freq == 0 : for key in self . keys : if \"val\" in key : self . val_writer . add_scalar ( key . replace ( \"val_\" , \"\" ), logs [ key ], epoch ) else : self . train_writer . add_scalar ( key , logs [ key ], epoch )","title":"on_epoch_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py 94 95 96 97 98 99 100 101 102 103 def on_train_batch_end ( self , batch : int , logs = None ): if not self . write_per_batch : return logs = logs or {} self . global_step = batch + self . current_epoch * ( self . steps ) if self . global_step % self . update_freq == 0 : if self . keys is None : self . keys = logs . keys () for key in self . keys : self . train_writer . add_scalar ( key , logs [ key ], self . global_step )","title":"on_train_batch_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 84 85 86 87 88 89 90 91 92 def on_train_begin ( self , logs = None ): self . train_writer = SummaryWriter ( os . path . join ( self . logdir , \"train\" ), purge_step = self . purge_step ) self . val_writer = SummaryWriter ( os . path . join ( self . logdir , \"val\" ), purge_step = self . purge_step ) self . steps = self . params [ \"steps\" ] self . global_step = 0","title":"on_train_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 133 134 135 def on_train_end ( self , logs = None ): self . train_writer . close () self . val_writer . close ()","title":"on_train_end()"},{"location":"api/callbacks/TerminateOnNaN/","text":"elegy.callbacks.TerminateOnNaN Callback that terminates training when a NaN loss is encountered. on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) inherited Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/terminate_nan.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) inherited Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"TerminateOnNaN"},{"location":"api/callbacks/TerminateOnNaN/#elegycallbacksterminateonnan","text":"","title":"elegy.callbacks.TerminateOnNaN"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN","text":"Callback that terminates training when a NaN loss is encountered.","title":"elegy.callbacks.terminate_nan.TerminateOnNaN"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/terminate_nan.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass","title":"on_epoch_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/data/DataLoader/","text":"elegy.data.DataLoader Loads samples from a dataset and combines them into batches. Can be directly passed to Model.fit() Example Usage: class MyDataset ( elegy . data . Dataset ): def __len__ ( self ): return 128 def __getitem__ ( self , i ): #dummy data return np . random . random ([ 224 , 224 , 3 ]), np . random . randint ( 10 ) ds = MyDataset () loader = elegy . data . DataLoader ( ds , batch_size = 8 , n_workers = 8 , worker_type = 'thread' , shuffle = True ) model . fit ( loader , epochs = 10 ) __init__ ( self , dataset , batch_size , n_workers = 0 , shuffle = False , worker_type = 'thread' ) special Parameters: Name Type Description Default dataset Dataset The dataset from which to load samples. A subclass of elegy.data.Dataset or an iterable which implements __getitem__ and __len__ . required batch_size int A positive integer specifying how many samples a batch should have. required n_workers int The number of parallel worker threads or processes which load data from the dataset. A value of 0 (default) means to load data from the main thread. 0 shuffle bool Whether to load the samples in random order or not. Reshuffles on every epoch if True. Default: False False worker_type str One of 'thread' (default), 'process', 'spawn', 'fork' or 'forkserver'. Only used if n_workers >0. Threads are light-weight but underly the limitations of Python's global interpreter lock. 'process' uses the default process type as defined in the multiprocessing module. 'spawn', 'fork' and 'forkserver' can be used to select a specific process type. For more information consult the Python multiprocessing documentation. 'thread' Source code in elegy/data/dataset.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , dataset : Dataset , batch_size : int , n_workers : int = 0 , shuffle : bool = False , worker_type : str = \"thread\" , ): \"\"\" Arguments: dataset: The dataset from which to load samples. A subclass of elegy.data.Dataset or an iterable which implements `__getitem__` and `__len__`. batch_size: A positive integer specifying how many samples a batch should have. n_workers: The number of parallel worker threads or processes which load data from the dataset. A value of 0 (default) means to load data from the main thread. shuffle: Whether to load the samples in random order or not. Reshuffles on every epoch if True. Default: False worker_type: One of 'thread' (default), 'process', 'spawn', 'fork' or 'forkserver'. Only used if `n_workers`>0. Threads are light-weight but underly the limitations of Python's global interpreter lock. 'process' uses the default process type as defined in the `multiprocessing` module. 'spawn', 'fork' and 'forkserver' can be used to select a specific process type. For more information consult the Python `multiprocessing` documentation. \"\"\" assert ( batch_size > 0 and type ( batch_size ) == int ), \"batch_size must be a positive integer\" assert worker_type in [ \"thread\" , \"process\" , \"spawn\" , \"fork\" , \"forkserver\" ] self . dataset = dataset self . batch_size = batch_size self . n_workers = n_workers self . shuffle = shuffle self . worker_type = worker_type __iter__ ( self ) special Returns a generator which generates batches of loaded data samples Source code in elegy/data/dataset.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __iter__ ( self ) -> tp . Generator [ tp . Any , None , None ]: \"\"\"Returns a generator which generates batches of loaded data samples\"\"\" indices = np . arange ( len ( self . dataset )) if self . shuffle : np . random . shuffle ( indices ) batched_indices = [ indices [ i :][: self . batch_size ] for i in range ( 0 , len ( indices ), self . batch_size ) ] if self . n_workers == 0 : return mainthread_data_iterator ( self . dataset , batched_indices ) else : return multiprocess_data_iterator ( self . dataset , batched_indices , self . n_workers , worker_type = self . worker_type , ) __len__ ( self ) special Returns the number of batches per epoch Source code in elegy/data/dataset.py 108 109 110 def __len__ ( self ) -> int : \"\"\"Returns the number of batches per epoch\"\"\" return int ( np . ceil ( len ( self . dataset ) / self . batch_size ))","title":"DataLoader"},{"location":"api/data/DataLoader/#elegydatadataloader","text":"","title":"elegy.data.DataLoader"},{"location":"api/data/DataLoader/#elegy.data.dataset.DataLoader","text":"Loads samples from a dataset and combines them into batches. Can be directly passed to Model.fit() Example Usage: class MyDataset ( elegy . data . Dataset ): def __len__ ( self ): return 128 def __getitem__ ( self , i ): #dummy data return np . random . random ([ 224 , 224 , 3 ]), np . random . randint ( 10 ) ds = MyDataset () loader = elegy . data . DataLoader ( ds , batch_size = 8 , n_workers = 8 , worker_type = 'thread' , shuffle = True ) model . fit ( loader , epochs = 10 )","title":"elegy.data.dataset.DataLoader"},{"location":"api/data/DataLoader/#elegy.data.dataset.DataLoader.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset from which to load samples. A subclass of elegy.data.Dataset or an iterable which implements __getitem__ and __len__ . required batch_size int A positive integer specifying how many samples a batch should have. required n_workers int The number of parallel worker threads or processes which load data from the dataset. A value of 0 (default) means to load data from the main thread. 0 shuffle bool Whether to load the samples in random order or not. Reshuffles on every epoch if True. Default: False False worker_type str One of 'thread' (default), 'process', 'spawn', 'fork' or 'forkserver'. Only used if n_workers >0. Threads are light-weight but underly the limitations of Python's global interpreter lock. 'process' uses the default process type as defined in the multiprocessing module. 'spawn', 'fork' and 'forkserver' can be used to select a specific process type. For more information consult the Python multiprocessing documentation. 'thread' Source code in elegy/data/dataset.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , dataset : Dataset , batch_size : int , n_workers : int = 0 , shuffle : bool = False , worker_type : str = \"thread\" , ): \"\"\" Arguments: dataset: The dataset from which to load samples. A subclass of elegy.data.Dataset or an iterable which implements `__getitem__` and `__len__`. batch_size: A positive integer specifying how many samples a batch should have. n_workers: The number of parallel worker threads or processes which load data from the dataset. A value of 0 (default) means to load data from the main thread. shuffle: Whether to load the samples in random order or not. Reshuffles on every epoch if True. Default: False worker_type: One of 'thread' (default), 'process', 'spawn', 'fork' or 'forkserver'. Only used if `n_workers`>0. Threads are light-weight but underly the limitations of Python's global interpreter lock. 'process' uses the default process type as defined in the `multiprocessing` module. 'spawn', 'fork' and 'forkserver' can be used to select a specific process type. For more information consult the Python `multiprocessing` documentation. \"\"\" assert ( batch_size > 0 and type ( batch_size ) == int ), \"batch_size must be a positive integer\" assert worker_type in [ \"thread\" , \"process\" , \"spawn\" , \"fork\" , \"forkserver\" ] self . dataset = dataset self . batch_size = batch_size self . n_workers = n_workers self . shuffle = shuffle self . worker_type = worker_type","title":"__init__()"},{"location":"api/data/DataLoader/#elegy.data.dataset.DataLoader.__iter__","text":"Returns a generator which generates batches of loaded data samples Source code in elegy/data/dataset.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __iter__ ( self ) -> tp . Generator [ tp . Any , None , None ]: \"\"\"Returns a generator which generates batches of loaded data samples\"\"\" indices = np . arange ( len ( self . dataset )) if self . shuffle : np . random . shuffle ( indices ) batched_indices = [ indices [ i :][: self . batch_size ] for i in range ( 0 , len ( indices ), self . batch_size ) ] if self . n_workers == 0 : return mainthread_data_iterator ( self . dataset , batched_indices ) else : return multiprocess_data_iterator ( self . dataset , batched_indices , self . n_workers , worker_type = self . worker_type , )","title":"__iter__()"},{"location":"api/data/DataLoader/#elegy.data.dataset.DataLoader.__len__","text":"Returns the number of batches per epoch Source code in elegy/data/dataset.py 108 109 110 def __len__ ( self ) -> int : \"\"\"Returns the number of batches per epoch\"\"\" return int ( np . ceil ( len ( self . dataset ) / self . batch_size ))","title":"__len__()"},{"location":"api/data/Dataset/","text":"elegy.data.Dataset Abstract base class for datasets. Subclasses should implement the __getitem__ and __len__ methods. Example Usage: class MyDataset ( elegy . data . Dataset ): def __len__ ( self ): return 128 def __getitem__ ( self , i ): #dummy data return np . random . random ([ 224 , 224 , 3 ]), np . random . randint ( 10 ) ds = MyDataset () loader = elegy . data . DataLoader ( ds , batch_size = 8 , n_workers = 8 , worker_type = 'thread' , shuffle = True ) model . fit ( loader , epochs = 10 ) __getitem__ ( self , i ) special Abstract method. In a subclass this should return the i -th data sample Source code in elegy/data/dataset.py 36 37 38 def __getitem__ ( self , i : int ) -> tp . Any : \"\"\"Abstract method. In a subclass this should return the `i`-th data sample\"\"\" raise NotImplementedError __len__ ( self ) special Abstract method. In a subclass this should return the number of data samples in the dataset. Source code in elegy/data/dataset.py 40 41 42 def __len__ ( self ) -> int : \"\"\"Abstract method. In a subclass this should return the number of data samples in the dataset.\"\"\" raise NotImplementedError","title":"Dataset"},{"location":"api/data/Dataset/#elegydatadataset","text":"","title":"elegy.data.Dataset"},{"location":"api/data/Dataset/#elegy.data.dataset.Dataset","text":"Abstract base class for datasets. Subclasses should implement the __getitem__ and __len__ methods. Example Usage: class MyDataset ( elegy . data . Dataset ): def __len__ ( self ): return 128 def __getitem__ ( self , i ): #dummy data return np . random . random ([ 224 , 224 , 3 ]), np . random . randint ( 10 ) ds = MyDataset () loader = elegy . data . DataLoader ( ds , batch_size = 8 , n_workers = 8 , worker_type = 'thread' , shuffle = True ) model . fit ( loader , epochs = 10 )","title":"elegy.data.dataset.Dataset"},{"location":"api/data/Dataset/#elegy.data.dataset.Dataset.__getitem__","text":"Abstract method. In a subclass this should return the i -th data sample Source code in elegy/data/dataset.py 36 37 38 def __getitem__ ( self , i : int ) -> tp . Any : \"\"\"Abstract method. In a subclass this should return the `i`-th data sample\"\"\" raise NotImplementedError","title":"__getitem__()"},{"location":"api/data/Dataset/#elegy.data.dataset.Dataset.__len__","text":"Abstract method. In a subclass this should return the number of data samples in the dataset. Source code in elegy/data/dataset.py 40 41 42 def __len__ ( self ) -> int : \"\"\"Abstract method. In a subclass this should return the number of data samples in the dataset.\"\"\" raise NotImplementedError","title":"__len__()"},{"location":"api/initializers/Constant/","text":"elegy.initializers.Constant Initializes with a constant. __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , constant ) special Constructs a Constant initializer. Parameters: Name Type Description Default constant Constant to initialize with. required Source code in elegy/initializers.py 55 56 57 58 59 60 61 62 def __init__ ( self , constant ): \"\"\" Constructs a Constant initializer. Args: constant: Constant to initialize with. \"\"\" self . constant = constant","title":"Constant"},{"location":"api/initializers/Constant/#elegyinitializersconstant","text":"","title":"elegy.initializers.Constant"},{"location":"api/initializers/Constant/#elegy.initializers.Constant","text":"Initializes with a constant.","title":"elegy.initializers.Constant"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__","text":"","title":"__class__"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__init__","text":"Constructs a Constant initializer. Parameters: Name Type Description Default constant Constant to initialize with. required Source code in elegy/initializers.py 55 56 57 58 59 60 61 62 def __init__ ( self , constant ): \"\"\" Constructs a Constant initializer. Args: constant: Constant to initialize with. \"\"\" self . constant = constant","title":"__init__()"},{"location":"api/initializers/Orthogonal/","text":"elegy.initializers.Orthogonal Uniform scaling initializer. __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , scale = 1.0 , axis =- 1 ) special Construct an initializer for uniformly distributed orthogonal matrices. These matrices will be row-orthonormal along the access specified by axis . If the rank of the weight is greater than 2, the shape will be flattened in all other dimensions and then will be row-orthonormal along the final dimension. Note that this only works if the axis dimension is larger, otherwise the matrix will be transposed (equivalently, it will be column orthonormal instead of row orthonormal). Parameters: Name Type Description Default scale Scale factor. 1.0 axis int Which axis corresponds to the \"output dimension\" of the tensor. -1 If the shape is not square, the matrices will have orthonormal rows or columns depending on which side is smaller. Source code in elegy/initializers.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def __init__ ( self , scale = 1.0 , axis =- 1 ): \"\"\" Construct an initializer for uniformly distributed orthogonal matrices. These matrices will be row-orthonormal along the access specified by `axis`. If the rank of the weight is greater than 2, the shape will be flattened in all other dimensions and then will be row-orthonormal along the final dimension. Note that this only works if the `axis` dimension is larger, otherwise the matrix will be transposed (equivalently, it will be column orthonormal instead of row orthonormal). Args: scale: Scale factor. axis (int): Which axis corresponds to the \"output dimension\" of the tensor. If the shape is not square, the matrices will have orthonormal rows or columns depending on which side is smaller. \"\"\" self . scale = scale self . axis = axis","title":"Orthogonal"},{"location":"api/initializers/Orthogonal/#elegyinitializersorthogonal","text":"","title":"elegy.initializers.Orthogonal"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal","text":"Uniform scaling initializer.","title":"elegy.initializers.Orthogonal"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__","text":"","title":"__class__"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__init__","text":"Construct an initializer for uniformly distributed orthogonal matrices. These matrices will be row-orthonormal along the access specified by axis . If the rank of the weight is greater than 2, the shape will be flattened in all other dimensions and then will be row-orthonormal along the final dimension. Note that this only works if the axis dimension is larger, otherwise the matrix will be transposed (equivalently, it will be column orthonormal instead of row orthonormal). Parameters: Name Type Description Default scale Scale factor. 1.0 axis int Which axis corresponds to the \"output dimension\" of the tensor. -1 If the shape is not square, the matrices will have orthonormal rows or columns depending on which side is smaller. Source code in elegy/initializers.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def __init__ ( self , scale = 1.0 , axis =- 1 ): \"\"\" Construct an initializer for uniformly distributed orthogonal matrices. These matrices will be row-orthonormal along the access specified by `axis`. If the rank of the weight is greater than 2, the shape will be flattened in all other dimensions and then will be row-orthonormal along the final dimension. Note that this only works if the `axis` dimension is larger, otherwise the matrix will be transposed (equivalently, it will be column orthonormal instead of row orthonormal). Args: scale: Scale factor. axis (int): Which axis corresponds to the \"output dimension\" of the tensor. If the shape is not square, the matrices will have orthonormal rows or columns depending on which side is smaller. \"\"\" self . scale = scale self . axis = axis","title":"__init__()"},{"location":"api/initializers/RandomNormal/","text":"elegy.initializers.RandomNormal Initializes by sampling from a normal distribution. __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , stddev = 1.0 , mean = 0.0 ) special Constructs a RandomNormal initializer. Parameters: Name Type Description Default stddev The standard deviation of the normal distribution to sample from. 1.0 mean The mean of the normal distribution to sample from. 0.0 Source code in elegy/initializers.py 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , stddev = 1.0 , mean = 0.0 ): \"\"\" Constructs a RandomNormal initializer. Args: stddev: The standard deviation of the normal distribution to sample from. mean: The mean of the normal distribution to sample from. \"\"\" self . stddev = stddev self . mean = mean","title":"RandomNormal"},{"location":"api/initializers/RandomNormal/#elegyinitializersrandomnormal","text":"","title":"elegy.initializers.RandomNormal"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal","text":"Initializes by sampling from a normal distribution.","title":"elegy.initializers.RandomNormal"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__","text":"","title":"__class__"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__init__","text":"Constructs a RandomNormal initializer. Parameters: Name Type Description Default stddev The standard deviation of the normal distribution to sample from. 1.0 mean The mean of the normal distribution to sample from. 0.0 Source code in elegy/initializers.py 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , stddev = 1.0 , mean = 0.0 ): \"\"\" Constructs a RandomNormal initializer. Args: stddev: The standard deviation of the normal distribution to sample from. mean: The mean of the normal distribution to sample from. \"\"\" self . stddev = stddev self . mean = mean","title":"__init__()"},{"location":"api/initializers/RandomUniform/","text":"elegy.initializers.RandomUniform Initializes by sampling from a uniform distribution. __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , minval = 0.0 , maxval = 1.0 ) special Constructs a RandomUniform initializer. Parameters: Name Type Description Default minval The lower limit of the uniform distribution. 0.0 maxval The upper limit of the uniform distribution. 1.0 Source code in elegy/initializers.py 115 116 117 118 119 120 121 122 123 124 def __init__ ( self , minval = 0.0 , maxval = 1.0 ): \"\"\" Constructs a RandomUniform initializer. Args: minval: The lower limit of the uniform distribution. maxval: The upper limit of the uniform distribution. \"\"\" self . minval = minval self . maxval = maxval","title":"RandomUniform"},{"location":"api/initializers/RandomUniform/#elegyinitializersrandomuniform","text":"","title":"elegy.initializers.RandomUniform"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform","text":"Initializes by sampling from a uniform distribution.","title":"elegy.initializers.RandomUniform"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__","text":"","title":"__class__"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__init__","text":"Constructs a RandomUniform initializer. Parameters: Name Type Description Default minval The lower limit of the uniform distribution. 0.0 maxval The upper limit of the uniform distribution. 1.0 Source code in elegy/initializers.py 115 116 117 118 119 120 121 122 123 124 def __init__ ( self , minval = 0.0 , maxval = 1.0 ): \"\"\" Constructs a RandomUniform initializer. Args: minval: The lower limit of the uniform distribution. maxval: The upper limit of the uniform distribution. \"\"\" self . minval = minval self . maxval = maxval","title":"__init__()"},{"location":"api/initializers/TruncatedNormal/","text":"elegy.initializers.TruncatedNormal Initializes by sampling from a truncated normal distribution. __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , stddev = 1.0 , mean = 0.0 ) special Constructs a RandomNormal initializer. Parameters: Name Type Description Default stddev The standard deviation parameter of the truncated normal distribution. 1.0 mean The mean of the truncated normal distribution. 0.0 Source code in elegy/initializers.py 91 92 93 94 95 96 97 98 99 100 101 def __init__ ( self , stddev = 1.0 , mean = 0.0 ): \"\"\" Constructs a RandomNormal initializer. Args: stddev: The standard deviation parameter of the truncated normal distribution. mean: The mean of the truncated normal distribution. \"\"\" self . stddev = stddev self . mean = mean","title":"TruncatedNormal"},{"location":"api/initializers/TruncatedNormal/#elegyinitializerstruncatednormal","text":"","title":"elegy.initializers.TruncatedNormal"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal","text":"Initializes by sampling from a truncated normal distribution.","title":"elegy.initializers.TruncatedNormal"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__","text":"","title":"__class__"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__init__","text":"Constructs a RandomNormal initializer. Parameters: Name Type Description Default stddev The standard deviation parameter of the truncated normal distribution. 1.0 mean The mean of the truncated normal distribution. 0.0 Source code in elegy/initializers.py 91 92 93 94 95 96 97 98 99 100 101 def __init__ ( self , stddev = 1.0 , mean = 0.0 ): \"\"\" Constructs a RandomNormal initializer. Args: stddev: The standard deviation parameter of the truncated normal distribution. mean: The mean of the truncated normal distribution. \"\"\" self . stddev = stddev self . mean = mean","title":"__init__()"},{"location":"api/initializers/UniformScaling/","text":"elegy.initializers.UniformScaling Uniform scaling initializer. Initializes by sampling from a uniform distribution, but with the variance scaled by the inverse square root of the number of input units, multiplied by the scale. __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , scale = 1.0 ) special Constructs the UniformScaling initializer. Parameters: Name Type Description Default scale Scale to multiply the upper limit of the uniform distribution by. 1.0 Source code in elegy/initializers.py 220 221 222 223 224 225 226 def __init__ ( self , scale = 1.0 ): \"\"\"Constructs the UniformScaling initializer. Args: scale: Scale to multiply the upper limit of the uniform distribution by. \"\"\" self . scale = scale","title":"UniformScaling"},{"location":"api/initializers/UniformScaling/#elegyinitializersuniformscaling","text":"","title":"elegy.initializers.UniformScaling"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling","text":"Uniform scaling initializer. Initializes by sampling from a uniform distribution, but with the variance scaled by the inverse square root of the number of input units, multiplied by the scale.","title":"elegy.initializers.UniformScaling"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__","text":"","title":"__class__"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__init__","text":"Constructs the UniformScaling initializer. Parameters: Name Type Description Default scale Scale to multiply the upper limit of the uniform distribution by. 1.0 Source code in elegy/initializers.py 220 221 222 223 224 225 226 def __init__ ( self , scale = 1.0 ): \"\"\"Constructs the UniformScaling initializer. Args: scale: Scale to multiply the upper limit of the uniform distribution by. \"\"\" self . scale = scale","title":"__init__()"},{"location":"api/initializers/VarianceScaling/","text":"elegy.initializers.VarianceScaling Initializer which adapts its scale to the shape of the initialized array. The initializer first computes the scaling factor s = scale / n , where n is: Number of input units in the weight tensor, if mode = fan_in . Number of output units, if mode = fan_out . Average of the numbers of input and output units, if mode = fan_avg . Then, with distribution=\"truncated_normal\" or \"normal\" , samples are drawn from a distribution with a mean of zero and a standard deviation (after truncation, if used) stddev = sqrt(s) . With distribution=uniform , samples are drawn from a uniform distribution within [-limit, limit] , with limit = sqrt(3 * s) . The variance scaling initializer can be configured to generate other standard initializers using the scale, mode and distribution arguments. Here are some example configurations: ============== ============================================================== Name Parameters ============== ============================================================== glorot_uniform scale=1.0, mode= fan_avg , distribution= uniform glorot_normal scale=1.0, mode= fan_avg , distribution= truncated_normal lecun_uniform scale=1.0, mode= fan_in , distribution= uniform lecun_normal scale=1.0, mode= fan_in , distribution= truncated_normal he_uniform scale=2.0, mode= fan_in , distribution= uniform he_normal scale=2.0, mode= fan_in , distribution= truncated_normal ============== ============================================================== __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , scale = 1.0 , mode = 'fan_in' , distribution = 'truncated_normal' ) special Constructs the VarianceScaling initializer. Parameters: Name Type Description Default scale Scale to multiply the variance by. 1.0 mode One of fan_in , fan_out , fan_avg 'fan_in' distribution Random distribution to use. One of truncated_normal , normal or uniform . 'truncated_normal' Source code in elegy/initializers.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def __init__ ( self , scale = 1.0 , mode = \"fan_in\" , distribution = \"truncated_normal\" ): \"\"\" Constructs the VarianceScaling initializer. Args: scale: Scale to multiply the variance by. mode: One of ``fan_in``, ``fan_out``, ``fan_avg`` distribution: Random distribution to use. One of ``truncated_normal``, ``normal`` or ``uniform``. \"\"\" if scale <= 0.0 : raise ValueError ( \"`scale` must be a positive float.\" ) if mode not in { \"fan_in\" , \"fan_out\" , \"fan_avg\" }: raise ValueError ( \"Invalid `mode` argument:\" , mode ) distribution = distribution . lower () if distribution not in { \"normal\" , \"truncated_normal\" , \"uniform\" }: raise ValueError ( \"Invalid `distribution` argument:\" , distribution ) self . scale = scale self . mode = mode self . distribution = distribution","title":"VarianceScaling"},{"location":"api/initializers/VarianceScaling/#elegyinitializersvariancescaling","text":"","title":"elegy.initializers.VarianceScaling"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling","text":"Initializer which adapts its scale to the shape of the initialized array. The initializer first computes the scaling factor s = scale / n , where n is: Number of input units in the weight tensor, if mode = fan_in . Number of output units, if mode = fan_out . Average of the numbers of input and output units, if mode = fan_avg . Then, with distribution=\"truncated_normal\" or \"normal\" , samples are drawn from a distribution with a mean of zero and a standard deviation (after truncation, if used) stddev = sqrt(s) . With distribution=uniform , samples are drawn from a uniform distribution within [-limit, limit] , with limit = sqrt(3 * s) . The variance scaling initializer can be configured to generate other standard initializers using the scale, mode and distribution arguments. Here are some example configurations: ============== ============================================================== Name Parameters ============== ============================================================== glorot_uniform scale=1.0, mode= fan_avg , distribution= uniform glorot_normal scale=1.0, mode= fan_avg , distribution= truncated_normal lecun_uniform scale=1.0, mode= fan_in , distribution= uniform lecun_normal scale=1.0, mode= fan_in , distribution= truncated_normal he_uniform scale=2.0, mode= fan_in , distribution= uniform he_normal scale=2.0, mode= fan_in , distribution= truncated_normal ============== ==============================================================","title":"elegy.initializers.VarianceScaling"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__","text":"","title":"__class__"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py 96 97 98 def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py 84 85 86 87 def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py 100 101 102 def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py 89 90 91 92 93 94 def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__init__","text":"Constructs the VarianceScaling initializer. Parameters: Name Type Description Default scale Scale to multiply the variance by. 1.0 mode One of fan_in , fan_out , fan_avg 'fan_in' distribution Random distribution to use. One of truncated_normal , normal or uniform . 'truncated_normal' Source code in elegy/initializers.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def __init__ ( self , scale = 1.0 , mode = \"fan_in\" , distribution = \"truncated_normal\" ): \"\"\" Constructs the VarianceScaling initializer. Args: scale: Scale to multiply the variance by. mode: One of ``fan_in``, ``fan_out``, ``fan_avg`` distribution: Random distribution to use. One of ``truncated_normal``, ``normal`` or ``uniform``. \"\"\" if scale <= 0.0 : raise ValueError ( \"`scale` must be a positive float.\" ) if mode not in { \"fan_in\" , \"fan_out\" , \"fan_avg\" }: raise ValueError ( \"Invalid `mode` argument:\" , mode ) distribution = distribution . lower () if distribution not in { \"normal\" , \"truncated_normal\" , \"uniform\" }: raise ValueError ( \"Invalid `distribution` argument:\" , distribution ) self . scale = scale self . mode = mode self . distribution = distribution","title":"__init__()"},{"location":"api/losses/BinaryCrossentropy/","text":"elegy.losses.BinaryCrossentropy Computes the cross-entropy loss between true labels and predicted labels. Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction. In the snippet below, each of the four examples has only a single floating-pointing value, and both y_pred and y_true have the shape [batch_size] . Usage: y_true = jnp . array ([[ 0. , 1. ], [ 0. , 0. ]]) y_pred = jnp . array [[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. bce = elegy . losses . BinaryCrossentropy () result = bce ( y_true , y_pred ) assert jnp . isclose ( result , 0.815 , rtol = 0.01 ) # Calling with 'sample_weight'. bce = elegy . losses . BinaryCrossentropy () result = bce ( y_true , y_pred , sample_weight = jnp . array ([ 1 , 0 ])) assert jnp . isclose ( result , 0.458 , rtol = 0.01 ) # Using 'sum' reduction type. bce = elegy . losses . BinaryCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = bce ( y_true , y_pred ) assert jnp . isclose ( result , 1.630 , rtol = 0.01 ) # Using 'none' reduction type. bce = elegy . losses . BinaryCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = bce ( y_true , y_pred ) assert jnp . all ( jnp . isclose ( result , [ 0.916 , 0.713 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . BinaryCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) __init__ ( self , from_logits = False , label_smoothing = 0 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE` will raise an error. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/binary_crossentropy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, ` or `SUM_OVER_BATCH_SIZE` will raise an error. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing call ( self , y_true , y_pred , sample_weight = None ) Invokes the BinaryCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/binary_crossentropy.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `BinaryCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return binary_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"BinaryCrossentropy"},{"location":"api/losses/BinaryCrossentropy/#elegylossesbinarycrossentropy","text":"","title":"elegy.losses.BinaryCrossentropy"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy","text":"Computes the cross-entropy loss between true labels and predicted labels. Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction. In the snippet below, each of the four examples has only a single floating-pointing value, and both y_pred and y_true have the shape [batch_size] . Usage: y_true = jnp . array ([[ 0. , 1. ], [ 0. , 0. ]]) y_pred = jnp . array [[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. bce = elegy . losses . BinaryCrossentropy () result = bce ( y_true , y_pred ) assert jnp . isclose ( result , 0.815 , rtol = 0.01 ) # Calling with 'sample_weight'. bce = elegy . losses . BinaryCrossentropy () result = bce ( y_true , y_pred , sample_weight = jnp . array ([ 1 , 0 ])) assert jnp . isclose ( result , 0.458 , rtol = 0.01 ) # Using 'sum' reduction type. bce = elegy . losses . BinaryCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = bce ( y_true , y_pred ) assert jnp . isclose ( result , 1.630 , rtol = 0.01 ) # Using 'none' reduction type. bce = elegy . losses . BinaryCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = bce ( y_true , y_pred ) assert jnp . all ( jnp . isclose ( result , [ 0.916 , 0.713 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . BinaryCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"elegy.losses.binary_crossentropy.BinaryCrossentropy"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.__init__","text":"Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE` will raise an error. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/binary_crossentropy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, ` or `SUM_OVER_BATCH_SIZE` will raise an error. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.call","text":"Invokes the BinaryCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/binary_crossentropy.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `BinaryCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return binary_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"call()"},{"location":"api/losses/CategoricalCrossentropy/","text":"elegy.losses.CategoricalCrossentropy Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Usage: y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 0 , 1 ]]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cce = elegy . losses . CategoricalCrossentropy () assert cce ( y_true , y_pred ) == 1.177 # Calling with 'sample_weight'. assert cce ( y_true , y_pred , sample_weight = tf . constant ([ 0.3 , 0.7 ])) == 0.814 # Using 'sum' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) assert cce ( y_true , y_pred ) == 2.354 # Using 'none' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) assert list ( cce ( y_true , y_pred )) == [ 0.0513 , 2.303 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) __init__ ( self , from_logits = False , label_smoothing = 0 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/categorical_crossentropy.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing call ( self , y_true , y_pred , sample_weight = None ) Invokes the CategoricalCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/categorical_crossentropy.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `CategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegylossescategoricalcrossentropy","text":"","title":"elegy.losses.CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy","text":"Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Usage: y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 0 , 1 ]]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cce = elegy . losses . CategoricalCrossentropy () assert cce ( y_true , y_pred ) == 1.177 # Calling with 'sample_weight'. assert cce ( y_true , y_pred , sample_weight = tf . constant ([ 0.3 , 0.7 ])) == 0.814 # Using 'sum' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) assert cce ( y_true , y_pred ) == 2.354 # Using 'none' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) assert list ( cce ( y_true , y_pred )) == [ 0.0513 , 2.303 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"elegy.losses.categorical_crossentropy.CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.__init__","text":"Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/categorical_crossentropy.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.call","text":"Invokes the CategoricalCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/categorical_crossentropy.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `CategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"call()"},{"location":"api/losses/CosineSimilarity/","text":"elegy.losses.CosineSimilarity Computes the mean squared logarithmic errors between labels and predictions. loss = -sum(l2_norm(y_true) * l2_norm(y_pred)) Usage: y_true = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) y_pred = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = elegy . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( y_true , y_pred ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( y_true , y_pred , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = elegy . losses . CosineSimilarity ( axis = 1 , reduction = elegy . losses . Reduction . SUM ) assert cosine_loss ( y_true , y_pred ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = elegy . losses . CosineSimilarity ( axis = 1 , reduction = elegy . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( y_true , y_pred ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), ) __init__ ( self , axis =- 1 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/cosine_similarity.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the CosineSimilarity instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/cosine_similarity.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( y_true , y_pred , self . axis )","title":"CosineSimilarity"},{"location":"api/losses/CosineSimilarity/#elegylossescosinesimilarity","text":"","title":"elegy.losses.CosineSimilarity"},{"location":"api/losses/CosineSimilarity/#elegy.losses.cosine_similarity.CosineSimilarity","text":"Computes the mean squared logarithmic errors between labels and predictions. loss = -sum(l2_norm(y_true) * l2_norm(y_pred)) Usage: y_true = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) y_pred = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = elegy . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( y_true , y_pred ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( y_true , y_pred , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = elegy . losses . CosineSimilarity ( axis = 1 , reduction = elegy . losses . Reduction . SUM ) assert cosine_loss ( y_true , y_pred ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = elegy . losses . CosineSimilarity ( axis = 1 , reduction = elegy . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( y_true , y_pred ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), )","title":"elegy.losses.cosine_similarity.CosineSimilarity"},{"location":"api/losses/CosineSimilarity/#elegy.losses.cosine_similarity.CosineSimilarity.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/cosine_similarity.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/CosineSimilarity/#elegy.losses.cosine_similarity.CosineSimilarity.call","text":"Invokes the CosineSimilarity instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/cosine_similarity.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( y_true , y_pred , self . axis )","title":"call()"},{"location":"api/losses/Huber/","text":"elegy.losses.Huber Computes the Huber loss between labels and predictions. For each value x in error = y_true - y_pred: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: y_true = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) y_pred = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = elegy . losses . Huber () assert huber_loss ( y_true , y_pred ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( y_true , y_pred , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = elegy . losses . Huber ( reduction = elegy . losses . Reduction . SUM ) assert huber_loss ( y_true , y_pred ) == 0.31 # Using 'none' reduction type. huber_loss = elegy . losses . Huber ( reduction = elegy . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( y_true , y_pred ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), ) __init__ ( self , delta = 1.0 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/huber.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the Huber instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/huber.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( y_true , y_pred , self . delta )","title":"Huber"},{"location":"api/losses/Huber/#elegylosseshuber","text":"","title":"elegy.losses.Huber"},{"location":"api/losses/Huber/#elegy.losses.huber.Huber","text":"Computes the Huber loss between labels and predictions. For each value x in error = y_true - y_pred: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: y_true = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) y_pred = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = elegy . losses . Huber () assert huber_loss ( y_true , y_pred ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( y_true , y_pred , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = elegy . losses . Huber ( reduction = elegy . losses . Reduction . SUM ) assert huber_loss ( y_true , y_pred ) == 0.31 # Using 'none' reduction type. huber_loss = elegy . losses . Huber ( reduction = elegy . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( y_true , y_pred ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), )","title":"elegy.losses.huber.Huber"},{"location":"api/losses/Huber/#elegy.losses.huber.Huber.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/huber.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/Huber/#elegy.losses.huber.Huber.call","text":"Invokes the Huber instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/huber.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( y_true , y_pred , self . delta )","title":"call()"},{"location":"api/losses/Loss/","text":"elegy.losses.Loss Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/loss.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"Loss"},{"location":"api/losses/Loss/#elegylossesloss","text":"","title":"elegy.losses.Loss"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss","text":"Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this.","title":"elegy.losses.loss.Loss"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/loss.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"__init__()"},{"location":"api/losses/MeanAbsoluteError/","text":"elegy.losses.MeanAbsoluteError Computes the mean absolute errors between labels and predictions. loss = mean(abs(y_true - y_pred)) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = elegy . losses . MeanAbsoluteError () assert mae ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mae ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = elegy . losses . MeanAbsoluteError ( reduction = elegy . losses . Reduction . SUM ) assert mae ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mae = elegy . losses . MeanAbsoluteError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mae ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_absolute_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_error.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( y_true , y_pred )","title":"MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#elegylossesmeanabsoluteerror","text":"","title":"elegy.losses.MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError","text":"Computes the mean absolute errors between labels and predictions. loss = mean(abs(y_true - y_pred)) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = elegy . losses . MeanAbsoluteError () assert mae ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mae ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = elegy . losses . MeanAbsoluteError ( reduction = elegy . losses . Reduction . SUM ) assert mae ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mae = elegy . losses . MeanAbsoluteError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mae ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), )","title":"elegy.losses.mean_absolute_error.MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_absolute_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.call","text":"Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_error.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/MeanAbsolutePercentageError/","text":"elegy.losses.MeanAbsolutePercentageError Computes the mean absolute errors between labels and predictions. loss = mean(abs((y_true - y_pred) / y_true)) Usage: y_true = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = elegy . losses . MeanAbsolutePercentageError () result = mape ( y_true , y_pred ) assert jnp . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert jnp . isclose ( mape ( y_true , y_pred , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = elegy . losses . MeanAbsolutePercentageError ( reduction = elegy . losses . Reduction . SUM ) assert jnp . isclose ( mape ( y_true , y_pred ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = elegy . losses . MeanAbsolutePercentageError ( reduction = elegy . losses . Reduction . NONE ) assert jnp . all ( jnp . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_absolute_percentage_error.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_percentage_error.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( y_true , y_pred )","title":"MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#elegylossesmeanabsolutepercentageerror","text":"","title":"elegy.losses.MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError","text":"Computes the mean absolute errors between labels and predictions. loss = mean(abs((y_true - y_pred) / y_true)) Usage: y_true = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = elegy . losses . MeanAbsolutePercentageError () result = mape ( y_true , y_pred ) assert jnp . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert jnp . isclose ( mape ( y_true , y_pred , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = elegy . losses . MeanAbsolutePercentageError ( reduction = elegy . losses . Reduction . SUM ) assert jnp . isclose ( mape ( y_true , y_pred ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = elegy . losses . MeanAbsolutePercentageError ( reduction = elegy . losses . Reduction . NONE ) assert jnp . all ( jnp . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), )","title":"elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_absolute_percentage_error.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.call","text":"Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_percentage_error.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/MeanSquaredError/","text":"elegy.losses.MeanSquaredError Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = elegy . losses . MeanSquaredError () assert mse ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mse ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . SUM ) assert mse ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mse ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_squared_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanSquaredError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_error.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( y_true , y_pred )","title":"MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegylossesmeansquarederror","text":"","title":"elegy.losses.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError","text":"Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = elegy . losses . MeanSquaredError () assert mse ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mse ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . SUM ) assert mse ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mse ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), )","title":"elegy.losses.mean_squared_error.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_squared_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.call","text":"Invokes the MeanSquaredError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_error.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/MeanSquaredLogarithmicError/","text":"elegy.losses.MeanSquaredLogarithmicError Computes the mean squared logarithmic errors between labels and predictions. loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = elegy . losses . MeanSquaredLogarithmicError () assert msle ( y_true , y_pred ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = elegy . losses . MeanSquaredLogarithmicError ( reduction = elegy . losses . Reduction . SUM ) assert msle ( y_true , y_pred ) == 0.48045287 # Using 'none' reduction type. msle = elegy . losses . MeanSquaredLogarithmicError ( reduction = elegy . losses . Reduction . NONE ) assert jnp . equal ( msle ( y_true , y_pred ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_squared_logarithmic_error.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_logarithmic_error.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( y_true , y_pred )","title":"MeanSquaredLogarithmicError"},{"location":"api/losses/MeanSquaredLogarithmicError/#elegylossesmeansquaredlogarithmicerror","text":"","title":"elegy.losses.MeanSquaredLogarithmicError"},{"location":"api/losses/MeanSquaredLogarithmicError/#elegy.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError","text":"Computes the mean squared logarithmic errors between labels and predictions. loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = elegy . losses . MeanSquaredLogarithmicError () assert msle ( y_true , y_pred ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = elegy . losses . MeanSquaredLogarithmicError ( reduction = elegy . losses . Reduction . SUM ) assert msle ( y_true , y_pred ) == 0.48045287 # Using 'none' reduction type. msle = elegy . losses . MeanSquaredLogarithmicError ( reduction = elegy . losses . Reduction . NONE ) assert jnp . equal ( msle ( y_true , y_pred ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), )","title":"elegy.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError"},{"location":"api/losses/MeanSquaredLogarithmicError/#elegy.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_squared_logarithmic_error.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanSquaredLogarithmicError/#elegy.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.call","text":"Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_logarithmic_error.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/Reduction/","text":"elegy.losses.Reduction Types of loss reduction. Contains the following values: * NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * SUM : Scalar sum of weighted losses. * SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses. __class__ inherited Metaclass for Enum __members__ property readonly special Returns a mapping of member name->value. This mapping lists all enum members, including aliases. Note that this is a read-only view of the internal mapping. __bool__ ( self ) special classes/types should always be True. Source code in elegy/losses/loss.py 272 273 274 275 276 def __bool__ ( self ): \"\"\" classes/types should always be True. \"\"\" return True __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ) special Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: value will be the name of the new class. names should be either a string of white-space/comma delimited names (values will start at start ), or an iterator/mapping of name, value pairs. module should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. qualname should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. type , if set, will be mixed in as the first base class. Source code in elegy/losses/loss.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ): \"\"\"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: `value` will be the name of the new class. `names` should be either a string of white-space/comma delimited names (values will start at `start`), or an iterator/mapping of name, value pairs. `module` should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. `qualname` should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. `type`, if set, will be mixed in as the first base class. \"\"\" if names is None : # simple value lookup return cls . __new__ ( cls , value ) # otherwise, functional API: we're creating a new Enum type return cls . _create_ ( value , names , module = module , qualname = qualname , type = type , start = start ) __getattr__ ( cls , name ) special Return the enum member matching name We use getattr instead of descriptors or inserting into the enum class' dict in order to support name and value being both properties for enum members (which live in the class' dict ) and enum members themselves. Source code in elegy/losses/loss.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def __getattr__ ( cls , name ): \"\"\"Return the enum member matching `name` We use __getattr__ instead of descriptors or inserting into the enum class' __dict__ in order to support `name` and `value` being both properties for enum members (which live in the class' __dict__) and enum members themselves. \"\"\" if _is_dunder ( name ): raise AttributeError ( name ) try : return cls . _member_map_ [ name ] except KeyError : raise AttributeError ( name ) from None __new__ ( metacls , cls , bases , classdict ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/losses/loss.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def __new__ ( metacls , cls , bases , classdict ): # an Enum class is final once enumeration items have been defined; it # cannot be mixed with other types (int, float, etc.) if it has an # inherited __new__ unless a new __new__ is defined (or the resulting # class will fail). # # remove any keys listed in _ignore_ classdict . setdefault ( '_ignore_' , []) . append ( '_ignore_' ) ignore = classdict [ '_ignore_' ] for key in ignore : classdict . pop ( key , None ) member_type , first_enum = metacls . _get_mixins_ ( bases ) __new__ , save_new , use_args = metacls . _find_new_ ( classdict , member_type , first_enum ) # save enum items into separate mapping so they don't get baked into # the new class enum_members = { k : classdict [ k ] for k in classdict . _member_names } for name in classdict . _member_names : del classdict [ name ] # adjust the sunders _order_ = classdict . pop ( '_order_' , None ) # check for illegal enum names (any others?) invalid_names = set ( enum_members ) & { 'mro' , '' } if invalid_names : raise ValueError ( 'Invalid enum member name: {0} ' . format ( ',' . join ( invalid_names ))) # create a default docstring if one has not been provided if '__doc__' not in classdict : classdict [ '__doc__' ] = 'An enumeration.' # create our new Enum type enum_class = super () . __new__ ( metacls , cls , bases , classdict ) enum_class . _member_names_ = [] # names in definition order enum_class . _member_map_ = {} # name->value map enum_class . _member_type_ = member_type # save DynamicClassAttribute attributes from super classes so we know # if we can take the shortcut of storing members in the class dict dynamic_attributes = { k for c in enum_class . mro () for k , v in c . __dict__ . items () if isinstance ( v , DynamicClassAttribute )} # Reverse value->name map for hashable values. enum_class . _value2member_map_ = {} # If a custom type is mixed into the Enum, and it does not know how # to pickle itself, pickle.dumps will succeed but pickle.loads will # fail. Rather than have the error show up later and possibly far # from the source, sabotage the pickle protocol for this class so # that pickle.dumps also fails. # # However, if the new class implements its own __reduce_ex__, do not # sabotage -- it's on them to make sure it works correctly. We use # __reduce_ex__ instead of any of the others as it is preferred by # pickle over __reduce__, and it handles all pickle protocols. if '__reduce_ex__' not in classdict : if member_type is not object : methods = ( '__getnewargs_ex__' , '__getnewargs__' , '__reduce_ex__' , '__reduce__' ) if not any ( m in member_type . __dict__ for m in methods ): _make_class_unpicklable ( enum_class ) # instantiate them, checking for duplicates as we go # we instantiate first instead of checking for duplicates first in case # a custom __new__ is doing something funky with the values -- such as # auto-numbering ;) for member_name in classdict . _member_names : value = enum_members [ member_name ] if not isinstance ( value , tuple ): args = ( value , ) else : args = value if member_type is tuple : # special case for tuple enums args = ( args , ) # wrap it one more time if not use_args : enum_member = __new__ ( enum_class ) if not hasattr ( enum_member , '_value_' ): enum_member . _value_ = value else : enum_member = __new__ ( enum_class , * args ) if not hasattr ( enum_member , '_value_' ): if member_type is object : enum_member . _value_ = value else : enum_member . _value_ = member_type ( * args ) value = enum_member . _value_ enum_member . _name_ = member_name enum_member . __objclass__ = enum_class enum_member . __init__ ( * args ) # If another member with the same value was already defined, the # new member becomes an alias to the existing one. for name , canonical_member in enum_class . _member_map_ . items (): if canonical_member . _value_ == enum_member . _value_ : enum_member = canonical_member break else : # Aliases don't appear in member names (only in __members__). enum_class . _member_names_ . append ( member_name ) # performance boost for any member that would not shadow # a DynamicClassAttribute if member_name not in dynamic_attributes : setattr ( enum_class , member_name , enum_member ) # now add to _member_map_ enum_class . _member_map_ [ member_name ] = enum_member try : # This may fail if value is not hashable. We can't add the value # to the map, and by-value lookups for this value will be # linear. enum_class . _value2member_map_ [ value ] = enum_member except TypeError : pass # double check that repr and friends are not the mixin's or various # things break (such as pickle) for name in ( '__repr__' , '__str__' , '__format__' , '__reduce_ex__' ): class_method = getattr ( enum_class , name ) obj_method = getattr ( member_type , name , None ) enum_method = getattr ( first_enum , name , None ) if obj_method is not None and obj_method is class_method : setattr ( enum_class , name , enum_method ) # replace any other __new__ with our own (as long as Enum is not None, # anyway) -- again, this is to support pickle if Enum is not None : # if the user defined their own __new__, save it before it gets # clobbered in case they subclass later if save_new : enum_class . __new_member__ = __new__ enum_class . __new__ = Enum . __new__ # py3 support for definition order (helps keep py2/py3 code in sync) if _order_ is not None : if isinstance ( _order_ , str ): _order_ = _order_ . replace ( ',' , ' ' ) . split () if _order_ != enum_class . _member_names_ : raise TypeError ( 'member order does not match _order_' ) return enum_class __prepare__ ( cls , bases ) classmethod special prepare () -> dict used to create the namespace for the class statement Source code in elegy/losses/loss.py 119 120 121 122 123 124 125 126 127 @classmethod def __prepare__ ( metacls , cls , bases ): # create the namespace dict enum_dict = _EnumDict () # inherit previous flags and _generate_next_value_ function member_type , first_enum = metacls . _get_mixins_ ( bases ) if first_enum is not None : enum_dict [ '_generate_next_value_' ] = getattr ( first_enum , '_generate_next_value_' , None ) return enum_dict __setattr__ ( cls , name , value ) special Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. Source code in elegy/losses/loss.py 368 369 370 371 372 373 374 375 376 377 378 379 def __setattr__ ( cls , name , value ): \"\"\"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. \"\"\" member_map = cls . __dict__ . get ( '_member_map_' , {}) if name in member_map : raise AttributeError ( 'Cannot reassign members.' ) super () . __setattr__ ( name , value )","title":"Reduction"},{"location":"api/losses/Reduction/#elegylossesreduction","text":"","title":"elegy.losses.Reduction"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction","text":"Types of loss reduction. Contains the following values: * NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * SUM : Scalar sum of weighted losses. * SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses.","title":"elegy.losses.loss.Reduction"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__","text":"Metaclass for Enum","title":"__class__"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__members__","text":"Returns a mapping of member name->value. This mapping lists all enum members, including aliases. Note that this is a read-only view of the internal mapping.","title":"__members__"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__bool__","text":"classes/types should always be True. Source code in elegy/losses/loss.py 272 273 274 275 276 def __bool__ ( self ): \"\"\" classes/types should always be True. \"\"\" return True","title":"__bool__()"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__call__","text":"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: value will be the name of the new class. names should be either a string of white-space/comma delimited names (values will start at start ), or an iterator/mapping of name, value pairs. module should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. qualname should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. type , if set, will be mixed in as the first base class. Source code in elegy/losses/loss.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ): \"\"\"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: `value` will be the name of the new class. `names` should be either a string of white-space/comma delimited names (values will start at `start`), or an iterator/mapping of name, value pairs. `module` should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. `qualname` should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. `type`, if set, will be mixed in as the first base class. \"\"\" if names is None : # simple value lookup return cls . __new__ ( cls , value ) # otherwise, functional API: we're creating a new Enum type return cls . _create_ ( value , names , module = module , qualname = qualname , type = type , start = start )","title":"__call__()"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__getattr__","text":"Return the enum member matching name We use getattr instead of descriptors or inserting into the enum class' dict in order to support name and value being both properties for enum members (which live in the class' dict ) and enum members themselves. Source code in elegy/losses/loss.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def __getattr__ ( cls , name ): \"\"\"Return the enum member matching `name` We use __getattr__ instead of descriptors or inserting into the enum class' __dict__ in order to support `name` and `value` being both properties for enum members (which live in the class' __dict__) and enum members themselves. \"\"\" if _is_dunder ( name ): raise AttributeError ( name ) try : return cls . _member_map_ [ name ] except KeyError : raise AttributeError ( name ) from None","title":"__getattr__()"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/losses/loss.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def __new__ ( metacls , cls , bases , classdict ): # an Enum class is final once enumeration items have been defined; it # cannot be mixed with other types (int, float, etc.) if it has an # inherited __new__ unless a new __new__ is defined (or the resulting # class will fail). # # remove any keys listed in _ignore_ classdict . setdefault ( '_ignore_' , []) . append ( '_ignore_' ) ignore = classdict [ '_ignore_' ] for key in ignore : classdict . pop ( key , None ) member_type , first_enum = metacls . _get_mixins_ ( bases ) __new__ , save_new , use_args = metacls . _find_new_ ( classdict , member_type , first_enum ) # save enum items into separate mapping so they don't get baked into # the new class enum_members = { k : classdict [ k ] for k in classdict . _member_names } for name in classdict . _member_names : del classdict [ name ] # adjust the sunders _order_ = classdict . pop ( '_order_' , None ) # check for illegal enum names (any others?) invalid_names = set ( enum_members ) & { 'mro' , '' } if invalid_names : raise ValueError ( 'Invalid enum member name: {0} ' . format ( ',' . join ( invalid_names ))) # create a default docstring if one has not been provided if '__doc__' not in classdict : classdict [ '__doc__' ] = 'An enumeration.' # create our new Enum type enum_class = super () . __new__ ( metacls , cls , bases , classdict ) enum_class . _member_names_ = [] # names in definition order enum_class . _member_map_ = {} # name->value map enum_class . _member_type_ = member_type # save DynamicClassAttribute attributes from super classes so we know # if we can take the shortcut of storing members in the class dict dynamic_attributes = { k for c in enum_class . mro () for k , v in c . __dict__ . items () if isinstance ( v , DynamicClassAttribute )} # Reverse value->name map for hashable values. enum_class . _value2member_map_ = {} # If a custom type is mixed into the Enum, and it does not know how # to pickle itself, pickle.dumps will succeed but pickle.loads will # fail. Rather than have the error show up later and possibly far # from the source, sabotage the pickle protocol for this class so # that pickle.dumps also fails. # # However, if the new class implements its own __reduce_ex__, do not # sabotage -- it's on them to make sure it works correctly. We use # __reduce_ex__ instead of any of the others as it is preferred by # pickle over __reduce__, and it handles all pickle protocols. if '__reduce_ex__' not in classdict : if member_type is not object : methods = ( '__getnewargs_ex__' , '__getnewargs__' , '__reduce_ex__' , '__reduce__' ) if not any ( m in member_type . __dict__ for m in methods ): _make_class_unpicklable ( enum_class ) # instantiate them, checking for duplicates as we go # we instantiate first instead of checking for duplicates first in case # a custom __new__ is doing something funky with the values -- such as # auto-numbering ;) for member_name in classdict . _member_names : value = enum_members [ member_name ] if not isinstance ( value , tuple ): args = ( value , ) else : args = value if member_type is tuple : # special case for tuple enums args = ( args , ) # wrap it one more time if not use_args : enum_member = __new__ ( enum_class ) if not hasattr ( enum_member , '_value_' ): enum_member . _value_ = value else : enum_member = __new__ ( enum_class , * args ) if not hasattr ( enum_member , '_value_' ): if member_type is object : enum_member . _value_ = value else : enum_member . _value_ = member_type ( * args ) value = enum_member . _value_ enum_member . _name_ = member_name enum_member . __objclass__ = enum_class enum_member . __init__ ( * args ) # If another member with the same value was already defined, the # new member becomes an alias to the existing one. for name , canonical_member in enum_class . _member_map_ . items (): if canonical_member . _value_ == enum_member . _value_ : enum_member = canonical_member break else : # Aliases don't appear in member names (only in __members__). enum_class . _member_names_ . append ( member_name ) # performance boost for any member that would not shadow # a DynamicClassAttribute if member_name not in dynamic_attributes : setattr ( enum_class , member_name , enum_member ) # now add to _member_map_ enum_class . _member_map_ [ member_name ] = enum_member try : # This may fail if value is not hashable. We can't add the value # to the map, and by-value lookups for this value will be # linear. enum_class . _value2member_map_ [ value ] = enum_member except TypeError : pass # double check that repr and friends are not the mixin's or various # things break (such as pickle) for name in ( '__repr__' , '__str__' , '__format__' , '__reduce_ex__' ): class_method = getattr ( enum_class , name ) obj_method = getattr ( member_type , name , None ) enum_method = getattr ( first_enum , name , None ) if obj_method is not None and obj_method is class_method : setattr ( enum_class , name , enum_method ) # replace any other __new__ with our own (as long as Enum is not None, # anyway) -- again, this is to support pickle if Enum is not None : # if the user defined their own __new__, save it before it gets # clobbered in case they subclass later if save_new : enum_class . __new_member__ = __new__ enum_class . __new__ = Enum . __new__ # py3 support for definition order (helps keep py2/py3 code in sync) if _order_ is not None : if isinstance ( _order_ , str ): _order_ = _order_ . replace ( ',' , ' ' ) . split () if _order_ != enum_class . _member_names_ : raise TypeError ( 'member order does not match _order_' ) return enum_class","title":"__new__()"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__prepare__","text":"prepare () -> dict used to create the namespace for the class statement Source code in elegy/losses/loss.py 119 120 121 122 123 124 125 126 127 @classmethod def __prepare__ ( metacls , cls , bases ): # create the namespace dict enum_dict = _EnumDict () # inherit previous flags and _generate_next_value_ function member_type , first_enum = metacls . _get_mixins_ ( bases ) if first_enum is not None : enum_dict [ '_generate_next_value_' ] = getattr ( first_enum , '_generate_next_value_' , None ) return enum_dict","title":"__prepare__()"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__setattr__","text":"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. Source code in elegy/losses/loss.py 368 369 370 371 372 373 374 375 376 377 378 379 def __setattr__ ( cls , name , value ): \"\"\"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. \"\"\" member_map = cls . __dict__ . get ( '_member_map_' , {}) if name in member_map : raise AttributeError ( 'Cannot reassign members.' ) super () . __setattr__ ( name , value )","title":"__setattr__()"},{"location":"api/losses/SparseCategoricalCrossentropy/","text":"elegy.losses.SparseCategoricalCrossentropy Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Usage: y_true = jnp . array ([ 1 , 2 ]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy () result = scce ( y_true , y_pred ) # 1.177 assert jnp . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( y_true , y_pred , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert jnp . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = scce ( y_true , y_pred ) # 2.354 assert jnp . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = scce ( y_true , y_pred ) # [0.0513, 2.303] assert jnp . all ( jnp . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . SparseCategoricalCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) __init__ ( self , from_logits = False , reduction = None , weight = None , on = None , check_bounds = True , ** kwargs ) special Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None check_bounds Optional[bool] If True (default), checks y_true for negative values and values larger or equal than the number of channels in y_pred . Sets loss to NaN if this is the case. If False , the check is disabled and the loss may contain incorrect values. True Source code in elegy/losses/sparse_categorical_crossentropy.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def __init__ ( self , from_logits : bool = False , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , check_bounds : tp . Optional [ bool ] = True , ** kwargs ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). check_bounds: If `True` (default), checks `y_true` for negative values and values larger or equal than the number of channels in `y_pred`. Sets loss to NaN if this is the case. If `False`, the check is disabled and the loss may contain incorrect values. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _check_bounds = check_bounds call ( self , y_true , y_pred , sample_weight = None ) Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default y_true Ground truth values. required y_pred The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/sparse_categorical_crossentropy.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def call ( self , y_true , y_pred , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return sparse_categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , check_bounds = self . _check_bounds , )","title":"SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegylossessparsecategoricalcrossentropy","text":"","title":"elegy.losses.SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy","text":"Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Usage: y_true = jnp . array ([ 1 , 2 ]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy () result = scce ( y_true , y_pred ) # 1.177 assert jnp . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( y_true , y_pred , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert jnp . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = scce ( y_true , y_pred ) # 2.354 assert jnp . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = scce ( y_true , y_pred ) # [0.0513, 2.303] assert jnp . all ( jnp . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . SparseCategoricalCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.__init__","text":"Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None check_bounds Optional[bool] If True (default), checks y_true for negative values and values larger or equal than the number of channels in y_pred . Sets loss to NaN if this is the case. If False , the check is disabled and the loss may contain incorrect values. True Source code in elegy/losses/sparse_categorical_crossentropy.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def __init__ ( self , from_logits : bool = False , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , check_bounds : tp . Optional [ bool ] = True , ** kwargs ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). check_bounds: If `True` (default), checks `y_true` for negative values and values larger or equal than the number of channels in `y_pred`. Sets loss to NaN if this is the case. If `False`, the check is disabled and the loss may contain incorrect values. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _check_bounds = check_bounds","title":"__init__()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.call","text":"Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default y_true Ground truth values. required y_pred The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/sparse_categorical_crossentropy.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def call ( self , y_true , y_pred , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return sparse_categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , check_bounds = self . _check_bounds , )","title":"call()"},{"location":"api/losses/binary_crossentropy/","text":"elegy.losses.binary_crossentropy Source code in elegy/losses/binary_crossentropy.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def binary_crossentropy ( y_true : jnp . ndarray , y_pred : jnp . ndarray , from_logits : bool = False ) -> jnp . ndarray : assert abs ( y_pred . ndim - y_true . ndim ) <= 1 y_true , y_pred = utils . maybe_expand_dims ( y_true , y_pred ) if from_logits : return - jnp . mean ( y_true * y_pred - jnp . logaddexp ( 0.0 , y_pred ), axis =- 1 ) y_pred = jnp . clip ( y_pred , utils . EPSILON , 1.0 - utils . EPSILON ) return - jnp . mean ( y_true * jnp . log ( y_pred ) + ( 1 - y_true ) * jnp . log ( 1 - y_pred ), axis =- 1 )","title":"binary_crossentropy"},{"location":"api/losses/binary_crossentropy/#elegylossesbinary_crossentropy","text":"","title":"elegy.losses.binary_crossentropy"},{"location":"api/losses/binary_crossentropy/#elegy.losses.binary_crossentropy.binary_crossentropy","text":"Source code in elegy/losses/binary_crossentropy.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def binary_crossentropy ( y_true : jnp . ndarray , y_pred : jnp . ndarray , from_logits : bool = False ) -> jnp . ndarray : assert abs ( y_pred . ndim - y_true . ndim ) <= 1 y_true , y_pred = utils . maybe_expand_dims ( y_true , y_pred ) if from_logits : return - jnp . mean ( y_true * y_pred - jnp . logaddexp ( 0.0 , y_pred ), axis =- 1 ) y_pred = jnp . clip ( y_pred , utils . EPSILON , 1.0 - utils . EPSILON ) return - jnp . mean ( y_true * jnp . log ( y_pred ) + ( 1 - y_true ) * jnp . log ( 1 - y_pred ), axis =- 1 )","title":"elegy.losses.binary_crossentropy.binary_crossentropy"},{"location":"api/losses/cosine_similarity/","text":"elegy.losses.cosine_similarity Computes the cosine similarity between labels and predictions. loss = - sum ( l2_norm ( y_true ) * l2_norm ( y_pred )) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . cosine_similarity ( y_true , y_pred , axis = 1 ) assert loss . shape == ( 2 ,) y_true = y_true / jnp . maximum ( jnp . linalg . norm ( y_true , axis = 1 , keepdims = True ), jnp . sqrt ( utils . EPSILON )) y_pred = y_pred / jnp . maximum ( jnp . linalg . norm ( y_pred , axis = 1 , keepdims = True ), jnp . sqrt ( utils . EPSILON )) assert jnp . array_equal ( loss , - jnp . sum ( y_true * y_pred , axis = 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required axis int The dimension along which the cosinemsimilarity is computed. required Returns: Type Description ndarray cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in elegy/losses/cosine_similarity.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def cosine_similarity ( y_true : jnp . ndarray , y_pred : jnp . ndarray , axis : int ) -> jnp . ndarray : \"\"\" Computes the cosine similarity between labels and predictions. ```python loss = -sum(l2_norm(y_true) * l2_norm(y_pred)) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.cosine_similarity(y_true, y_pred, axis=1) assert loss.shape == (2,) y_true = y_true / jnp.maximum(jnp.linalg.norm(y_true, axis=1, keepdims=True), jnp.sqrt(utils.EPSILON)) y_pred = y_pred / jnp.maximum(jnp.linalg.norm(y_pred, axis=1, keepdims=True), jnp.sqrt(utils.EPSILON)) assert jnp.array_equal(loss, -jnp.sum(y_true * y_pred, axis=1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. axis: The dimension along which the cosinemsimilarity is computed. Returns: cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" y_true = y_true / jnp . maximum ( jnp . linalg . norm ( y_true , axis = axis , keepdims = True ), jnp . sqrt ( utils . EPSILON ) ) y_pred = y_pred / jnp . maximum ( jnp . linalg . norm ( y_pred , axis = axis , keepdims = True ), jnp . sqrt ( utils . EPSILON ) ) return - jnp . sum ( y_true * y_pred , axis = axis )","title":"cosine_similarity"},{"location":"api/losses/cosine_similarity/#elegylossescosine_similarity","text":"","title":"elegy.losses.cosine_similarity"},{"location":"api/losses/cosine_similarity/#elegy.losses.cosine_similarity.cosine_similarity","text":"Computes the cosine similarity between labels and predictions. loss = - sum ( l2_norm ( y_true ) * l2_norm ( y_pred )) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . cosine_similarity ( y_true , y_pred , axis = 1 ) assert loss . shape == ( 2 ,) y_true = y_true / jnp . maximum ( jnp . linalg . norm ( y_true , axis = 1 , keepdims = True ), jnp . sqrt ( utils . EPSILON )) y_pred = y_pred / jnp . maximum ( jnp . linalg . norm ( y_pred , axis = 1 , keepdims = True ), jnp . sqrt ( utils . EPSILON )) assert jnp . array_equal ( loss , - jnp . sum ( y_true * y_pred , axis = 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required axis int The dimension along which the cosinemsimilarity is computed. required Returns: Type Description ndarray cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in elegy/losses/cosine_similarity.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def cosine_similarity ( y_true : jnp . ndarray , y_pred : jnp . ndarray , axis : int ) -> jnp . ndarray : \"\"\" Computes the cosine similarity between labels and predictions. ```python loss = -sum(l2_norm(y_true) * l2_norm(y_pred)) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.cosine_similarity(y_true, y_pred, axis=1) assert loss.shape == (2,) y_true = y_true / jnp.maximum(jnp.linalg.norm(y_true, axis=1, keepdims=True), jnp.sqrt(utils.EPSILON)) y_pred = y_pred / jnp.maximum(jnp.linalg.norm(y_pred, axis=1, keepdims=True), jnp.sqrt(utils.EPSILON)) assert jnp.array_equal(loss, -jnp.sum(y_true * y_pred, axis=1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. axis: The dimension along which the cosinemsimilarity is computed. Returns: cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" y_true = y_true / jnp . maximum ( jnp . linalg . norm ( y_true , axis = axis , keepdims = True ), jnp . sqrt ( utils . EPSILON ) ) y_pred = y_pred / jnp . maximum ( jnp . linalg . norm ( y_pred , axis = axis , keepdims = True ), jnp . sqrt ( utils . EPSILON ) ) return - jnp . sum ( y_true * y_pred , axis = axis )","title":"elegy.losses.cosine_similarity.cosine_similarity"},{"location":"api/losses/huber/","text":"elegy.losses.huber Computes the Huber loss between labels and predictions. For each value x in error = y_true - y_pred: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . huber ( y_true , y_pred , delta = 1.0 ) assert loss . shape == ( 2 ,) y_pred = y_pred . astype ( float ) y_true = y_true . astype ( float ) delta = 1.0 error = jnp . subtract ( y_pred , y_true ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) assert jnp . array_equal ( loss , jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic ) ), jnp . multiply ( delta , linear )), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required delta float A float, the point where the Huber loss function changes from a quadratic to linear. required Returns: Type Description ndarray huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in elegy/losses/huber.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def huber ( y_true : jnp . ndarray , y_pred : jnp . ndarray , delta : float ) -> jnp . ndarray : r \"\"\" Computes the Huber loss between labels and predictions. For each value x in error = y_true - y_pred: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.huber(y_true, y_pred, delta=1.0) assert loss.shape == (2,) y_pred = y_pred.astype(float) y_true = y_true.astype(float) delta = 1.0 error = jnp.subtract(y_pred, y_true) abs_error = jnp.abs(error) quadratic = jnp.minimum(abs_error, delta) linear = jnp.subtract(abs_error, quadratic) assert jnp.array_equal(loss, jnp.mean( jnp.add( jnp.multiply( 0.5, jnp.multiply(quadratic, quadratic) ), jnp.multiply(delta, linear)), axis=-1 )) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. delta: A float, the point where the Huber loss function changes from a quadratic to linear. Returns: huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" y_pred = y_pred . astype ( float ) y_true = y_true . astype ( float ) delta = float ( delta ) error = jnp . subtract ( y_pred , y_true ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) return jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic )), jnp . multiply ( delta , linear ), ), axis =- 1 , )","title":"huber"},{"location":"api/losses/huber/#elegylosseshuber","text":"","title":"elegy.losses.huber"},{"location":"api/losses/huber/#elegy.losses.huber.huber","text":"Computes the Huber loss between labels and predictions. For each value x in error = y_true - y_pred: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . huber ( y_true , y_pred , delta = 1.0 ) assert loss . shape == ( 2 ,) y_pred = y_pred . astype ( float ) y_true = y_true . astype ( float ) delta = 1.0 error = jnp . subtract ( y_pred , y_true ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) assert jnp . array_equal ( loss , jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic ) ), jnp . multiply ( delta , linear )), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required delta float A float, the point where the Huber loss function changes from a quadratic to linear. required Returns: Type Description ndarray huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in elegy/losses/huber.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def huber ( y_true : jnp . ndarray , y_pred : jnp . ndarray , delta : float ) -> jnp . ndarray : r \"\"\" Computes the Huber loss between labels and predictions. For each value x in error = y_true - y_pred: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.huber(y_true, y_pred, delta=1.0) assert loss.shape == (2,) y_pred = y_pred.astype(float) y_true = y_true.astype(float) delta = 1.0 error = jnp.subtract(y_pred, y_true) abs_error = jnp.abs(error) quadratic = jnp.minimum(abs_error, delta) linear = jnp.subtract(abs_error, quadratic) assert jnp.array_equal(loss, jnp.mean( jnp.add( jnp.multiply( 0.5, jnp.multiply(quadratic, quadratic) ), jnp.multiply(delta, linear)), axis=-1 )) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. delta: A float, the point where the Huber loss function changes from a quadratic to linear. Returns: huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" y_pred = y_pred . astype ( float ) y_true = y_true . astype ( float ) delta = float ( delta ) error = jnp . subtract ( y_pred , y_true ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) return jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic )), jnp . multiply ( delta , linear ), ), axis =- 1 , )","title":"elegy.losses.huber.huber"},{"location":"api/losses/mean_absolute_error/","text":"elegy.losses.mean_absolute_error Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean ( abs ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . abs ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_absolute_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . abs ( y_pred - y_true ), axis =- 1 )","title":"mean_absolute_error"},{"location":"api/losses/mean_absolute_error/#elegylossesmean_absolute_error","text":"","title":"elegy.losses.mean_absolute_error"},{"location":"api/losses/mean_absolute_error/#elegy.losses.mean_absolute_error.mean_absolute_error","text":"Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean ( abs ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . abs ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_absolute_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . abs ( y_pred - y_true ), axis =- 1 )","title":"elegy.losses.mean_absolute_error.mean_absolute_error"},{"location":"api/losses/mean_absolute_percentage_error/","text":"elegy.losses.mean_absolute_percentage_error Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_percentage_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , 100. * jnp . mean ( jnp . abs (( y_pred - y_true ) / jnp . clip ( y_true , utils . EPSILON , None )))) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_percentage_error.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def mean_absolute_percentage_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((y_pred - y_true) / jnp.clip(y_true, utils.EPSILON, None)))) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) diff = jnp . abs (( y_pred - y_true ) / jnp . maximum ( jnp . abs ( y_true ), utils . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"mean_absolute_percentage_error"},{"location":"api/losses/mean_absolute_percentage_error/#elegylossesmean_absolute_percentage_error","text":"","title":"elegy.losses.mean_absolute_percentage_error"},{"location":"api/losses/mean_absolute_percentage_error/#elegy.losses.mean_absolute_percentage_error.mean_absolute_percentage_error","text":"Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_percentage_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , 100. * jnp . mean ( jnp . abs (( y_pred - y_true ) / jnp . clip ( y_true , utils . EPSILON , None )))) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_percentage_error.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def mean_absolute_percentage_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((y_pred - y_true) / jnp.clip(y_true, utils.EPSILON, None)))) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) diff = jnp . abs (( y_pred - y_true ) / jnp . maximum ( jnp . abs ( y_true ), utils . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"elegy.losses.mean_absolute_percentage_error.mean_absolute_percentage_error"},{"location":"api/losses/mean_squared_error/","text":"elegy.losses.mean_squared_error Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"mean_squared_error"},{"location":"api/losses/mean_squared_error/#elegylossesmean_squared_error","text":"","title":"elegy.losses.mean_squared_error"},{"location":"api/losses/mean_squared_error/#elegy.losses.mean_squared_error.mean_squared_error","text":"Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"elegy.losses.mean_squared_error.mean_squared_error"},{"location":"api/losses/mean_squared_logarithmic_error/","text":"elegy.losses.mean_squared_logarithmic_error Computes the mean squared logarithmic error between labels and predictions. loss = mean ( square ( log ( y_true + 1 ) - log ( y_pred + 1 )), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_logarithmic_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) first_log = jnp . log ( jnp . maximum ( y_true , utils . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( y_pred , utils . EPSILON ) + 1.0 ) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_logarithmic_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def mean_squared_logarithmic_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared logarithmic error between labels and predictions. ```python loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) first_log = jnp.log(jnp.maximum(y_true, utils.EPSILON) + 1.0) second_log = jnp.log(jnp.maximum(y_pred, utils.EPSILON) + 1.0) assert jnp.array_equal(loss, jnp.mean(jnp.square(first_log - second_log), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) first_log = jnp . log ( jnp . maximum ( y_true , utils . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( y_pred , utils . EPSILON ) + 1.0 ) return jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )","title":"mean_squared_logarithmic_error"},{"location":"api/losses/mean_squared_logarithmic_error/#elegylossesmean_squared_logarithmic_error","text":"","title":"elegy.losses.mean_squared_logarithmic_error"},{"location":"api/losses/mean_squared_logarithmic_error/#elegy.losses.mean_squared_logarithmic_error.mean_squared_logarithmic_error","text":"Computes the mean squared logarithmic error between labels and predictions. loss = mean ( square ( log ( y_true + 1 ) - log ( y_pred + 1 )), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_logarithmic_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) first_log = jnp . log ( jnp . maximum ( y_true , utils . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( y_pred , utils . EPSILON ) + 1.0 ) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_logarithmic_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def mean_squared_logarithmic_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared logarithmic error between labels and predictions. ```python loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) first_log = jnp.log(jnp.maximum(y_true, utils.EPSILON) + 1.0) second_log = jnp.log(jnp.maximum(y_pred, utils.EPSILON) + 1.0) assert jnp.array_equal(loss, jnp.mean(jnp.square(first_log - second_log), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) first_log = jnp . log ( jnp . maximum ( y_true , utils . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( y_pred , utils . EPSILON ) + 1.0 ) return jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )","title":"elegy.losses.mean_squared_logarithmic_error.mean_squared_logarithmic_error"},{"location":"api/losses/sparse_categorical_crossentropy/","text":"elegy.losses.sparse_categorical_crossentropy Source code in elegy/losses/sparse_categorical_crossentropy.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def sparse_categorical_crossentropy ( y_true : jnp . ndarray , y_pred : jnp . ndarray , from_logits : bool = False , check_bounds : bool = True , ) -> jnp . ndarray : n_classes = y_pred . shape [ - 1 ] if from_logits : y_pred = jax . nn . log_softmax ( y_pred ) loss = - jnp . take_along_axis ( y_pred , y_true [ ... , None ], axis =- 1 )[ ... , 0 ] else : # select output value y_pred = jnp . take_along_axis ( y_pred , y_true [ ... , None ], axis =- 1 )[ ... , 0 ] # calculate log y_pred = jnp . maximum ( y_pred , utils . EPSILON ) y_pred = jnp . log ( y_pred ) loss = - y_pred if check_bounds : # set NaN where y_true is negative or larger/equal to the number of y_pred channels loss = jnp . where ( y_true < 0 , jnp . nan , loss ) loss = jnp . where ( y_true >= n_classes , jnp . nan , loss ) return loss","title":"sparse_categorical_crossentropy"},{"location":"api/losses/sparse_categorical_crossentropy/#elegylossessparse_categorical_crossentropy","text":"","title":"elegy.losses.sparse_categorical_crossentropy"},{"location":"api/losses/sparse_categorical_crossentropy/#elegy.losses.sparse_categorical_crossentropy.sparse_categorical_crossentropy","text":"Source code in elegy/losses/sparse_categorical_crossentropy.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def sparse_categorical_crossentropy ( y_true : jnp . ndarray , y_pred : jnp . ndarray , from_logits : bool = False , check_bounds : bool = True , ) -> jnp . ndarray : n_classes = y_pred . shape [ - 1 ] if from_logits : y_pred = jax . nn . log_softmax ( y_pred ) loss = - jnp . take_along_axis ( y_pred , y_true [ ... , None ], axis =- 1 )[ ... , 0 ] else : # select output value y_pred = jnp . take_along_axis ( y_pred , y_true [ ... , None ], axis =- 1 )[ ... , 0 ] # calculate log y_pred = jnp . maximum ( y_pred , utils . EPSILON ) y_pred = jnp . log ( y_pred ) loss = - y_pred if check_bounds : # set NaN where y_true is negative or larger/equal to the number of y_pred channels loss = jnp . where ( y_true < 0 , jnp . nan , loss ) loss = jnp . where ( y_true >= n_classes , jnp . nan , loss ) return loss","title":"elegy.losses.sparse_categorical_crossentropy.sparse_categorical_crossentropy"},{"location":"api/metrics/Accuracy/","text":"elegy.metrics.Accuracy Calculates how often predictions equals labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. accuracy = elegy . metrics . Accuracy () result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ]) ) assert result == 0.75 # 3 / 4 result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ]) ) assert result == 0.5 # 4 / 8 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) __init__ ( self , on = None , ** kwargs ) special Creates a Accuracy instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/accuracy.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `Accuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/accuracy.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"Accuracy"},{"location":"api/metrics/Accuracy/#elegymetricsaccuracy","text":"","title":"elegy.metrics.Accuracy"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy","text":"Calculates how often predictions equals labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. accuracy = elegy . metrics . Accuracy () result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ]) ) assert result == 0.75 # 3 / 4 result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ]) ) assert result == 0.5 # 4 / 8 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"elegy.metrics.accuracy.Accuracy"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.__init__","text":"Creates a Accuracy instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/accuracy.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `Accuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/accuracy.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/BinaryAccuracy/","text":"elegy.metrics.BinaryAccuracy Calculates how often predictions matches binary labels. Standalone usage: m = elegy . metrics . BinaryAccuracy () result = m ( y_true = np . array ([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]), y_pred = np . array ([[ 0.98 ], [ 1 ], [ 0 ], [ 0.6 ]]), ) assert result == 0.75 m = elegy . metrics . BinaryAccuracy () result = m ( y_true = np . array ([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]), y_pred = np . array ([[ 0.98 ], [ 1 ], [ 0 ], [ 0.6 ]]), sample_weight = np . array ([ 1 , 0 , 0 , 1 ]), ) assert result == 0.5 Usage with Model API: model = elegy . Model ( ... metrics = [ tf . keras . metrics . BinaryAccuracy ()], ) __init__ ( self , threshold = 0.5 , on = None , ** kwargs ) special Creates a BinaryAccuracy instance. Parameters: Name Type Description Default threshold float Float representing the threshold for deciding whether prediction values are 1 or 0. 0.5 on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/binary_accuracy.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def __init__ ( self , threshold : float = 0.5 , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `BinaryAccuracy` instance. Arguments: threshold: Float representing the threshold for deciding whether prediction values are 1 or 0. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = threshold call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/binary_accuracy.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = binary_accuracy ( y_true = y_true , y_pred = y_pred , threshold = self . threshold ), sample_weight = sample_weight , )","title":"BinaryAccuracy"},{"location":"api/metrics/BinaryAccuracy/#elegymetricsbinaryaccuracy","text":"","title":"elegy.metrics.BinaryAccuracy"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy","text":"Calculates how often predictions matches binary labels. Standalone usage: m = elegy . metrics . BinaryAccuracy () result = m ( y_true = np . array ([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]), y_pred = np . array ([[ 0.98 ], [ 1 ], [ 0 ], [ 0.6 ]]), ) assert result == 0.75 m = elegy . metrics . BinaryAccuracy () result = m ( y_true = np . array ([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]), y_pred = np . array ([[ 0.98 ], [ 1 ], [ 0 ], [ 0.6 ]]), sample_weight = np . array ([ 1 , 0 , 0 , 1 ]), ) assert result == 0.5 Usage with Model API: model = elegy . Model ( ... metrics = [ tf . keras . metrics . BinaryAccuracy ()], )","title":"elegy.metrics.binary_accuracy.BinaryAccuracy"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.__init__","text":"Creates a BinaryAccuracy instance. Parameters: Name Type Description Default threshold float Float representing the threshold for deciding whether prediction values are 1 or 0. 0.5 on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/binary_accuracy.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def __init__ ( self , threshold : float = 0.5 , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `BinaryAccuracy` instance. Arguments: threshold: Float representing the threshold for deciding whether prediction values are 1 or 0. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = threshold","title":"__init__()"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/binary_accuracy.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = binary_accuracy ( y_true = y_true , y_pred = y_pred , threshold = self . threshold ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/BinaryCrossentropy/","text":"elegy.metrics.BinaryCrossentropy Computes the crossentropy metric between the labels and predictions. This is the crossentropy metric class to be used when there are only two label classes (0 and 1). Usage: y_true = jnp . array ([[ 0. , 1. ], [ 0. , 0. ]]), y_pred = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) bce = elegy . metrics . BinaryCrossentropy () result = bce ( y_true = y_true , y_pred = y_pred , ) assert jnp . isclose ( result , 0.815 , rtol = 0.01 ) # BCE using sample_weight bce = elegy . metrics . BinaryCrossentropy () result = bce ( y_true , y_pred , sample_weight = jnp . array ([ 1. , 0. ])) assert jnp . isclose ( result , 0.916 , rtol = 0.01 ) Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . BinaryCrossentropy (), ) __init__ ( self , from_logits = False , on = None , ** kwargs ) special Creates a BinaryCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/binary_crossentropy.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , from_logits : bool = False , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `BinaryCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . _from_logits = from_logits call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/binary_crossentropy.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = binary_crossentropy ( y_true = y_true , y_pred = y_pred , from_logits = self . _from_logits ), sample_weight = sample_weight , )","title":"BinaryCrossentropy"},{"location":"api/metrics/BinaryCrossentropy/#elegymetricsbinarycrossentropy","text":"","title":"elegy.metrics.BinaryCrossentropy"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy","text":"Computes the crossentropy metric between the labels and predictions. This is the crossentropy metric class to be used when there are only two label classes (0 and 1). Usage: y_true = jnp . array ([[ 0. , 1. ], [ 0. , 0. ]]), y_pred = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) bce = elegy . metrics . BinaryCrossentropy () result = bce ( y_true = y_true , y_pred = y_pred , ) assert jnp . isclose ( result , 0.815 , rtol = 0.01 ) # BCE using sample_weight bce = elegy . metrics . BinaryCrossentropy () result = bce ( y_true , y_pred , sample_weight = jnp . array ([ 1. , 0. ])) assert jnp . isclose ( result , 0.916 , rtol = 0.01 ) Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . BinaryCrossentropy (), )","title":"elegy.metrics.binary_crossentropy.BinaryCrossentropy"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.__init__","text":"Creates a BinaryCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/binary_crossentropy.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , from_logits : bool = False , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `BinaryCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . _from_logits = from_logits","title":"__init__()"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/binary_crossentropy.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = binary_crossentropy ( y_true = y_true , y_pred = y_pred , from_logits = self . _from_logits ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/CategoricalAccuracy/","text":"elegy.metrics.CategoricalAccuracy Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . CategoricalAccuracy () result = accuracy ( y_true = jnp . array ([[ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . CategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), ) __init__ ( self , on = None , ** kwargs ) special Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/categorical_accuracy.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/categorical_accuracy.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegymetricscategoricalaccuracy","text":"","title":"elegy.metrics.CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy","text":"Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . CategoricalAccuracy () result = accuracy ( y_true = jnp . array ([[ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . CategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"elegy.metrics.categorical_accuracy.CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.__init__","text":"Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/categorical_accuracy.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/categorical_accuracy.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/F1/","text":"elegy.metrics.F1 The metric calculates the armonic mean between precision and recall. This value is ultimately returned as f1 . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values. If class_id is specified, we calculate precision by considering only the entries in the batch for which class_id is above the threshold and computing the fraction of them for which class_id is indeed a correct label. f1 = elegy . metrics . F1 () result = f1 ( y_true = jnp . array ([ 0 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 1 , 1 ]) ) assert result == 0.6666667 # 2 * (0.44445 / 1.33334) result = f1 ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 1 , 0 , 0 ]) ) assert result == 666667 # 2 * (0.5 / 1.5) Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . F1 (), optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , on = None , threshold = None , class_id = None , ** kwargs ) special Creates a F1 instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None threshold (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. None class_id (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/f1.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , threshold = None , class_id = None , ** kwargs ): \"\"\" Creates a `F1` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). threshold: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval `[0, num_classes)`, where `num_classes` is the last dimension of predictions. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = 0.5 if threshold is None else threshold self . class_id = 1 if class_id is None else class_id self . precision = Precision ( threshold = self . threshold ) self . recall = Recall ( threshold = self . threshold ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates f1 values (armonic mean between precision and recall). Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description ndarray Array with the cumulative f1. Source code in elegy/metrics/f1.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates f1 values (armonic mean between precision and recall). Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0, or the same rank as `y_true`, and must be broadcastable to `y_true`. Returns: Array with the cumulative f1. \"\"\" return f1 ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight , precision = self . precision , recall = self . recall , )","title":"F1"},{"location":"api/metrics/F1/#elegymetricsf1","text":"","title":"elegy.metrics.F1"},{"location":"api/metrics/F1/#elegy.metrics.f1.F1","text":"The metric calculates the armonic mean between precision and recall. This value is ultimately returned as f1 . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values. If class_id is specified, we calculate precision by considering only the entries in the batch for which class_id is above the threshold and computing the fraction of them for which class_id is indeed a correct label. f1 = elegy . metrics . F1 () result = f1 ( y_true = jnp . array ([ 0 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 1 , 1 ]) ) assert result == 0.6666667 # 2 * (0.44445 / 1.33334) result = f1 ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 1 , 0 , 0 ]) ) assert result == 666667 # 2 * (0.5 / 1.5) Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . F1 (), optimizer = optix . adam ( 1e-3 ), )","title":"elegy.metrics.f1.F1"},{"location":"api/metrics/F1/#elegy.metrics.f1.F1.__init__","text":"Creates a F1 instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None threshold (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. None class_id (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/f1.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , threshold = None , class_id = None , ** kwargs ): \"\"\" Creates a `F1` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). threshold: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval `[0, num_classes)`, where `num_classes` is the last dimension of predictions. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = 0.5 if threshold is None else threshold self . class_id = 1 if class_id is None else class_id self . precision = Precision ( threshold = self . threshold ) self . recall = Recall ( threshold = self . threshold )","title":"__init__()"},{"location":"api/metrics/F1/#elegy.metrics.f1.F1.call","text":"Accumulates f1 values (armonic mean between precision and recall). Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description ndarray Array with the cumulative f1. Source code in elegy/metrics/f1.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates f1 values (armonic mean between precision and recall). Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0, or the same rank as `y_true`, and must be broadcastable to `y_true`. Returns: Array with the cumulative f1. \"\"\" return f1 ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight , precision = self . precision , recall = self . recall , )","title":"call()"},{"location":"api/metrics/Mean/","text":"elegy.metrics.Mean Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , on = None , ** kwargs ) special Creates a Mean instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean.py 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\"Creates a `Mean` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . WEIGHTED_MEAN , on = on , ** kwargs ) call ( self , values , sample_weight = None ) Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ndarray Array with the cumulative mean. Source code in elegy/metrics/mean.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" return super () . call ( values = values , sample_weight = sample_weight )","title":"Mean"},{"location":"api/metrics/Mean/#elegymetricsmean","text":"","title":"elegy.metrics.Mean"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean","text":"Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), )","title":"elegy.metrics.mean.Mean"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.__init__","text":"Creates a Mean instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean.py 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\"Creates a `Mean` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . WEIGHTED_MEAN , on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.call","text":"Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ndarray Array with the cumulative mean. Source code in elegy/metrics/mean.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" return super () . call ( values = values , sample_weight = sample_weight )","title":"call()"},{"location":"api/metrics/MeanAbsoluteError/","text":"elegy.metrics.MeanAbsoluteError Computes the cumulative mean absolute error between y_true and y_pred . Usage: mae = elegy . metrics . MeanAbsoluteError () result = mae ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mae ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . MeanAbsoluteError (), ) __init__ ( self , on = None , ** kwargs ) special Creates a MeanAbsoluteError instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_absolute_error.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanAbsoluteError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_absolute_error.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_absolute_error ( y_true = y_true , y_pred = y_pred ))","title":"MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#elegymetricsmeanabsoluteerror","text":"","title":"elegy.metrics.MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError","text":"Computes the cumulative mean absolute error between y_true and y_pred . Usage: mae = elegy . metrics . MeanAbsoluteError () result = mae ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mae ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . MeanAbsoluteError (), )","title":"elegy.metrics.mean_absolute_error.MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.__init__","text":"Creates a MeanAbsoluteError instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_absolute_error.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanAbsoluteError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_absolute_error.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_absolute_error ( y_true = y_true , y_pred = y_pred ))","title":"call()"},{"location":"api/metrics/MeanSquaredError/","text":"elegy.metrics.MeanSquaredError Computes the cumulative mean squared error between y_true and y_pred . Usage: mse = elegy . metrics . MeanSquaredError () result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . MeanSquaredError (), ) __init__ ( self , on = None , ** kwargs ) special Creates a MeanSquaredError instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_squared_error.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanSquaredError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_squared_error.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_squared_error ( y_true = y_true , y_pred = y_pred ))","title":"MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegymetricsmeansquarederror","text":"","title":"elegy.metrics.MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError","text":"Computes the cumulative mean squared error between y_true and y_pred . Usage: mse = elegy . metrics . MeanSquaredError () result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . MeanSquaredError (), )","title":"elegy.metrics.mean_squared_error.MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.__init__","text":"Creates a MeanSquaredError instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_squared_error.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanSquaredError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_squared_error.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_squared_error ( y_true = y_true , y_pred = y_pred ))","title":"call()"},{"location":"api/metrics/Metric/","text":"elegy.metrics.Metric Encapsulates metric logic and state. Usage: m = SomeMetric ( ... ) for input in ... : result = m ( input ) print ( 'Final result: ' , result ) Usage with the Model API: class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), ], metrics = [ elegy . metrics . SparseCategoricalAccuracy () ], optimizer = optax . rmsprop ( 1e-3 ), ) To be implemented by subclasses: call() : All state variables should be created in this method by calling self.add_parameter(..., trainable=False) , update this state by calling self.update_parameter(...) , and return a result based on these states. Example subclass implementation: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = hk . add_parameter ( \"total\" , initializer = 0 , trainable = False ) count = hk . add_parameter ( \"count\" , initializer = 0 , trainable = False ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) hk . update_parameter ( \"total\" , total ) hk . update_parameter ( \"count\" , count ) return total / count __init__ ( self , on = None , ** kwargs ) special Base Metric constructor. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/metrics/metric.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Base Metric constructor. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"Metric"},{"location":"api/metrics/Metric/#elegymetricsmetric","text":"","title":"elegy.metrics.Metric"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric","text":"Encapsulates metric logic and state. Usage: m = SomeMetric ( ... ) for input in ... : result = m ( input ) print ( 'Final result: ' , result ) Usage with the Model API: class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), ], metrics = [ elegy . metrics . SparseCategoricalAccuracy () ], optimizer = optax . rmsprop ( 1e-3 ), ) To be implemented by subclasses: call() : All state variables should be created in this method by calling self.add_parameter(..., trainable=False) , update this state by calling self.update_parameter(...) , and return a result based on these states. Example subclass implementation: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = hk . add_parameter ( \"total\" , initializer = 0 , trainable = False ) count = hk . add_parameter ( \"count\" , initializer = 0 , trainable = False ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) hk . update_parameter ( \"total\" , total ) hk . update_parameter ( \"count\" , count ) return total / count","title":"elegy.metrics.metric.Metric"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric.__init__","text":"Base Metric constructor. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/metrics/metric.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Base Metric constructor. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"__init__()"},{"location":"api/metrics/Precision/","text":"elegy.metrics.Precision The metric creates two local variables, true_positives and false_positives that are used to compute the precision. This value is ultimately returned as precision , an idempotent operation that simply divides true_positives by the sum of true_positives and false_positives . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values. If class_id is specified, we calculate precision by considering only the entries in the batch for which class_id is above the threshold and computing the fraction of them for which class_id is indeed a correct label. precision = elegy . metrics . Precision () result = precision ( y_true = jnp . array ([ 0 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 1 , 1 ]) ) assert result == 0.6666667 # 2 / 3 result = precision ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 1 , 0 , 0 ]) ) assert result == 0.8 # 4 / 5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Precision (), optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , on = None , threshold = None , class_id = None , ** kwargs ) special Creates a Precision instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None threshold (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. None class_id (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/precision.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , threshold = None , class_id = None , ** kwargs ): \"\"\" Creates a `Precision` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). threshold: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval `[0, num_classes)`, where `num_classes` is the last dimension of predictions. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = 0.5 if threshold is None else threshold self . class_id = 1 if class_id is None else class_id self . true_positives = ReduceConfusionMatrix ( reduction = Reduction . TRUE_POSITIVES ) self . false_positives = ReduceConfusionMatrix ( reduction = Reduction . FALSE_POSITIVES ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description ndarray Array with the cumulative precision. Source code in elegy/metrics/precision.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0, or the same rank as `y_true`, and must be broadcastable to `y_true`. Returns: Array with the cumulative precision. \"\"\" return precision ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight , threshold = self . threshold , class_id = self . class_id , true_positives = self . true_positives , false_positives = self . false_positives , )","title":"Precision"},{"location":"api/metrics/Precision/#elegymetricsprecision","text":"","title":"elegy.metrics.Precision"},{"location":"api/metrics/Precision/#elegy.metrics.precision.Precision","text":"The metric creates two local variables, true_positives and false_positives that are used to compute the precision. This value is ultimately returned as precision , an idempotent operation that simply divides true_positives by the sum of true_positives and false_positives . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values. If class_id is specified, we calculate precision by considering only the entries in the batch for which class_id is above the threshold and computing the fraction of them for which class_id is indeed a correct label. precision = elegy . metrics . Precision () result = precision ( y_true = jnp . array ([ 0 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 1 , 1 ]) ) assert result == 0.6666667 # 2 / 3 result = precision ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 1 , 0 , 0 ]) ) assert result == 0.8 # 4 / 5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Precision (), optimizer = optix . adam ( 1e-3 ), )","title":"elegy.metrics.precision.Precision"},{"location":"api/metrics/Precision/#elegy.metrics.precision.Precision.__init__","text":"Creates a Precision instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None threshold (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. None class_id (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/precision.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , threshold = None , class_id = None , ** kwargs ): \"\"\" Creates a `Precision` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). threshold: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval `[0, num_classes)`, where `num_classes` is the last dimension of predictions. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = 0.5 if threshold is None else threshold self . class_id = 1 if class_id is None else class_id self . true_positives = ReduceConfusionMatrix ( reduction = Reduction . TRUE_POSITIVES ) self . false_positives = ReduceConfusionMatrix ( reduction = Reduction . FALSE_POSITIVES )","title":"__init__()"},{"location":"api/metrics/Precision/#elegy.metrics.precision.Precision.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description ndarray Array with the cumulative precision. Source code in elegy/metrics/precision.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0, or the same rank as `y_true`, and must be broadcastable to `y_true`. Returns: Array with the cumulative precision. \"\"\" return precision ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight , threshold = self . threshold , class_id = self . class_id , true_positives = self . true_positives , false_positives = self . false_positives , )","title":"call()"},{"location":"api/metrics/Recall/","text":"elegy.metrics.Recall This metric creates two local variables, true_positives and false_negatives , that are used to compute the recall. This value is ultimately returned as recall , an idempotent operation that simply divides true_positives by the sum of true_positives and false_negatives . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values. If class_id is specified, we calculate recall by considering only the entries in the batch for which class_id is above the threshold and computing the fraction of them for which class_id is indeed a correct label. recall = elegy . metrics . Recall () result = recall ( y_true = jnp . array ([ 0 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 1 , 1 ]) ) assert result == 0.6666667 # 2 / 3 result = recall ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ]) ) assert result == 0.42857143 # 3 / 7 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Recall (), optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , on = None , threshold = None , class_id = None , ** kwargs ) special Creates a Recall instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None threshold (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate recall with threshold=0.5. None class_id (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/recall.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , threshold = None , class_id = None , ** kwargs ): \"\"\" Creates a `Recall` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). threshold: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate recall with threshold=0.5. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval `[0, num_classes)`, where `num_classes` is the last dimension of predictions. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = 0.5 if threshold is None else threshold self . class_id = 1 if class_id is None else class_id self . true_positives = ReduceConfusionMatrix ( reduction = Reduction . TRUE_POSITIVES ) self . false_negatives = ReduceConfusionMatrix ( reduction = Reduction . FALSE_NEGATIVES ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description ndarray Array with the cumulative recall. Source code in elegy/metrics/recall.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0, or the same rank as `y_true`, and must be broadcastable to `y_true`. Returns: Array with the cumulative recall. \"\"\" return recall ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight , threshold = self . threshold , class_id = self . class_id , true_positives = self . true_positives , false_negatives = self . false_negatives , )","title":"Recall"},{"location":"api/metrics/Recall/#elegymetricsrecall","text":"","title":"elegy.metrics.Recall"},{"location":"api/metrics/Recall/#elegy.metrics.recall.Recall","text":"This metric creates two local variables, true_positives and false_negatives , that are used to compute the recall. This value is ultimately returned as recall , an idempotent operation that simply divides true_positives by the sum of true_positives and false_negatives . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values. If class_id is specified, we calculate recall by considering only the entries in the batch for which class_id is above the threshold and computing the fraction of them for which class_id is indeed a correct label. recall = elegy . metrics . Recall () result = recall ( y_true = jnp . array ([ 0 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 1 , 1 ]) ) assert result == 0.6666667 # 2 / 3 result = recall ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ]) ) assert result == 0.42857143 # 3 / 7 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Recall (), optimizer = optix . adam ( 1e-3 ), )","title":"elegy.metrics.recall.Recall"},{"location":"api/metrics/Recall/#elegy.metrics.recall.Recall.__init__","text":"Creates a Recall instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None threshold (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate recall with threshold=0.5. None class_id (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/recall.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , threshold = None , class_id = None , ** kwargs ): \"\"\" Creates a `Recall` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). threshold: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate recall with threshold=0.5. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval `[0, num_classes)`, where `num_classes` is the last dimension of predictions. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = 0.5 if threshold is None else threshold self . class_id = 1 if class_id is None else class_id self . true_positives = ReduceConfusionMatrix ( reduction = Reduction . TRUE_POSITIVES ) self . false_negatives = ReduceConfusionMatrix ( reduction = Reduction . FALSE_NEGATIVES )","title":"__init__()"},{"location":"api/metrics/Recall/#elegy.metrics.recall.Recall.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description ndarray Array with the cumulative recall. Source code in elegy/metrics/recall.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0, or the same rank as `y_true`, and must be broadcastable to `y_true`. Returns: Array with the cumulative recall. \"\"\" return recall ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight , threshold = self . threshold , class_id = self . class_id , true_positives = self . true_positives , false_negatives = self . false_negatives , )","title":"call()"},{"location":"api/metrics/Reduce/","text":"elegy.metrics.Reduce Encapsulates metrics that perform a reduce operation on the values. call ( self , values , sample_weight = None ) Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ndarray Array with the cummulative reduce. Source code in elegy/metrics/reduce.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cummulative reduce. \"\"\" total = self . add_parameter ( \"total\" , shape = [], dtype = self . dtype , initializer = initializers . Constant ( 0 ), trainable = False , ) if self . _reduction in ( Reduction . SUM_OVER_BATCH_SIZE , Reduction . WEIGHTED_MEAN , ): count = self . add_parameter ( \"count\" , shape = [], dtype = jnp . int32 , initializer = initializers . Constant ( 0 ), trainable = False , ) else : count = None value , total , count = reduce ( total = total , count = count , values = values , reduction = self . _reduction , sample_weight = sample_weight , dtype = self . dtype , ) self . update_parameter ( \"total\" , total ) if count is not None : self . update_parameter ( \"count\" , count ) return value","title":"Reduce"},{"location":"api/metrics/Reduce/#elegymetricsreduce","text":"","title":"elegy.metrics.Reduce"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce","text":"Encapsulates metrics that perform a reduce operation on the values.","title":"elegy.metrics.reduce.Reduce"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce.call","text":"Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ndarray Array with the cummulative reduce. Source code in elegy/metrics/reduce.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cummulative reduce. \"\"\" total = self . add_parameter ( \"total\" , shape = [], dtype = self . dtype , initializer = initializers . Constant ( 0 ), trainable = False , ) if self . _reduction in ( Reduction . SUM_OVER_BATCH_SIZE , Reduction . WEIGHTED_MEAN , ): count = self . add_parameter ( \"count\" , shape = [], dtype = jnp . int32 , initializer = initializers . Constant ( 0 ), trainable = False , ) else : count = None value , total , count = reduce ( total = total , count = count , values = values , reduction = self . _reduction , sample_weight = sample_weight , dtype = self . dtype , ) self . update_parameter ( \"total\" , total ) if count is not None : self . update_parameter ( \"count\" , count ) return value","title":"call()"},{"location":"api/metrics/SparseCategoricalAccuracy/","text":"elegy.metrics.SparseCategoricalAccuracy Calculates how often predictions matches integer labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . SparseCategoricalAccuracy () result = accuracy ( y_true = jnp . array ([ 2 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]) ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([ 1 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), ) __init__ ( self , on = None , ** kwargs ) special Creates a SparseCategoricalAccuracy instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/sparse_categorical_accuracy.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `SparseCategoricalAccuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape except y_true should not have the last dimension of y_pred . Parameters: Name Type Description Default y_true ndarray Sparse ground truth values. shape = [batch_size, d0, .. dN-1] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN-1, dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/sparse_categorical_accuracy.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape except `y_true` should not have the last dimension of `y_pred`. Arguments: y_true: Sparse ground truth values. shape = `[batch_size, d0, .. dN-1]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN-1, dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = sparse_categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegymetricssparsecategoricalaccuracy","text":"","title":"elegy.metrics.SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy","text":"Calculates how often predictions matches integer labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . SparseCategoricalAccuracy () result = accuracy ( y_true = jnp . array ([ 2 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]) ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([ 1 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.__init__","text":"Creates a SparseCategoricalAccuracy instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/sparse_categorical_accuracy.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `SparseCategoricalAccuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape except y_true should not have the last dimension of y_pred . Parameters: Name Type Description Default y_true ndarray Sparse ground truth values. shape = [batch_size, d0, .. dN-1] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN-1, dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/sparse_categorical_accuracy.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape except `y_true` should not have the last dimension of `y_pred`. Arguments: y_true: Sparse ground truth values. shape = `[batch_size, d0, .. dN-1]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN-1, dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = sparse_categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/Sum/","text":"elegy.metrics.Sum Computes the (weighted) sum of the given values. For example, if values is [1, 3, 5, 7] then the sum is 16. If the weights were specified as [1, 1, 0, 0] then the sum would be 4. This metric creates one variable, total , that is used to compute the sum of values . This is ultimately returned as sum . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: m = elegy . metrics . Sum () assert 16.0 == m ([ 1 , 3 , 5 , 7 ]) Usage with Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Sum ( name = 'sum_1' ), ) model = elegy . Model ( inputs , outputs ) __init__ ( self , on = None , ** kwargs ) special Creates a Sum instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/sum.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\"Creates a `Sum` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . SUM , on = on , ** kwargs ) call ( self , values , sample_weight = None ) inherited Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ndarray Array with the cummulative reduce. Source code in elegy/metrics/sum.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cummulative reduce. \"\"\" total = self . add_parameter ( \"total\" , shape = [], dtype = self . dtype , initializer = initializers . Constant ( 0 ), trainable = False , ) if self . _reduction in ( Reduction . SUM_OVER_BATCH_SIZE , Reduction . WEIGHTED_MEAN , ): count = self . add_parameter ( \"count\" , shape = [], dtype = jnp . int32 , initializer = initializers . Constant ( 0 ), trainable = False , ) else : count = None value , total , count = reduce ( total = total , count = count , values = values , reduction = self . _reduction , sample_weight = sample_weight , dtype = self . dtype , ) self . update_parameter ( \"total\" , total ) if count is not None : self . update_parameter ( \"count\" , count ) return value","title":"Sum"},{"location":"api/metrics/Sum/#elegymetricssum","text":"","title":"elegy.metrics.Sum"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum","text":"Computes the (weighted) sum of the given values. For example, if values is [1, 3, 5, 7] then the sum is 16. If the weights were specified as [1, 1, 0, 0] then the sum would be 4. This metric creates one variable, total , that is used to compute the sum of values . This is ultimately returned as sum . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: m = elegy . metrics . Sum () assert 16.0 == m ([ 1 , 3 , 5 , 7 ]) Usage with Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Sum ( name = 'sum_1' ), ) model = elegy . Model ( inputs , outputs )","title":"elegy.metrics.sum.Sum"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.__init__","text":"Creates a Sum instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/sum.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\"Creates a `Sum` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . SUM , on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.call","text":"Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ndarray Array with the cummulative reduce. Source code in elegy/metrics/sum.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cummulative reduce. \"\"\" total = self . add_parameter ( \"total\" , shape = [], dtype = self . dtype , initializer = initializers . Constant ( 0 ), trainable = False , ) if self . _reduction in ( Reduction . SUM_OVER_BATCH_SIZE , Reduction . WEIGHTED_MEAN , ): count = self . add_parameter ( \"count\" , shape = [], dtype = jnp . int32 , initializer = initializers . Constant ( 0 ), trainable = False , ) else : count = None value , total , count = reduce ( total = total , count = count , values = values , reduction = self . _reduction , sample_weight = sample_weight , dtype = self . dtype , ) self . update_parameter ( \"total\" , total ) if count is not None : self . update_parameter ( \"count\" , count ) return value","title":"call()"},{"location":"api/metrics/accuracy/","text":"elegy.metrics.accuracy Source code in elegy/metrics/accuracy.py 10 11 12 13 14 15 16 17 18 19 def accuracy ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : # [y_pred, y_true], _ = metrics_utils.ragged_assert_compatible_and_get_flat_values( # [y_pred, y_true] # ) # y_pred.shape.assert_is_compatible_with(y_true.shape) if y_true . dtype != y_pred . dtype : y_pred = y_pred . astype ( y_true . dtype ) return ( y_true == y_pred ) . astype ( jnp . float32 )","title":"accuracy"},{"location":"api/metrics/accuracy/#elegymetricsaccuracy","text":"","title":"elegy.metrics.accuracy"},{"location":"api/metrics/accuracy/#elegy.metrics.accuracy.accuracy","text":"Source code in elegy/metrics/accuracy.py 10 11 12 13 14 15 16 17 18 19 def accuracy ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : # [y_pred, y_true], _ = metrics_utils.ragged_assert_compatible_and_get_flat_values( # [y_pred, y_true] # ) # y_pred.shape.assert_is_compatible_with(y_true.shape) if y_true . dtype != y_pred . dtype : y_pred = y_pred . astype ( y_true . dtype ) return ( y_true == y_pred ) . astype ( jnp . float32 )","title":"elegy.metrics.accuracy.accuracy"},{"location":"api/metrics/binary_accuracy/","text":"elegy.metrics.binary_accuracy Calculates how often predictions matches binary labels. Standalone usage: y_true = np . array ([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]) y_pred = np . array ([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]) m = elegy . metrics . binary_accuracy ( y_true , y_pred ) assert m . shape == ( 4 ,) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required threshold float Float representing the threshold for deciding whether prediction values are 1 or 0. 0.5 Returns: Type Description ndarray Binary accuracy values. shape = [batch_size, d0, .. dN-1] Source code in elegy/metrics/binary_accuracy.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def binary_accuracy ( y_true : np . ndarray , y_pred : np . ndarray , threshold : float = 0.5 ) -> np . ndarray : \"\"\" Calculates how often predictions matches binary labels. Standalone usage: ```python y_true = np.array([[1], [1], [0], [0]]) y_pred = np.array([[1], [1], [0], [0]]) m = elegy.metrics.binary_accuracy(y_true, y_pred) assert m.shape == (4,) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. threshold: Float representing the threshold for deciding whether prediction values are 1 or 0. Returns: Binary accuracy values. shape = `[batch_size, d0, .. dN-1]` \"\"\" assert abs ( y_pred . ndim - y_true . ndim ) <= 1 y_true , y_pred = utils . maybe_expand_dims ( y_true , y_pred ) y_pred = y_pred > threshold return jnp . mean ( y_true == y_pred , axis =- 1 )","title":"binary_accuracy"},{"location":"api/metrics/binary_accuracy/#elegymetricsbinary_accuracy","text":"","title":"elegy.metrics.binary_accuracy"},{"location":"api/metrics/binary_accuracy/#elegy.metrics.binary_accuracy.binary_accuracy","text":"Calculates how often predictions matches binary labels. Standalone usage: y_true = np . array ([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]) y_pred = np . array ([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]) m = elegy . metrics . binary_accuracy ( y_true , y_pred ) assert m . shape == ( 4 ,) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required threshold float Float representing the threshold for deciding whether prediction values are 1 or 0. 0.5 Returns: Type Description ndarray Binary accuracy values. shape = [batch_size, d0, .. dN-1] Source code in elegy/metrics/binary_accuracy.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def binary_accuracy ( y_true : np . ndarray , y_pred : np . ndarray , threshold : float = 0.5 ) -> np . ndarray : \"\"\" Calculates how often predictions matches binary labels. Standalone usage: ```python y_true = np.array([[1], [1], [0], [0]]) y_pred = np.array([[1], [1], [0], [0]]) m = elegy.metrics.binary_accuracy(y_true, y_pred) assert m.shape == (4,) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. threshold: Float representing the threshold for deciding whether prediction values are 1 or 0. Returns: Binary accuracy values. shape = `[batch_size, d0, .. dN-1]` \"\"\" assert abs ( y_pred . ndim - y_true . ndim ) <= 1 y_true , y_pred = utils . maybe_expand_dims ( y_true , y_pred ) y_pred = y_pred > threshold return jnp . mean ( y_true == y_pred , axis =- 1 )","title":"elegy.metrics.binary_accuracy.binary_accuracy"},{"location":"api/metrics/binary_crossentropy/","text":"elegy.metrics.binary_crossentropy Source code in elegy/losses/binary_crossentropy.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def binary_crossentropy ( y_true : jnp . ndarray , y_pred : jnp . ndarray , from_logits : bool = False ) -> jnp . ndarray : assert abs ( y_pred . ndim - y_true . ndim ) <= 1 y_true , y_pred = utils . maybe_expand_dims ( y_true , y_pred ) if from_logits : return - jnp . mean ( y_true * y_pred - jnp . logaddexp ( 0.0 , y_pred ), axis =- 1 ) y_pred = jnp . clip ( y_pred , utils . EPSILON , 1.0 - utils . EPSILON ) return - jnp . mean ( y_true * jnp . log ( y_pred ) + ( 1 - y_true ) * jnp . log ( 1 - y_pred ), axis =- 1 )","title":"binary_crossentropy"},{"location":"api/metrics/binary_crossentropy/#elegymetricsbinary_crossentropy","text":"","title":"elegy.metrics.binary_crossentropy"},{"location":"api/metrics/binary_crossentropy/#elegy.losses.binary_crossentropy.binary_crossentropy","text":"Source code in elegy/losses/binary_crossentropy.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def binary_crossentropy ( y_true : jnp . ndarray , y_pred : jnp . ndarray , from_logits : bool = False ) -> jnp . ndarray : assert abs ( y_pred . ndim - y_true . ndim ) <= 1 y_true , y_pred = utils . maybe_expand_dims ( y_true , y_pred ) if from_logits : return - jnp . mean ( y_true * y_pred - jnp . logaddexp ( 0.0 , y_pred ), axis =- 1 ) y_pred = jnp . clip ( y_pred , utils . EPSILON , 1.0 - utils . EPSILON ) return - jnp . mean ( y_true * jnp . log ( y_pred ) + ( 1 - y_true ) * jnp . log ( 1 - y_pred ), axis =- 1 )","title":"elegy.losses.binary_crossentropy.binary_crossentropy"},{"location":"api/metrics/categorical_accuracy/","text":"elegy.metrics.categorical_accuracy Source code in elegy/metrics/categorical_accuracy.py 11 12 13 14 15 16 def categorical_accuracy ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : y_true = jnp . argmax ( y_true , axis =- 1 ) y_pred = jnp . argmax ( y_pred , axis =- 1 ) return accuracy ( y_true , y_pred )","title":"categorical_accuracy"},{"location":"api/metrics/categorical_accuracy/#elegymetricscategorical_accuracy","text":"","title":"elegy.metrics.categorical_accuracy"},{"location":"api/metrics/categorical_accuracy/#elegy.metrics.categorical_accuracy.categorical_accuracy","text":"Source code in elegy/metrics/categorical_accuracy.py 11 12 13 14 15 16 def categorical_accuracy ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : y_true = jnp . argmax ( y_true , axis =- 1 ) y_pred = jnp . argmax ( y_pred , axis =- 1 ) return accuracy ( y_true , y_pred )","title":"elegy.metrics.categorical_accuracy.categorical_accuracy"},{"location":"api/metrics/f1/","text":"elegy.metrics.f1 Source code in elegy/metrics/f1.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def f1 ( y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : jnp . ndarray , precision : Precision , recall : Recall , ) -> jnp . ndarray : precision_values = precision ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) recall_values = recall ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) return 2 * jnp . divide ( ( precision_values * recall_values ), ( precision_values + recall_values ) )","title":"f1"},{"location":"api/metrics/f1/#elegymetricsf1","text":"","title":"elegy.metrics.f1"},{"location":"api/metrics/f1/#elegy.metrics.f1.f1","text":"Source code in elegy/metrics/f1.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def f1 ( y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : jnp . ndarray , precision : Precision , recall : Recall , ) -> jnp . ndarray : precision_values = precision ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) recall_values = recall ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) return 2 * jnp . divide ( ( precision_values * recall_values ), ( precision_values + recall_values ) )","title":"elegy.metrics.f1.f1"},{"location":"api/metrics/mean_absolute_error/","text":"elegy.metrics.mean_absolute_error Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean ( abs ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . abs ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_absolute_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . abs ( y_pred - y_true ), axis =- 1 )","title":"mean_absolute_error"},{"location":"api/metrics/mean_absolute_error/#elegymetricsmean_absolute_error","text":"","title":"elegy.metrics.mean_absolute_error"},{"location":"api/metrics/mean_absolute_error/#elegy.losses.mean_absolute_error.mean_absolute_error","text":"Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean ( abs ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . abs ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_absolute_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . abs ( y_pred - y_true ), axis =- 1 )","title":"elegy.losses.mean_absolute_error.mean_absolute_error"},{"location":"api/metrics/mean_squared_error/","text":"elegy.metrics.mean_squared_error Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"mean_squared_error"},{"location":"api/metrics/mean_squared_error/#elegymetricsmean_squared_error","text":"","title":"elegy.metrics.mean_squared_error"},{"location":"api/metrics/mean_squared_error/#elegy.losses.mean_squared_error.mean_squared_error","text":"Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"elegy.losses.mean_squared_error.mean_squared_error"},{"location":"api/metrics/precision/","text":"elegy.metrics.precision Source code in elegy/metrics/precision.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def precision ( y_true : jnp . ndarray , y_pred : jnp . ndarray , threshold : jnp . ndarray , class_id : jnp . ndarray , sample_weight : jnp . ndarray , true_positives : ReduceConfusionMatrix , false_positives : ReduceConfusionMatrix , ) -> jnp . ndarray : # TODO: class_id behavior y_pred = ( y_pred > threshold ) . astype ( jnp . float32 ) if y_true . dtype != y_pred . dtype : y_pred = y_pred . astype ( y_true . dtype ) true_positives = true_positives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) false_positives = false_positives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) return jnp . nan_to_num ( jnp . divide ( true_positives , true_positives + false_positives ))","title":"precision"},{"location":"api/metrics/precision/#elegymetricsprecision","text":"","title":"elegy.metrics.precision"},{"location":"api/metrics/precision/#elegy.metrics.precision.precision","text":"Source code in elegy/metrics/precision.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def precision ( y_true : jnp . ndarray , y_pred : jnp . ndarray , threshold : jnp . ndarray , class_id : jnp . ndarray , sample_weight : jnp . ndarray , true_positives : ReduceConfusionMatrix , false_positives : ReduceConfusionMatrix , ) -> jnp . ndarray : # TODO: class_id behavior y_pred = ( y_pred > threshold ) . astype ( jnp . float32 ) if y_true . dtype != y_pred . dtype : y_pred = y_pred . astype ( y_true . dtype ) true_positives = true_positives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) false_positives = false_positives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) return jnp . nan_to_num ( jnp . divide ( true_positives , true_positives + false_positives ))","title":"elegy.metrics.precision.precision"},{"location":"api/metrics/recall/","text":"elegy.metrics.recall Source code in elegy/metrics/recall.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def recall ( y_true : jnp . ndarray , y_pred : jnp . ndarray , threshold : jnp . ndarray , class_id : jnp . ndarray , sample_weight : jnp . ndarray , true_positives : ReduceConfusionMatrix , false_negatives : ReduceConfusionMatrix , ) -> jnp . ndarray : # TODO: class_id behavior y_pred = ( y_pred > threshold ) . astype ( jnp . float32 ) if y_true . dtype != y_pred . dtype : y_pred = y_pred . astype ( y_true . dtype ) true_positives = true_positives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) false_negatives = false_negatives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) return jnp . nan_to_num ( jnp . divide ( true_positives , true_positives + false_negatives ))","title":"recall"},{"location":"api/metrics/recall/#elegymetricsrecall","text":"","title":"elegy.metrics.recall"},{"location":"api/metrics/recall/#elegy.metrics.recall.recall","text":"Source code in elegy/metrics/recall.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def recall ( y_true : jnp . ndarray , y_pred : jnp . ndarray , threshold : jnp . ndarray , class_id : jnp . ndarray , sample_weight : jnp . ndarray , true_positives : ReduceConfusionMatrix , false_negatives : ReduceConfusionMatrix , ) -> jnp . ndarray : # TODO: class_id behavior y_pred = ( y_pred > threshold ) . astype ( jnp . float32 ) if y_true . dtype != y_pred . dtype : y_pred = y_pred . astype ( y_true . dtype ) true_positives = true_positives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) false_negatives = false_negatives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) return jnp . nan_to_num ( jnp . divide ( true_positives , true_positives + false_negatives ))","title":"elegy.metrics.recall.recall"},{"location":"api/metrics/reduce/","text":"elegy.metrics.reduce Source code in elegy/metrics/reduce.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def reduce ( total : jnp . ndarray , count : tp . Optional [ jnp . ndarray ], values : jnp . ndarray , reduction : Reduction , sample_weight : tp . Optional [ np . ndarray ], dtype : jnp . dtype , ) -> tp . Tuple [ jnp . ndarray , jnp . ndarray , tp . Optional [ jnp . ndarray ]]: if sample_weight is not None : sample_weight = sample_weight . astype ( dtype ) # Update dimensions of weights to match with values if possible. # values, _, sample_weight = tf_losses_utils.squeeze_or_expand_dimensions( # values, sample_weight=sample_weight # ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array ndim = values . ndim weight_ndim = sample_weight . ndim if reduction == Reduction . SUM : values = jnp . sum ( values , axis = list ( range ( weight_ndim , ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , ndim ))) values = values * sample_weight value_sum = jnp . sum ( values ) total += value_sum # Exit early if the reduction doesn't have a denominator. if reduction == Reduction . SUM : num_values = None # Update `count` for reductions that require a denominator. elif reduction == Reduction . SUM_OVER_BATCH_SIZE : num_values = jnp . prod ( values . shape ) . astype ( dtype ) else : if sample_weight is None : num_values = jnp . prod ( jnp . array ( values . shape )) . astype ( dtype ) else : num_values = jnp . sum ( sample_weight ) if count is not None and num_values is not None : count += num_values if reduction == Reduction . SUM : value = total else : value = total / count return value , total , count","title":"reduce"},{"location":"api/metrics/reduce/#elegymetricsreduce","text":"","title":"elegy.metrics.reduce"},{"location":"api/metrics/reduce/#elegy.metrics.reduce.reduce","text":"Source code in elegy/metrics/reduce.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def reduce ( total : jnp . ndarray , count : tp . Optional [ jnp . ndarray ], values : jnp . ndarray , reduction : Reduction , sample_weight : tp . Optional [ np . ndarray ], dtype : jnp . dtype , ) -> tp . Tuple [ jnp . ndarray , jnp . ndarray , tp . Optional [ jnp . ndarray ]]: if sample_weight is not None : sample_weight = sample_weight . astype ( dtype ) # Update dimensions of weights to match with values if possible. # values, _, sample_weight = tf_losses_utils.squeeze_or_expand_dimensions( # values, sample_weight=sample_weight # ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array ndim = values . ndim weight_ndim = sample_weight . ndim if reduction == Reduction . SUM : values = jnp . sum ( values , axis = list ( range ( weight_ndim , ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , ndim ))) values = values * sample_weight value_sum = jnp . sum ( values ) total += value_sum # Exit early if the reduction doesn't have a denominator. if reduction == Reduction . SUM : num_values = None # Update `count` for reductions that require a denominator. elif reduction == Reduction . SUM_OVER_BATCH_SIZE : num_values = jnp . prod ( values . shape ) . astype ( dtype ) else : if sample_weight is None : num_values = jnp . prod ( jnp . array ( values . shape )) . astype ( dtype ) else : num_values = jnp . sum ( sample_weight ) if count is not None and num_values is not None : count += num_values if reduction == Reduction . SUM : value = total else : value = total / count return value , total , count","title":"elegy.metrics.reduce.reduce"},{"location":"api/metrics/sparse_categorical_accuracy/","text":"elegy.metrics.sparse_categorical_accuracy Source code in elegy/metrics/sparse_categorical_accuracy.py 11 12 13 14 15 16 17 def sparse_categorical_accuracy ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : y_pred = jnp . argmax ( y_pred , axis =- 1 ) return accuracy ( y_true , y_pred )","title":"sparse_categorical_accuracy"},{"location":"api/metrics/sparse_categorical_accuracy/#elegymetricssparse_categorical_accuracy","text":"","title":"elegy.metrics.sparse_categorical_accuracy"},{"location":"api/metrics/sparse_categorical_accuracy/#elegy.metrics.sparse_categorical_accuracy.sparse_categorical_accuracy","text":"Source code in elegy/metrics/sparse_categorical_accuracy.py 11 12 13 14 15 16 17 def sparse_categorical_accuracy ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : y_pred = jnp . argmax ( y_pred , axis =- 1 ) return accuracy ( y_true , y_pred )","title":"elegy.metrics.sparse_categorical_accuracy.sparse_categorical_accuracy"},{"location":"api/module/Module/","text":"elegy.module.Module Basic Elegy Module. For more information check out the Module System guide . Attributes: Name Type Description initialized Whether or not the module is initialized. __init__ ( self , name = None , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>) special Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Parameters: Name Type Description Default name Optional[str] An optional string name for the class. Must be a valid elsePython identifier. If name is not provided then the class name for the current instance is converted to lower_snake_case and used instead. None Source code in elegy/module.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : np . dtype = jnp . float32 ): \"\"\" Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Arguments: name: An optional string name for the class. Must be a valid elsePython identifier. If ``name`` is not provided then the class name for the current instance is converted to ``lower_snake_case`` and used instead. \"\"\" self . name = name if name else utils . lower_snake_case ( self . __class__ . __name__ ) self . dtype = dtype self . _params = {} self . _states = [] self . _submodules = [] self . _dynamic_submodules = [] self . _initialized = False self . _trainable = True _init = self . init def init ( * args , ** kwargs ): return _init ( * args , ** kwargs ) self . init = init utils . wraps ( self . call )( self . init ) utils . wraps ( self . call )( self ) self . _jit_functions () add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/module.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value get_parameters ( self , trainable = None ) Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/module.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) Initializes the module, Source code in elegy/module.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/module.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/module.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"Module"},{"location":"api/module/Module/#elegymodulemodule","text":"","title":"elegy.module.Module"},{"location":"api/module/Module/#elegy.module.Module","text":"Basic Elegy Module. For more information check out the Module System guide . Attributes: Name Type Description initialized Whether or not the module is initialized.","title":"elegy.module.Module"},{"location":"api/module/Module/#elegy.module.Module.__init__","text":"Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Parameters: Name Type Description Default name Optional[str] An optional string name for the class. Must be a valid elsePython identifier. If name is not provided then the class name for the current instance is converted to lower_snake_case and used instead. None Source code in elegy/module.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : np . dtype = jnp . float32 ): \"\"\" Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Arguments: name: An optional string name for the class. Must be a valid elsePython identifier. If ``name`` is not provided then the class name for the current instance is converted to ``lower_snake_case`` and used instead. \"\"\" self . name = name if name else utils . lower_snake_case ( self . __class__ . __name__ ) self . dtype = dtype self . _params = {} self . _states = [] self . _submodules = [] self . _dynamic_submodules = [] self . _initialized = False self . _trainable = True _init = self . init def init ( * args , ** kwargs ): return _init ( * args , ** kwargs ) self . init = init utils . wraps ( self . call )( self . init ) utils . wraps ( self . call )( self ) self . _jit_functions ()","title":"__init__()"},{"location":"api/module/Module/#elegy.module.Module.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/module.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/module/Module/#elegy.module.Module.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/module.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/module/Module/#elegy.module.Module.init","text":"Initializes the module, Source code in elegy/module.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/module/Module/#elegy.module.Module.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/module.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/module/Module/#elegy.module.Module.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/module.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/module/add_loss/","text":"elegy.module.add_loss A hook that lets you define a loss within a [ module ][elegy.module.Module]. w = self . add_parameter ( \"w\" , [ 3 , 5 ], initializer = jnp . ones ) # L2 regularization penalty elegy . add_loss ( \"l2_regularization\" , 0.01 * jnp . mean ( w ** 2 )) Parameters: Name Type Description Default name str The name of the loss. If a name is repeated on different calls values will be added together. required value ndarray The value for the loss. required Source code in elegy/module.py 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def add_loss ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a loss within a [`module`][elegy.module.Module]. ```python w = self.add_parameter(\"w\", [3, 5], initializer=jnp.ones) # L2 regularization penalty elegy.add_loss(\"l2_regularization\", 0.01 * jnp.mean(w ** 2)) ``` Arguments: name: The name of the loss. If a `name` is repeated on different calls values will be added together. value: The value for the loss. \"\"\" if LOCAL . losses is None : return if not name . endswith ( \"loss\" ): name += \"_loss\" if name in LOCAL . losses : LOCAL . losses [ name ] += value else : LOCAL . losses [ name ] = value","title":"add_loss"},{"location":"api/module/add_loss/#elegymoduleadd_loss","text":"","title":"elegy.module.add_loss"},{"location":"api/module/add_loss/#elegy.module.add_loss","text":"A hook that lets you define a loss within a [ module ][elegy.module.Module]. w = self . add_parameter ( \"w\" , [ 3 , 5 ], initializer = jnp . ones ) # L2 regularization penalty elegy . add_loss ( \"l2_regularization\" , 0.01 * jnp . mean ( w ** 2 )) Parameters: Name Type Description Default name str The name of the loss. If a name is repeated on different calls values will be added together. required value ndarray The value for the loss. required Source code in elegy/module.py 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def add_loss ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a loss within a [`module`][elegy.module.Module]. ```python w = self.add_parameter(\"w\", [3, 5], initializer=jnp.ones) # L2 regularization penalty elegy.add_loss(\"l2_regularization\", 0.01 * jnp.mean(w ** 2)) ``` Arguments: name: The name of the loss. If a `name` is repeated on different calls values will be added together. value: The value for the loss. \"\"\" if LOCAL . losses is None : return if not name . endswith ( \"loss\" ): name += \"_loss\" if name in LOCAL . losses : LOCAL . losses [ name ] += value else : LOCAL . losses [ name ] = value","title":"elegy.module.add_loss"},{"location":"api/module/add_metric/","text":"elegy.module.add_metric A hook that lets you define a metric within a [ module ][elegy.module.Module]. y = jax . nn . relu ( x ) elegy . add_metric ( \"activation_mean\" , jnp . mean ( y )) Parameters: Name Type Description Default name str The name of the loss. If a metric with the same name already exists a unique identifier will be generated. required value ndarray The value for the metric. required Source code in elegy/module.py 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 def add_metric ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a metric within a [`module`][elegy.module.Module]. ```python y = jax.nn.relu(x) elegy.add_metric(\"activation_mean\", jnp.mean(y)) ``` Arguments: name: The name of the loss. If a metric with the same `name` already exists a unique identifier will be generated. value: The value for the metric. \"\"\" if LOCAL . metrics is None : return name = f \" { base_name () } / { name } \" name = get_unique_name ( set ( LOCAL . metrics ), name ) LOCAL . metrics [ name ] = value","title":"add_metric"},{"location":"api/module/add_metric/#elegymoduleadd_metric","text":"","title":"elegy.module.add_metric"},{"location":"api/module/add_metric/#elegy.module.add_metric","text":"A hook that lets you define a metric within a [ module ][elegy.module.Module]. y = jax . nn . relu ( x ) elegy . add_metric ( \"activation_mean\" , jnp . mean ( y )) Parameters: Name Type Description Default name str The name of the loss. If a metric with the same name already exists a unique identifier will be generated. required value ndarray The value for the metric. required Source code in elegy/module.py 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 def add_metric ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a metric within a [`module`][elegy.module.Module]. ```python y = jax.nn.relu(x) elegy.add_metric(\"activation_mean\", jnp.mean(y)) ``` Arguments: name: The name of the loss. If a metric with the same `name` already exists a unique identifier will be generated. value: The value for the metric. \"\"\" if LOCAL . metrics is None : return name = f \" { base_name () } / { name } \" name = get_unique_name ( set ( LOCAL . metrics ), name ) LOCAL . metrics [ name ] = value","title":"elegy.module.add_metric"},{"location":"api/module/add_summary/","text":"elegy.module.add_summary A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so [ Model.summary ][elegy.model.model.Model.summary] can show a representation of architecture. def call ( self , x ): ... y = jax . nn . relu ( x ) elegy . add_summary ( \"relu\" , y ) ... Parameters: Name Type Description Default module_or_name Union[elegy.module.Module, str] The name of the summary or alternatively the module that this summary will represent. If a summary with the same name already exists a unique identifier will be generated. required value ndarray The value for the summary. required Source code in elegy/module.py 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 def add_summary ( module_or_name : tp . Union [ Module , str ], value : np . ndarray ) -> None : \"\"\" A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so [`Model.summary`][elegy.model.model.Model.summary] can show a representation of architecture. ```python def call(self, x): ... y = jax.nn.relu(x) elegy.add_summary(\"relu\", y) ... ``` Arguments: module_or_name: The name of the summary or alternatively the module that this summary will represent. If a summary with the same name already exists a unique identifier will be generated. value: The value for the summary. \"\"\" if LOCAL . summaries is None : return name = base_name () if isinstance ( module_or_name , str ): name = f \" { name } / { module_or_name } \" if name else module_or_name name = get_unique_name ({ t [ 1 ] for t in LOCAL . summaries }, name ) module = None else : module = module_or_name LOCAL . summaries . append (( module , name , value ))","title":"add_summary"},{"location":"api/module/add_summary/#elegymoduleadd_summary","text":"","title":"elegy.module.add_summary"},{"location":"api/module/add_summary/#elegy.module.add_summary","text":"A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so [ Model.summary ][elegy.model.model.Model.summary] can show a representation of architecture. def call ( self , x ): ... y = jax . nn . relu ( x ) elegy . add_summary ( \"relu\" , y ) ... Parameters: Name Type Description Default module_or_name Union[elegy.module.Module, str] The name of the summary or alternatively the module that this summary will represent. If a summary with the same name already exists a unique identifier will be generated. required value ndarray The value for the summary. required Source code in elegy/module.py 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 def add_summary ( module_or_name : tp . Union [ Module , str ], value : np . ndarray ) -> None : \"\"\" A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so [`Model.summary`][elegy.model.model.Model.summary] can show a representation of architecture. ```python def call(self, x): ... y = jax.nn.relu(x) elegy.add_summary(\"relu\", y) ... ``` Arguments: module_or_name: The name of the summary or alternatively the module that this summary will represent. If a summary with the same name already exists a unique identifier will be generated. value: The value for the summary. \"\"\" if LOCAL . summaries is None : return name = base_name () if isinstance ( module_or_name , str ): name = f \" { name } / { module_or_name } \" if name else module_or_name name = get_unique_name ({ t [ 1 ] for t in LOCAL . summaries }, name ) module = None else : module = module_or_name LOCAL . summaries . append (( module , name , value ))","title":"elegy.module.add_summary"},{"location":"api/module/next_rng_key/","text":"elegy.module.next_rng_key Returns a key usable with jax.random.* functions. Source code in elegy/module.py 679 680 681 682 683 def next_rng_key () -> jnp . ndarray : \"\"\" Returns a key usable with `jax.random.*` functions. \"\"\" return LOCAL . rng ()","title":"next_rng_key"},{"location":"api/module/next_rng_key/#elegymodulenext_rng_key","text":"","title":"elegy.module.next_rng_key"},{"location":"api/module/next_rng_key/#elegy.module.next_rng_key","text":"Returns a key usable with jax.random.* functions. Source code in elegy/module.py 679 680 681 682 683 def next_rng_key () -> jnp . ndarray : \"\"\" Returns a key usable with `jax.random.*` functions. \"\"\" return LOCAL . rng ()","title":"elegy.module.next_rng_key"},{"location":"api/module/to_module/","text":"elegy.module.to_module Source code in elegy/module.py 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 def to_module ( f ): class ToModule ( Module ): def __init__ ( self , name : tp . Optional [ str ] = None ): super () . __init__ ( name = utils . lower_snake_case ( f . __name__ ) if name is None else name ) self . call = f def call ( self , * args , ** kwargs ): ... ToModule . __name__ = f . __name__ return ToModule","title":"to_module"},{"location":"api/module/to_module/#elegymoduleto_module","text":"","title":"elegy.module.to_module"},{"location":"api/module/to_module/#elegy.module.to_module","text":"Source code in elegy/module.py 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 def to_module ( f ): class ToModule ( Module ): def __init__ ( self , name : tp . Optional [ str ] = None ): super () . __init__ ( name = utils . lower_snake_case ( f . __name__ ) if name is None else name ) self . call = f def call ( self , * args , ** kwargs ): ... ToModule . __name__ = f . __name__ return ToModule","title":"elegy.module.to_module"},{"location":"api/nets/resnet/ResNet/","text":"elegy.nets.resnet.ResNet A generic ResNet V1 architecture that can be customized for non-standard configurations Original Paper: Deep Residual Learning for Image Recognition __init__ ( self , stages , block_type , lowres = False , * args , ** kwargs ) special Parameters: Name Type Description Default stages List[int] A list of integers representing the number of blocks in each stage. e.g: [3, 4, 6, 3] for a ResNet50 required block_type Union[elegy.nets.resnet.ResNetBlock, elegy.nets.resnet.BottleneckResNetBlock] Which ResNet block type to use. required lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. required Source code in elegy/nets/resnet.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , stages : tp . List [ int ], block_type : tp . Union [ ResNetBlock , BottleneckResNetBlock ], lowres : tp . Optional [ bool ] = False , * args , ** kwargs ): \"\"\" Arguments: stages: A list of integers representing the number of blocks in each stage. e.g: [3, 4, 6, 3] for a ResNet50 block_type: Which ResNet block type to use. lowres: Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) dtype: Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. \"\"\" super () . __init__ ( * args , ** kwargs ) self . stages = stages self . block_type = block_type self . lowres = lowres","title":"ResNet"},{"location":"api/nets/resnet/ResNet/#elegynetsresnetresnet","text":"","title":"elegy.nets.resnet.ResNet"},{"location":"api/nets/resnet/ResNet/#elegy.nets.resnet.ResNet","text":"A generic ResNet V1 architecture that can be customized for non-standard configurations Original Paper: Deep Residual Learning for Image Recognition","title":"elegy.nets.resnet.ResNet"},{"location":"api/nets/resnet/ResNet/#elegy.nets.resnet.ResNet.__init__","text":"Parameters: Name Type Description Default stages List[int] A list of integers representing the number of blocks in each stage. e.g: [3, 4, 6, 3] for a ResNet50 required block_type Union[elegy.nets.resnet.ResNetBlock, elegy.nets.resnet.BottleneckResNetBlock] Which ResNet block type to use. required lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. required Source code in elegy/nets/resnet.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , stages : tp . List [ int ], block_type : tp . Union [ ResNetBlock , BottleneckResNetBlock ], lowres : tp . Optional [ bool ] = False , * args , ** kwargs ): \"\"\" Arguments: stages: A list of integers representing the number of blocks in each stage. e.g: [3, 4, 6, 3] for a ResNet50 block_type: Which ResNet block type to use. lowres: Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) dtype: Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. \"\"\" super () . __init__ ( * args , ** kwargs ) self . stages = stages self . block_type = block_type self . lowres = lowres","title":"__init__()"},{"location":"api/nets/resnet/ResNet101/","text":"elegy.nets.resnet.ResNet101 __init__ ( self , lowres = False , dtype = 'float32' , * args , ** kwargs ) special Instantiates the ResNet101 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional[Union[float16, float32]] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. 'float32' Source code in elegy/nets/resnet.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def __init__ ( self , lowres : tp . Optional [ bool ] = False , dtype : tp . Optional [ tp . Union [ \"float16\" , \"float32\" ]] = \"float32\" , * args , ** kwargs ): super () . __init__ ( stages = [ 3 , 4 , 23 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , dtype = dtype , * args , ** kwargs )","title":"ResNet101"},{"location":"api/nets/resnet/ResNet101/#elegynetsresnetresnet101","text":"","title":"elegy.nets.resnet.ResNet101"},{"location":"api/nets/resnet/ResNet101/#elegy.nets.resnet.ResNet101","text":"","title":"elegy.nets.resnet.ResNet101"},{"location":"api/nets/resnet/ResNet101/#elegy.nets.resnet.ResNet101.__init__","text":"Instantiates the ResNet101 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional[Union[float16, float32]] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. 'float32' Source code in elegy/nets/resnet.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def __init__ ( self , lowres : tp . Optional [ bool ] = False , dtype : tp . Optional [ tp . Union [ \"float16\" , \"float32\" ]] = \"float32\" , * args , ** kwargs ): super () . __init__ ( stages = [ 3 , 4 , 23 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , dtype = dtype , * args , ** kwargs )","title":"__init__()"},{"location":"api/nets/resnet/ResNet152/","text":"elegy.nets.resnet.ResNet152 __init__ ( self , lowres = False , dtype = 'float32' , * args , ** kwargs ) special Instantiates the ResNet152 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional[Union[float16, float32]] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. 'float32' Source code in elegy/nets/resnet.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def __init__ ( self , lowres : tp . Optional [ bool ] = False , dtype : tp . Optional [ tp . Union [ \"float16\" , \"float32\" ]] = \"float32\" , * args , ** kwargs ): super () . __init__ ( stages = [ 3 , 8 , 36 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , dtype = dtype , * args , ** kwargs )","title":"ResNet152"},{"location":"api/nets/resnet/ResNet152/#elegynetsresnetresnet152","text":"","title":"elegy.nets.resnet.ResNet152"},{"location":"api/nets/resnet/ResNet152/#elegy.nets.resnet.ResNet152","text":"","title":"elegy.nets.resnet.ResNet152"},{"location":"api/nets/resnet/ResNet152/#elegy.nets.resnet.ResNet152.__init__","text":"Instantiates the ResNet152 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional[Union[float16, float32]] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. 'float32' Source code in elegy/nets/resnet.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def __init__ ( self , lowres : tp . Optional [ bool ] = False , dtype : tp . Optional [ tp . Union [ \"float16\" , \"float32\" ]] = \"float32\" , * args , ** kwargs ): super () . __init__ ( stages = [ 3 , 8 , 36 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , dtype = dtype , * args , ** kwargs )","title":"__init__()"},{"location":"api/nets/resnet/ResNet18/","text":"elegy.nets.resnet.ResNet18 __init__ ( self , lowres = False , dtype = 'float32' , * args , ** kwargs ) special Instantiates the ResNet18 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional[Union[float16, float32]] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. 'float32' Source code in elegy/nets/resnet.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def __init__ ( self , lowres : tp . Optional [ bool ] = False , dtype : tp . Optional [ tp . Union [ \"float16\" , \"float32\" ]] = \"float32\" , * args , ** kwargs ): super () . __init__ ( stages = [ 2 , 2 , 2 , 2 ], block_type = ResNetBlock , lowres = lowres , dtype = dtype , * args , ** kwargs )","title":"ResNet18"},{"location":"api/nets/resnet/ResNet18/#elegynetsresnetresnet18","text":"","title":"elegy.nets.resnet.ResNet18"},{"location":"api/nets/resnet/ResNet18/#elegy.nets.resnet.ResNet18","text":"","title":"elegy.nets.resnet.ResNet18"},{"location":"api/nets/resnet/ResNet18/#elegy.nets.resnet.ResNet18.__init__","text":"Instantiates the ResNet18 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional[Union[float16, float32]] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. 'float32' Source code in elegy/nets/resnet.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def __init__ ( self , lowres : tp . Optional [ bool ] = False , dtype : tp . Optional [ tp . Union [ \"float16\" , \"float32\" ]] = \"float32\" , * args , ** kwargs ): super () . __init__ ( stages = [ 2 , 2 , 2 , 2 ], block_type = ResNetBlock , lowres = lowres , dtype = dtype , * args , ** kwargs )","title":"__init__()"},{"location":"api/nets/resnet/ResNet200/","text":"elegy.nets.resnet.ResNet200 __init__ ( self , lowres = False , dtype = 'float32' , * args , ** kwargs ) special Instantiates the ResNet200 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional[Union[float16, float32]] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. 'float32' Source code in elegy/nets/resnet.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def __init__ ( self , lowres : tp . Optional [ bool ] = False , dtype : tp . Optional [ tp . Union [ \"float16\" , \"float32\" ]] = \"float32\" , * args , ** kwargs ): super () . __init__ ( stages = [ 3 , 24 , 36 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , dtype = dtype , * args , ** kwargs )","title":"ResNet200"},{"location":"api/nets/resnet/ResNet200/#elegynetsresnetresnet200","text":"","title":"elegy.nets.resnet.ResNet200"},{"location":"api/nets/resnet/ResNet200/#elegy.nets.resnet.ResNet200","text":"","title":"elegy.nets.resnet.ResNet200"},{"location":"api/nets/resnet/ResNet200/#elegy.nets.resnet.ResNet200.__init__","text":"Instantiates the ResNet200 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional[Union[float16, float32]] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. 'float32' Source code in elegy/nets/resnet.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def __init__ ( self , lowres : tp . Optional [ bool ] = False , dtype : tp . Optional [ tp . Union [ \"float16\" , \"float32\" ]] = \"float32\" , * args , ** kwargs ): super () . __init__ ( stages = [ 3 , 24 , 36 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , dtype = dtype , * args , ** kwargs )","title":"__init__()"},{"location":"api/nets/resnet/ResNet34/","text":"elegy.nets.resnet.ResNet34 __init__ ( self , lowres = False , dtype = 'float32' , * args , ** kwargs ) special Instantiates the ResNet34 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional[Union[float16, float32]] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. 'float32' Source code in elegy/nets/resnet.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def __init__ ( self , lowres : tp . Optional [ bool ] = False , dtype : tp . Optional [ tp . Union [ \"float16\" , \"float32\" ]] = \"float32\" , * args , ** kwargs ): super () . __init__ ( stages = [ 3 , 4 , 6 , 3 ], block_type = ResNetBlock , lowres = lowres , dtype = dtype , * args , ** kwargs )","title":"ResNet34"},{"location":"api/nets/resnet/ResNet34/#elegynetsresnetresnet34","text":"","title":"elegy.nets.resnet.ResNet34"},{"location":"api/nets/resnet/ResNet34/#elegy.nets.resnet.ResNet34","text":"","title":"elegy.nets.resnet.ResNet34"},{"location":"api/nets/resnet/ResNet34/#elegy.nets.resnet.ResNet34.__init__","text":"Instantiates the ResNet34 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional[Union[float16, float32]] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. 'float32' Source code in elegy/nets/resnet.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def __init__ ( self , lowres : tp . Optional [ bool ] = False , dtype : tp . Optional [ tp . Union [ \"float16\" , \"float32\" ]] = \"float32\" , * args , ** kwargs ): super () . __init__ ( stages = [ 3 , 4 , 6 , 3 ], block_type = ResNetBlock , lowres = lowres , dtype = dtype , * args , ** kwargs )","title":"__init__()"},{"location":"api/nets/resnet/ResNet50/","text":"elegy.nets.resnet.ResNet50 __init__ ( self , lowres = False , dtype = 'float32' , * args , ** kwargs ) special Instantiates the ResNet50 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional[Union[float16, float32]] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. 'float32' Source code in elegy/nets/resnet.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def __init__ ( self , lowres : tp . Optional [ bool ] = False , dtype : tp . Optional [ tp . Union [ \"float16\" , \"float32\" ]] = \"float32\" , * args , ** kwargs ): super () . __init__ ( stages = [ 3 , 4 , 6 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , dtype = dtype , * args , ** kwargs )","title":"ResNet50"},{"location":"api/nets/resnet/ResNet50/#elegynetsresnetresnet50","text":"","title":"elegy.nets.resnet.ResNet50"},{"location":"api/nets/resnet/ResNet50/#elegy.nets.resnet.ResNet50","text":"","title":"elegy.nets.resnet.ResNet50"},{"location":"api/nets/resnet/ResNet50/#elegy.nets.resnet.ResNet50.__init__","text":"Instantiates the ResNet50 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres Optional[bool] Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False dtype Optional[Union[float16, float32]] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. 'float32' Source code in elegy/nets/resnet.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def __init__ ( self , lowres : tp . Optional [ bool ] = False , dtype : tp . Optional [ tp . Union [ \"float16\" , \"float32\" ]] = \"float32\" , * args , ** kwargs ): super () . __init__ ( stages = [ 3 , 4 , 6 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , dtype = dtype , * args , ** kwargs )","title":"__init__()"},{"location":"api/nn/AvgPool/","text":"elegy.nn.AvgPool Average pool. Equivalent to partial application of :func: avg_pool . __init__ ( self , window_shape , strides , padding , channel_axis =- 1 , name = None ) special Average pool. Parameters: Name Type Description Default window_shape Union[int, Sequence[int]] Shape of window to pool over. Same rank as value or int . required strides Union[int, Sequence[int]] Strides for the window. Same rank as value or int . required padding str Padding algorithm. Either VALID or SAME . required channel_axis Optional[int] Axis of the spatial channels for which pooling is skipped. -1 Source code in elegy/nn/pool.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def __init__ ( self , window_shape : Union [ int , Sequence [ int ]], strides : Union [ int , Sequence [ int ]], padding : str , channel_axis : Optional [ int ] = - 1 , name : Optional [ str ] = None , ): \"\"\"Average pool. Args: window_shape: Shape of window to pool over. Same rank as value or ``int``. strides: Strides for the window. Same rank as value or ``int``. padding: Padding algorithm. Either ``VALID`` or ``SAME``. channel_axis: Axis of the spatial channels for which pooling is skipped. \"\"\" super () . __init__ ( name = name ) self . window_shape = window_shape self . strides = strides self . padding = padding self . channel_axis = channel_axis add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/pool.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/pool.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/pool.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/pool.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/pool.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"AvgPool"},{"location":"api/nn/AvgPool/#elegynnavgpool","text":"","title":"elegy.nn.AvgPool"},{"location":"api/nn/AvgPool/#elegy.nn.pool.AvgPool","text":"Average pool. Equivalent to partial application of :func: avg_pool .","title":"elegy.nn.pool.AvgPool"},{"location":"api/nn/AvgPool/#elegy.nn.pool.AvgPool.__init__","text":"Average pool. Parameters: Name Type Description Default window_shape Union[int, Sequence[int]] Shape of window to pool over. Same rank as value or int . required strides Union[int, Sequence[int]] Strides for the window. Same rank as value or int . required padding str Padding algorithm. Either VALID or SAME . required channel_axis Optional[int] Axis of the spatial channels for which pooling is skipped. -1 Source code in elegy/nn/pool.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def __init__ ( self , window_shape : Union [ int , Sequence [ int ]], strides : Union [ int , Sequence [ int ]], padding : str , channel_axis : Optional [ int ] = - 1 , name : Optional [ str ] = None , ): \"\"\"Average pool. Args: window_shape: Shape of window to pool over. Same rank as value or ``int``. strides: Strides for the window. Same rank as value or ``int``. padding: Padding algorithm. Either ``VALID`` or ``SAME``. channel_axis: Axis of the spatial channels for which pooling is skipped. \"\"\" super () . __init__ ( name = name ) self . window_shape = window_shape self . strides = strides self . padding = padding self . channel_axis = channel_axis","title":"__init__()"},{"location":"api/nn/AvgPool/#elegy.nn.pool.AvgPool.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/pool.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/AvgPool/#elegy.nn.pool.AvgPool.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/pool.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/AvgPool/#elegy.nn.pool.AvgPool.init","text":"Initializes the module, Source code in elegy/nn/pool.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/AvgPool/#elegy.nn.pool.AvgPool.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/pool.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/AvgPool/#elegy.nn.pool.AvgPool.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/pool.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/BatchNormalization/","text":"elegy.nn.BatchNormalization Normalizes inputs to maintain a mean of ~0 and stddev of ~1. See: https://arxiv.org/abs/1502.03167. There are many different variations for how users want to manage scale and offset if they require them at all. These are: No scale/offset in which case create_* should be set to False and scale / offset aren't passed when the module is called. Trainable scale/offset in which case create_* should be set to True and again scale / offset aren't passed when the module is called. In this case this module creates and owns the scale / offset variables. Externally generated scale / offset , such as for conditional normalization, in which case create_* should be set to False and then the values fed in at call time. NOTE: jax.vmap(hk.transform(BatchNorm)) will update summary statistics and normalize values on a per-batch basis; we currently do not support normalizing across a batch axis introduced by vmap. __init__ ( self , create_scale = True , create_offset = True , decay_rate = 0.99 , eps = 1e-05 , scale_init = None , offset_init = None , axis = None , cross_replica_axis = None , data_format = 'channels_last' , ** kwargs ) special Constructs a BatchNorm module. Parameters: Name Type Description Default create_scale bool Whether to include a trainable scaling factor. True create_offset bool Whether to include a trainable offset. True decay_rate float Decay rate for EMA. 0.99 eps float Small epsilon to avoid division by zero variance. Defaults 1e-5 , as in the paper and Sonnet. 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for gain (aka scale). Can only be set if create_scale=True . By default, 1 . None offset_init Optional[elegy.types.Initializer] Optional initializer for bias (aka offset). Can only be set if create_offset=True . By default, 0 . None axis Optional[Sequence[int]] Which axes to reduce over. The default ( None ) signifies that all but the channel axis should be normalized. Otherwise this is a list of axis indices which will have normalization statistics calculated. None cross_replica_axis Optional[str] If not None , it should be a string representing the axis name over which this module is being run within a jax.pmap . Supplying this argument means that batch statistics are calculated across all replicas on that axis. None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default it is channels_last . 'channels_last' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/batch_normalization.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , create_scale : bool = True , create_offset : bool = True , decay_rate : float = 0.99 , eps : float = 1e-5 , scale_init : Optional [ initializers . Initializer ] = None , offset_init : Optional [ initializers . Initializer ] = None , axis : Optional [ Sequence [ int ]] = None , cross_replica_axis : Optional [ str ] = None , data_format : str = \"channels_last\" , ** kwargs ): \"\"\"Constructs a BatchNorm module. Args: create_scale: Whether to include a trainable scaling factor. create_offset: Whether to include a trainable offset. decay_rate: Decay rate for EMA. eps: Small epsilon to avoid division by zero variance. Defaults ``1e-5``, as in the paper and Sonnet. scale_init: Optional initializer for gain (aka scale). Can only be set if ``create_scale=True``. By default, ``1``. offset_init: Optional initializer for bias (aka offset). Can only be set if ``create_offset=True``. By default, ``0``. axis: Which axes to reduce over. The default (``None``) signifies that all but the channel axis should be normalized. Otherwise this is a list of axis indices which will have normalization statistics calculated. cross_replica_axis: If not ``None``, it should be a string representing the axis name over which this module is being run within a ``jax.pmap``. Supplying this argument means that batch statistics are calculated across all replicas on that axis. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default it is ``channels_last``. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if not create_scale and scale_init is not None : raise ValueError ( \"Cannot set `scale_init` if `create_scale=False`\" ) if not create_offset and offset_init is not None : raise ValueError ( \"Cannot set `offset_init` if `create_offset=False`\" ) self . create_scale = create_scale self . create_offset = create_offset self . eps = eps self . scale_init = scale_init or jnp . ones self . offset_init = offset_init or jnp . zeros self . axis = axis self . cross_replica_axis = cross_replica_axis self . channel_index = haiku_utils . get_channel_index ( data_format ) self . mean_ema = ExponentialMovingAverage ( decay_rate , name = \"mean_ema\" ) self . var_ema = ExponentialMovingAverage ( decay_rate , name = \"var_ema\" ) add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/batch_normalization.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value call ( self , inputs , training = None , test_local_stats = False , scale = None , offset = None ) Computes the normalized version of the input. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [..., C] . required training Optional[bool] Whether training is currently happening. None test_local_stats bool Whether local stats are used when training=False. False scale Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized across all but the last dimension. Source code in elegy/nn/batch_normalization.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def call ( self , inputs : jnp . ndarray , training : tp . Optional [ bool ] = None , test_local_stats : bool = False , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Computes the normalized version of the input. Args: inputs: An array, where the data format is ``[..., C]``. training: Whether training is currently happening. test_local_stats: Whether local stats are used when training=False. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized across all but the last dimension. \"\"\" inputs = jnp . asarray ( inputs , jnp . float32 ) if training is None : training = module . is_training () if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) channel_index = self . channel_index if channel_index < 0 : channel_index += inputs . ndim if self . axis is not None : axis = self . axis else : axis = [ i for i in range ( inputs . ndim ) if i != channel_index ] if training or test_local_stats : cross_replica_axis = self . cross_replica_axis if self . cross_replica_axis : mean = jnp . mean ( inputs , axis , keepdims = True ) mean = jax . lax . pmean ( mean , cross_replica_axis ) mean_of_squares = jnp . mean ( inputs ** 2 , axis , keepdims = True ) mean_of_squares = jax . lax . pmean ( mean_of_squares , cross_replica_axis ) var = mean_of_squares - mean ** 2 else : mean = jnp . mean ( inputs , axis , keepdims = True ) # This uses E[(X - E[X])^2]. # TODO(tycai): Consider the faster, but possibly less stable # E[X^2] - E[X]^2 method. var = jnp . var ( inputs , axis , keepdims = True ) else : mean = self . mean_ema . average var = self . var_ema . average if training : self . mean_ema ( mean ) self . var_ema ( var ) w_shape = [ 1 if i in axis else inputs . shape [ i ] for i in range ( inputs . ndim )] w_dtype = jnp . float32 if self . create_scale : scale = self . add_parameter ( \"scale\" , w_shape , w_dtype , self . scale_init ) elif scale is None : scale = np . ones ([], dtype = w_dtype ) if self . create_offset : offset = self . add_parameter ( \"offset\" , w_shape , w_dtype , self . offset_init ) elif offset is None : offset = np . zeros ([], dtype = w_dtype ) inv = scale * jax . lax . rsqrt ( var + self . eps ) output = ( inputs - mean ) * inv + offset return jnp . asarray ( output , self . dtype ) get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/batch_normalization.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/batch_normalization.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/batch_normalization.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/batch_normalization.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"BatchNormalization"},{"location":"api/nn/BatchNormalization/#elegynnbatchnormalization","text":"","title":"elegy.nn.BatchNormalization"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization","text":"Normalizes inputs to maintain a mean of ~0 and stddev of ~1. See: https://arxiv.org/abs/1502.03167. There are many different variations for how users want to manage scale and offset if they require them at all. These are: No scale/offset in which case create_* should be set to False and scale / offset aren't passed when the module is called. Trainable scale/offset in which case create_* should be set to True and again scale / offset aren't passed when the module is called. In this case this module creates and owns the scale / offset variables. Externally generated scale / offset , such as for conditional normalization, in which case create_* should be set to False and then the values fed in at call time. NOTE: jax.vmap(hk.transform(BatchNorm)) will update summary statistics and normalize values on a per-batch basis; we currently do not support normalizing across a batch axis introduced by vmap.","title":"elegy.nn.batch_normalization.BatchNormalization"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.__init__","text":"Constructs a BatchNorm module. Parameters: Name Type Description Default create_scale bool Whether to include a trainable scaling factor. True create_offset bool Whether to include a trainable offset. True decay_rate float Decay rate for EMA. 0.99 eps float Small epsilon to avoid division by zero variance. Defaults 1e-5 , as in the paper and Sonnet. 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for gain (aka scale). Can only be set if create_scale=True . By default, 1 . None offset_init Optional[elegy.types.Initializer] Optional initializer for bias (aka offset). Can only be set if create_offset=True . By default, 0 . None axis Optional[Sequence[int]] Which axes to reduce over. The default ( None ) signifies that all but the channel axis should be normalized. Otherwise this is a list of axis indices which will have normalization statistics calculated. None cross_replica_axis Optional[str] If not None , it should be a string representing the axis name over which this module is being run within a jax.pmap . Supplying this argument means that batch statistics are calculated across all replicas on that axis. None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default it is channels_last . 'channels_last' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/batch_normalization.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , create_scale : bool = True , create_offset : bool = True , decay_rate : float = 0.99 , eps : float = 1e-5 , scale_init : Optional [ initializers . Initializer ] = None , offset_init : Optional [ initializers . Initializer ] = None , axis : Optional [ Sequence [ int ]] = None , cross_replica_axis : Optional [ str ] = None , data_format : str = \"channels_last\" , ** kwargs ): \"\"\"Constructs a BatchNorm module. Args: create_scale: Whether to include a trainable scaling factor. create_offset: Whether to include a trainable offset. decay_rate: Decay rate for EMA. eps: Small epsilon to avoid division by zero variance. Defaults ``1e-5``, as in the paper and Sonnet. scale_init: Optional initializer for gain (aka scale). Can only be set if ``create_scale=True``. By default, ``1``. offset_init: Optional initializer for bias (aka offset). Can only be set if ``create_offset=True``. By default, ``0``. axis: Which axes to reduce over. The default (``None``) signifies that all but the channel axis should be normalized. Otherwise this is a list of axis indices which will have normalization statistics calculated. cross_replica_axis: If not ``None``, it should be a string representing the axis name over which this module is being run within a ``jax.pmap``. Supplying this argument means that batch statistics are calculated across all replicas on that axis. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default it is ``channels_last``. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if not create_scale and scale_init is not None : raise ValueError ( \"Cannot set `scale_init` if `create_scale=False`\" ) if not create_offset and offset_init is not None : raise ValueError ( \"Cannot set `offset_init` if `create_offset=False`\" ) self . create_scale = create_scale self . create_offset = create_offset self . eps = eps self . scale_init = scale_init or jnp . ones self . offset_init = offset_init or jnp . zeros self . axis = axis self . cross_replica_axis = cross_replica_axis self . channel_index = haiku_utils . get_channel_index ( data_format ) self . mean_ema = ExponentialMovingAverage ( decay_rate , name = \"mean_ema\" ) self . var_ema = ExponentialMovingAverage ( decay_rate , name = \"var_ema\" )","title":"__init__()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/batch_normalization.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.call","text":"Computes the normalized version of the input. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [..., C] . required training Optional[bool] Whether training is currently happening. None test_local_stats bool Whether local stats are used when training=False. False scale Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized across all but the last dimension. Source code in elegy/nn/batch_normalization.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def call ( self , inputs : jnp . ndarray , training : tp . Optional [ bool ] = None , test_local_stats : bool = False , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Computes the normalized version of the input. Args: inputs: An array, where the data format is ``[..., C]``. training: Whether training is currently happening. test_local_stats: Whether local stats are used when training=False. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized across all but the last dimension. \"\"\" inputs = jnp . asarray ( inputs , jnp . float32 ) if training is None : training = module . is_training () if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) channel_index = self . channel_index if channel_index < 0 : channel_index += inputs . ndim if self . axis is not None : axis = self . axis else : axis = [ i for i in range ( inputs . ndim ) if i != channel_index ] if training or test_local_stats : cross_replica_axis = self . cross_replica_axis if self . cross_replica_axis : mean = jnp . mean ( inputs , axis , keepdims = True ) mean = jax . lax . pmean ( mean , cross_replica_axis ) mean_of_squares = jnp . mean ( inputs ** 2 , axis , keepdims = True ) mean_of_squares = jax . lax . pmean ( mean_of_squares , cross_replica_axis ) var = mean_of_squares - mean ** 2 else : mean = jnp . mean ( inputs , axis , keepdims = True ) # This uses E[(X - E[X])^2]. # TODO(tycai): Consider the faster, but possibly less stable # E[X^2] - E[X]^2 method. var = jnp . var ( inputs , axis , keepdims = True ) else : mean = self . mean_ema . average var = self . var_ema . average if training : self . mean_ema ( mean ) self . var_ema ( var ) w_shape = [ 1 if i in axis else inputs . shape [ i ] for i in range ( inputs . ndim )] w_dtype = jnp . float32 if self . create_scale : scale = self . add_parameter ( \"scale\" , w_shape , w_dtype , self . scale_init ) elif scale is None : scale = np . ones ([], dtype = w_dtype ) if self . create_offset : offset = self . add_parameter ( \"offset\" , w_shape , w_dtype , self . offset_init ) elif offset is None : offset = np . zeros ([], dtype = w_dtype ) inv = scale * jax . lax . rsqrt ( var + self . eps ) output = ( inputs - mean ) * inv + offset return jnp . asarray ( output , self . dtype )","title":"call()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/batch_normalization.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.init","text":"Initializes the module, Source code in elegy/nn/batch_normalization.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/batch_normalization.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/batch_normalization.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/Conv1D/","text":"elegy.nn.Conv1D One dimensional convolution. __init__ ( self , output_channels , kernel_shape , stride = 1 , rate = 1 , padding = 'SAME' , with_bias = True , w_init = None , b_init = None , data_format = 'NWC' , mask = None , groups = 1 , ** kwargs ) special Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 1. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 1. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 1. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 1. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NWC or NCW . By default, NWC . 'NWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NWC\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 1. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 1. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 1. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 1. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NWC`` or ``NCW``. By default, ``NWC``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 1 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , groups = groups , ** kwargs , ) add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/conv.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value call ( self , inputs ) inherited Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , w_shape , jnp . float32 , initializer = w_init ) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , bias_shape , jnp . float32 , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/conv.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"Conv1D"},{"location":"api/nn/Conv1D/#elegynnconv1d","text":"","title":"elegy.nn.Conv1D"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D","text":"One dimensional convolution.","title":"elegy.nn.conv.Conv1D"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.__init__","text":"Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 1. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 1. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 1. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 1. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NWC or NCW . By default, NWC . 'NWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NWC\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 1. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 1. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 1. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 1. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NWC`` or ``NCW``. By default, ``NWC``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 1 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , groups = groups , ** kwargs , )","title":"__init__()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/conv.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.call","text":"Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , w_shape , jnp . float32 , initializer = w_init ) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , bias_shape , jnp . float32 , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out","title":"call()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.init","text":"Initializes the module, Source code in elegy/nn/conv.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/Conv2D/","text":"elegy.nn.Conv2D Two dimensional convolution. __init__ ( self , output_channels , kernel_shape , stride = 1 , rate = 1 , padding = 'SAME' , with_bias = True , w_init = None , b_init = None , data_format = 'NHWC' , mask = None , groups = 1 , ** kwargs ) special Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 2. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 2. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 2. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 2. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NHWC or NCHW . By default, NHWC . 'NHWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NHWC\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 2. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 2. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 2. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 2. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NHWC`` or ``NCHW``. By default, ``NHWC``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 2 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , groups = groups , ** kwargs , ) add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/conv.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value call ( self , inputs ) inherited Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , w_shape , jnp . float32 , initializer = w_init ) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , bias_shape , jnp . float32 , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/conv.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"Conv2D"},{"location":"api/nn/Conv2D/#elegynnconv2d","text":"","title":"elegy.nn.Conv2D"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D","text":"Two dimensional convolution.","title":"elegy.nn.conv.Conv2D"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.__init__","text":"Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 2. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 2. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 2. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 2. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NHWC or NCHW . By default, NHWC . 'NHWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NHWC\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 2. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 2. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 2. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 2. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NHWC`` or ``NCHW``. By default, ``NHWC``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 2 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , groups = groups , ** kwargs , )","title":"__init__()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/conv.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.call","text":"Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , w_shape , jnp . float32 , initializer = w_init ) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , bias_shape , jnp . float32 , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out","title":"call()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.init","text":"Initializes the module, Source code in elegy/nn/conv.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/Conv3D/","text":"elegy.nn.Conv3D Three dimensional convolution. __init__ ( self , output_channels , kernel_shape , stride = 1 , rate = 1 , padding = 'SAME' , with_bias = True , w_init = None , b_init = None , data_format = 'NDHWC' , mask = None , groups = 1 , ** kwargs ) special Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 3. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 3. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 3. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 3. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NDHWC or NCDHW . By default, NDHWC . 'NDHWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NDHWC\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 3. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 3. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 3. 1 corresponds to standard ND convolution, `rate > 1` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 3. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NDHWC`` or ``NCDHW``. By default, ``NDHWC``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 3 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , groups = groups , ** kwargs , ) add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/conv.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value call ( self , inputs ) inherited Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , w_shape , jnp . float32 , initializer = w_init ) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , bias_shape , jnp . float32 , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/conv.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"Conv3D"},{"location":"api/nn/Conv3D/#elegynnconv3d","text":"","title":"elegy.nn.Conv3D"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D","text":"Three dimensional convolution.","title":"elegy.nn.conv.Conv3D"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.__init__","text":"Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 3. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 3. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 3. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 3. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NDHWC or NCDHW . By default, NDHWC . 'NDHWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NDHWC\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 3. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 3. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 3. 1 corresponds to standard ND convolution, `rate > 1` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 3. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NDHWC`` or ``NCDHW``. By default, ``NDHWC``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 3 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , groups = groups , ** kwargs , )","title":"__init__()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/conv.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.call","text":"Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , w_shape , jnp . float32 , initializer = w_init ) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , bias_shape , jnp . float32 , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out","title":"call()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.init","text":"Initializes the module, Source code in elegy/nn/conv.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/ConvND/","text":"elegy.nn.ConvND General N-dimensional convolutional. __init__ ( self , num_spatial_dims , output_channels , kernel_shape , stride = 1 , rate = 1 , padding = 'SAME' , with_bias = True , w_init = None , b_init = None , data_format = 'channels_last' , mask = None , groups = 1 , ** kwargs ) special Initializes the module. Parameters: Name Type Description Default num_spatial_dims int The number of spatial dimensions of the input. required output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length num_spatial_dims . required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length num_spatial_dims . Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length num_spatial_dims . 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a sequence of n (low, high) integer pairs that give the padding to apply before and after each spatial dimension. or a callable or sequence of callables of size num_spatial_dims . Any callables must take a single integer argument equal to the effective kernel size and return a sequence of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default, channels_last . 'channels_last' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def __init__ ( self , num_spatial_dims : int , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"channels_last\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: num_spatial_dims: The number of spatial dimensions of the input. output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length ``num_spatial_dims``. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length ``num_spatial_dims``. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length ``num_spatial_dims``. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a sequence of n ``(low, high)`` integer pairs that give the padding to apply before and after each spatial dimension. or a callable or sequence of callables of size ``num_spatial_dims``. Any callables must take a single integer argument equal to the effective kernel size and return a sequence of two integers representing the padding before and after. See ``haiku.pad.*`` for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default, ``channels_last``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if num_spatial_dims <= 0 : raise ValueError ( \"We only support convolution operations for `num_spatial_dims` \" f \"greater than 0, received num_spatial_dims= { num_spatial_dims } .\" ) self . num_spatial_dims = num_spatial_dims self . output_channels = output_channels self . kernel_shape = hk_utils . replicate ( kernel_shape , num_spatial_dims , \"kernel_shape\" ) self . with_bias = with_bias self . stride = hk_utils . replicate ( stride , num_spatial_dims , \"strides\" ) self . w_init = w_init self . b_init = b_init or jnp . zeros self . mask = mask self . lhs_dilation = hk_utils . replicate ( 1 , num_spatial_dims , \"lhs_dilation\" ) self . kernel_dilation = hk_utils . replicate ( rate , num_spatial_dims , \"kernel_dilation\" ) self . data_format = data_format self . channel_index = hk_utils . get_channel_index ( data_format ) self . dimension_numbers = to_dimension_numbers ( num_spatial_dims , channels_last = ( self . channel_index == - 1 ), transpose = False ) self . groups = groups if isinstance ( padding , str ): self . padding = padding . upper () else : self . padding = hk . pad . create ( padding = padding , kernel = self . kernel_shape , rate = self . kernel_dilation , n = self . num_spatial_dims , ) add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/conv.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value call ( self , inputs ) Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , w_shape , jnp . float32 , initializer = w_init ) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , bias_shape , jnp . float32 , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/conv.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"ConvND"},{"location":"api/nn/ConvND/#elegynnconvnd","text":"","title":"elegy.nn.ConvND"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND","text":"General N-dimensional convolutional.","title":"elegy.nn.conv.ConvND"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.__init__","text":"Initializes the module. Parameters: Name Type Description Default num_spatial_dims int The number of spatial dimensions of the input. required output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length num_spatial_dims . required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length num_spatial_dims . Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length num_spatial_dims . 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a sequence of n (low, high) integer pairs that give the padding to apply before and after each spatial dimension. or a callable or sequence of callables of size num_spatial_dims . Any callables must take a single integer argument equal to the effective kernel size and return a sequence of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default, channels_last . 'channels_last' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def __init__ ( self , num_spatial_dims : int , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"channels_last\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: num_spatial_dims: The number of spatial dimensions of the input. output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length ``num_spatial_dims``. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length ``num_spatial_dims``. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length ``num_spatial_dims``. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a sequence of n ``(low, high)`` integer pairs that give the padding to apply before and after each spatial dimension. or a callable or sequence of callables of size ``num_spatial_dims``. Any callables must take a single integer argument equal to the effective kernel size and return a sequence of two integers representing the padding before and after. See ``haiku.pad.*`` for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default, ``channels_last``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if num_spatial_dims <= 0 : raise ValueError ( \"We only support convolution operations for `num_spatial_dims` \" f \"greater than 0, received num_spatial_dims= { num_spatial_dims } .\" ) self . num_spatial_dims = num_spatial_dims self . output_channels = output_channels self . kernel_shape = hk_utils . replicate ( kernel_shape , num_spatial_dims , \"kernel_shape\" ) self . with_bias = with_bias self . stride = hk_utils . replicate ( stride , num_spatial_dims , \"strides\" ) self . w_init = w_init self . b_init = b_init or jnp . zeros self . mask = mask self . lhs_dilation = hk_utils . replicate ( 1 , num_spatial_dims , \"lhs_dilation\" ) self . kernel_dilation = hk_utils . replicate ( rate , num_spatial_dims , \"kernel_dilation\" ) self . data_format = data_format self . channel_index = hk_utils . get_channel_index ( data_format ) self . dimension_numbers = to_dimension_numbers ( num_spatial_dims , channels_last = ( self . channel_index == - 1 ), transpose = False ) self . groups = groups if isinstance ( padding , str ): self . padding = padding . upper () else : self . padding = hk . pad . create ( padding = padding , kernel = self . kernel_shape , rate = self . kernel_dilation , n = self . num_spatial_dims , )","title":"__init__()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/conv.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.call","text":"Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , w_shape , jnp . float32 , initializer = w_init ) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , bias_shape , jnp . float32 , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out","title":"call()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.init","text":"Initializes the module, Source code in elegy/nn/conv.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/Dropout/","text":"elegy.nn.Dropout Applies Dropout to the input. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged. Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit , training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer. Example dropout = elegy . nn . Dropout ( 0.2 ) data = np . arange ( 10 ) . reshape ( 5 , 2 ) . astype ( np . float32 ) print ( data ) # [[0. 1.] # [2. 3.] # [4. 5.] # [6. 7.] # [8. 9.]] outputs = dropout ( data , training = True ) print ( outputs ) # [[ 0. 1.25] # [ 2.5 3.75] # [ 5. 6.25] # [ 7.5 0. ] # [10. 0. ]] add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/dropout.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value call ( self , x , training = None , rng = None ) Parameters: Name Type Description Default x ndarray The value to be dropped out. required training Optional[bool] Whether training is currently happening. None rng Optional[numpy.ndarray] Optional RNGKey. None Returns: Type Description ndarray x but dropped out and scaled by 1 / (1 - rate) . Source code in elegy/nn/dropout.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def call ( self , x : np . ndarray , training : tp . Optional [ bool ] = None , rng : tp . Optional [ np . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Arguments: x: The value to be dropped out. training: Whether training is currently happening. rng: Optional RNGKey. Returns: x but dropped out and scaled by `1 / (1 - rate)`. \"\"\" if training is None : training = module . is_training () return hk . dropout ( rng = rng if rng is not None else module . next_rng_key (), rate = self . rate if training else 0.0 , x = x , ) get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/dropout.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/dropout.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/dropout.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/dropout.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"Dropout"},{"location":"api/nn/Dropout/#elegynndropout","text":"","title":"elegy.nn.Dropout"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout","text":"Applies Dropout to the input. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged. Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit , training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer.","title":"elegy.nn.dropout.Dropout"},{"location":"api/nn/Dropout/#example","text":"dropout = elegy . nn . Dropout ( 0.2 ) data = np . arange ( 10 ) . reshape ( 5 , 2 ) . astype ( np . float32 ) print ( data ) # [[0. 1.] # [2. 3.] # [4. 5.] # [6. 7.] # [8. 9.]] outputs = dropout ( data , training = True ) print ( outputs ) # [[ 0. 1.25] # [ 2.5 3.75] # [ 5. 6.25] # [ 7.5 0. ] # [10. 0. ]]","title":"Example"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/dropout.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.call","text":"Parameters: Name Type Description Default x ndarray The value to be dropped out. required training Optional[bool] Whether training is currently happening. None rng Optional[numpy.ndarray] Optional RNGKey. None Returns: Type Description ndarray x but dropped out and scaled by 1 / (1 - rate) . Source code in elegy/nn/dropout.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def call ( self , x : np . ndarray , training : tp . Optional [ bool ] = None , rng : tp . Optional [ np . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Arguments: x: The value to be dropped out. training: Whether training is currently happening. rng: Optional RNGKey. Returns: x but dropped out and scaled by `1 / (1 - rate)`. \"\"\" if training is None : training = module . is_training () return hk . dropout ( rng = rng if rng is not None else module . next_rng_key (), rate = self . rate if training else 0.0 , x = x , )","title":"call()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/dropout.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.init","text":"Initializes the module, Source code in elegy/nn/dropout.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/dropout.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/dropout.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/EmbedLookupStyle/","text":"elegy.nn.EmbedLookupStyle How to return the embedding matrices given IDs. __class__ inherited Metaclass for Enum __members__ property readonly special Returns a mapping of member name->value. This mapping lists all enum members, including aliases. Note that this is a read-only view of the internal mapping. __bool__ ( self ) special classes/types should always be True. Source code in elegy/nn/embedding.py 272 273 274 275 276 def __bool__ ( self ): \"\"\" classes/types should always be True. \"\"\" return True __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ) special Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: value will be the name of the new class. names should be either a string of white-space/comma delimited names (values will start at start ), or an iterator/mapping of name, value pairs. module should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. qualname should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. type , if set, will be mixed in as the first base class. Source code in elegy/nn/embedding.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ): \"\"\"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: `value` will be the name of the new class. `names` should be either a string of white-space/comma delimited names (values will start at `start`), or an iterator/mapping of name, value pairs. `module` should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. `qualname` should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. `type`, if set, will be mixed in as the first base class. \"\"\" if names is None : # simple value lookup return cls . __new__ ( cls , value ) # otherwise, functional API: we're creating a new Enum type return cls . _create_ ( value , names , module = module , qualname = qualname , type = type , start = start ) __getattr__ ( cls , name ) special Return the enum member matching name We use getattr instead of descriptors or inserting into the enum class' dict in order to support name and value being both properties for enum members (which live in the class' dict ) and enum members themselves. Source code in elegy/nn/embedding.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def __getattr__ ( cls , name ): \"\"\"Return the enum member matching `name` We use __getattr__ instead of descriptors or inserting into the enum class' __dict__ in order to support `name` and `value` being both properties for enum members (which live in the class' __dict__) and enum members themselves. \"\"\" if _is_dunder ( name ): raise AttributeError ( name ) try : return cls . _member_map_ [ name ] except KeyError : raise AttributeError ( name ) from None __new__ ( metacls , cls , bases , classdict ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/nn/embedding.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def __new__ ( metacls , cls , bases , classdict ): # an Enum class is final once enumeration items have been defined; it # cannot be mixed with other types (int, float, etc.) if it has an # inherited __new__ unless a new __new__ is defined (or the resulting # class will fail). # # remove any keys listed in _ignore_ classdict . setdefault ( '_ignore_' , []) . append ( '_ignore_' ) ignore = classdict [ '_ignore_' ] for key in ignore : classdict . pop ( key , None ) member_type , first_enum = metacls . _get_mixins_ ( bases ) __new__ , save_new , use_args = metacls . _find_new_ ( classdict , member_type , first_enum ) # save enum items into separate mapping so they don't get baked into # the new class enum_members = { k : classdict [ k ] for k in classdict . _member_names } for name in classdict . _member_names : del classdict [ name ] # adjust the sunders _order_ = classdict . pop ( '_order_' , None ) # check for illegal enum names (any others?) invalid_names = set ( enum_members ) & { 'mro' , '' } if invalid_names : raise ValueError ( 'Invalid enum member name: {0} ' . format ( ',' . join ( invalid_names ))) # create a default docstring if one has not been provided if '__doc__' not in classdict : classdict [ '__doc__' ] = 'An enumeration.' # create our new Enum type enum_class = super () . __new__ ( metacls , cls , bases , classdict ) enum_class . _member_names_ = [] # names in definition order enum_class . _member_map_ = {} # name->value map enum_class . _member_type_ = member_type # save DynamicClassAttribute attributes from super classes so we know # if we can take the shortcut of storing members in the class dict dynamic_attributes = { k for c in enum_class . mro () for k , v in c . __dict__ . items () if isinstance ( v , DynamicClassAttribute )} # Reverse value->name map for hashable values. enum_class . _value2member_map_ = {} # If a custom type is mixed into the Enum, and it does not know how # to pickle itself, pickle.dumps will succeed but pickle.loads will # fail. Rather than have the error show up later and possibly far # from the source, sabotage the pickle protocol for this class so # that pickle.dumps also fails. # # However, if the new class implements its own __reduce_ex__, do not # sabotage -- it's on them to make sure it works correctly. We use # __reduce_ex__ instead of any of the others as it is preferred by # pickle over __reduce__, and it handles all pickle protocols. if '__reduce_ex__' not in classdict : if member_type is not object : methods = ( '__getnewargs_ex__' , '__getnewargs__' , '__reduce_ex__' , '__reduce__' ) if not any ( m in member_type . __dict__ for m in methods ): _make_class_unpicklable ( enum_class ) # instantiate them, checking for duplicates as we go # we instantiate first instead of checking for duplicates first in case # a custom __new__ is doing something funky with the values -- such as # auto-numbering ;) for member_name in classdict . _member_names : value = enum_members [ member_name ] if not isinstance ( value , tuple ): args = ( value , ) else : args = value if member_type is tuple : # special case for tuple enums args = ( args , ) # wrap it one more time if not use_args : enum_member = __new__ ( enum_class ) if not hasattr ( enum_member , '_value_' ): enum_member . _value_ = value else : enum_member = __new__ ( enum_class , * args ) if not hasattr ( enum_member , '_value_' ): if member_type is object : enum_member . _value_ = value else : enum_member . _value_ = member_type ( * args ) value = enum_member . _value_ enum_member . _name_ = member_name enum_member . __objclass__ = enum_class enum_member . __init__ ( * args ) # If another member with the same value was already defined, the # new member becomes an alias to the existing one. for name , canonical_member in enum_class . _member_map_ . items (): if canonical_member . _value_ == enum_member . _value_ : enum_member = canonical_member break else : # Aliases don't appear in member names (only in __members__). enum_class . _member_names_ . append ( member_name ) # performance boost for any member that would not shadow # a DynamicClassAttribute if member_name not in dynamic_attributes : setattr ( enum_class , member_name , enum_member ) # now add to _member_map_ enum_class . _member_map_ [ member_name ] = enum_member try : # This may fail if value is not hashable. We can't add the value # to the map, and by-value lookups for this value will be # linear. enum_class . _value2member_map_ [ value ] = enum_member except TypeError : pass # double check that repr and friends are not the mixin's or various # things break (such as pickle) for name in ( '__repr__' , '__str__' , '__format__' , '__reduce_ex__' ): class_method = getattr ( enum_class , name ) obj_method = getattr ( member_type , name , None ) enum_method = getattr ( first_enum , name , None ) if obj_method is not None and obj_method is class_method : setattr ( enum_class , name , enum_method ) # replace any other __new__ with our own (as long as Enum is not None, # anyway) -- again, this is to support pickle if Enum is not None : # if the user defined their own __new__, save it before it gets # clobbered in case they subclass later if save_new : enum_class . __new_member__ = __new__ enum_class . __new__ = Enum . __new__ # py3 support for definition order (helps keep py2/py3 code in sync) if _order_ is not None : if isinstance ( _order_ , str ): _order_ = _order_ . replace ( ',' , ' ' ) . split () if _order_ != enum_class . _member_names_ : raise TypeError ( 'member order does not match _order_' ) return enum_class __prepare__ ( cls , bases ) classmethod special prepare () -> dict used to create the namespace for the class statement Source code in elegy/nn/embedding.py 119 120 121 122 123 124 125 126 127 @classmethod def __prepare__ ( metacls , cls , bases ): # create the namespace dict enum_dict = _EnumDict () # inherit previous flags and _generate_next_value_ function member_type , first_enum = metacls . _get_mixins_ ( bases ) if first_enum is not None : enum_dict [ '_generate_next_value_' ] = getattr ( first_enum , '_generate_next_value_' , None ) return enum_dict __setattr__ ( cls , name , value ) special Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. Source code in elegy/nn/embedding.py 368 369 370 371 372 373 374 375 376 377 378 379 def __setattr__ ( cls , name , value ): \"\"\"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. \"\"\" member_map = cls . __dict__ . get ( '_member_map_' , {}) if name in member_map : raise AttributeError ( 'Cannot reassign members.' ) super () . __setattr__ ( name , value )","title":"EmbedLookupStyle"},{"location":"api/nn/EmbedLookupStyle/#elegynnembedlookupstyle","text":"","title":"elegy.nn.EmbedLookupStyle"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle","text":"How to return the embedding matrices given IDs.","title":"elegy.nn.embedding.EmbedLookupStyle"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__","text":"Metaclass for Enum","title":"__class__"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__members__","text":"Returns a mapping of member name->value. This mapping lists all enum members, including aliases. Note that this is a read-only view of the internal mapping.","title":"__members__"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__bool__","text":"classes/types should always be True. Source code in elegy/nn/embedding.py 272 273 274 275 276 def __bool__ ( self ): \"\"\" classes/types should always be True. \"\"\" return True","title":"__bool__()"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__call__","text":"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: value will be the name of the new class. names should be either a string of white-space/comma delimited names (values will start at start ), or an iterator/mapping of name, value pairs. module should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. qualname should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. type , if set, will be mixed in as the first base class. Source code in elegy/nn/embedding.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ): \"\"\"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: `value` will be the name of the new class. `names` should be either a string of white-space/comma delimited names (values will start at `start`), or an iterator/mapping of name, value pairs. `module` should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. `qualname` should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. `type`, if set, will be mixed in as the first base class. \"\"\" if names is None : # simple value lookup return cls . __new__ ( cls , value ) # otherwise, functional API: we're creating a new Enum type return cls . _create_ ( value , names , module = module , qualname = qualname , type = type , start = start )","title":"__call__()"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__getattr__","text":"Return the enum member matching name We use getattr instead of descriptors or inserting into the enum class' dict in order to support name and value being both properties for enum members (which live in the class' dict ) and enum members themselves. Source code in elegy/nn/embedding.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def __getattr__ ( cls , name ): \"\"\"Return the enum member matching `name` We use __getattr__ instead of descriptors or inserting into the enum class' __dict__ in order to support `name` and `value` being both properties for enum members (which live in the class' __dict__) and enum members themselves. \"\"\" if _is_dunder ( name ): raise AttributeError ( name ) try : return cls . _member_map_ [ name ] except KeyError : raise AttributeError ( name ) from None","title":"__getattr__()"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/nn/embedding.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def __new__ ( metacls , cls , bases , classdict ): # an Enum class is final once enumeration items have been defined; it # cannot be mixed with other types (int, float, etc.) if it has an # inherited __new__ unless a new __new__ is defined (or the resulting # class will fail). # # remove any keys listed in _ignore_ classdict . setdefault ( '_ignore_' , []) . append ( '_ignore_' ) ignore = classdict [ '_ignore_' ] for key in ignore : classdict . pop ( key , None ) member_type , first_enum = metacls . _get_mixins_ ( bases ) __new__ , save_new , use_args = metacls . _find_new_ ( classdict , member_type , first_enum ) # save enum items into separate mapping so they don't get baked into # the new class enum_members = { k : classdict [ k ] for k in classdict . _member_names } for name in classdict . _member_names : del classdict [ name ] # adjust the sunders _order_ = classdict . pop ( '_order_' , None ) # check for illegal enum names (any others?) invalid_names = set ( enum_members ) & { 'mro' , '' } if invalid_names : raise ValueError ( 'Invalid enum member name: {0} ' . format ( ',' . join ( invalid_names ))) # create a default docstring if one has not been provided if '__doc__' not in classdict : classdict [ '__doc__' ] = 'An enumeration.' # create our new Enum type enum_class = super () . __new__ ( metacls , cls , bases , classdict ) enum_class . _member_names_ = [] # names in definition order enum_class . _member_map_ = {} # name->value map enum_class . _member_type_ = member_type # save DynamicClassAttribute attributes from super classes so we know # if we can take the shortcut of storing members in the class dict dynamic_attributes = { k for c in enum_class . mro () for k , v in c . __dict__ . items () if isinstance ( v , DynamicClassAttribute )} # Reverse value->name map for hashable values. enum_class . _value2member_map_ = {} # If a custom type is mixed into the Enum, and it does not know how # to pickle itself, pickle.dumps will succeed but pickle.loads will # fail. Rather than have the error show up later and possibly far # from the source, sabotage the pickle protocol for this class so # that pickle.dumps also fails. # # However, if the new class implements its own __reduce_ex__, do not # sabotage -- it's on them to make sure it works correctly. We use # __reduce_ex__ instead of any of the others as it is preferred by # pickle over __reduce__, and it handles all pickle protocols. if '__reduce_ex__' not in classdict : if member_type is not object : methods = ( '__getnewargs_ex__' , '__getnewargs__' , '__reduce_ex__' , '__reduce__' ) if not any ( m in member_type . __dict__ for m in methods ): _make_class_unpicklable ( enum_class ) # instantiate them, checking for duplicates as we go # we instantiate first instead of checking for duplicates first in case # a custom __new__ is doing something funky with the values -- such as # auto-numbering ;) for member_name in classdict . _member_names : value = enum_members [ member_name ] if not isinstance ( value , tuple ): args = ( value , ) else : args = value if member_type is tuple : # special case for tuple enums args = ( args , ) # wrap it one more time if not use_args : enum_member = __new__ ( enum_class ) if not hasattr ( enum_member , '_value_' ): enum_member . _value_ = value else : enum_member = __new__ ( enum_class , * args ) if not hasattr ( enum_member , '_value_' ): if member_type is object : enum_member . _value_ = value else : enum_member . _value_ = member_type ( * args ) value = enum_member . _value_ enum_member . _name_ = member_name enum_member . __objclass__ = enum_class enum_member . __init__ ( * args ) # If another member with the same value was already defined, the # new member becomes an alias to the existing one. for name , canonical_member in enum_class . _member_map_ . items (): if canonical_member . _value_ == enum_member . _value_ : enum_member = canonical_member break else : # Aliases don't appear in member names (only in __members__). enum_class . _member_names_ . append ( member_name ) # performance boost for any member that would not shadow # a DynamicClassAttribute if member_name not in dynamic_attributes : setattr ( enum_class , member_name , enum_member ) # now add to _member_map_ enum_class . _member_map_ [ member_name ] = enum_member try : # This may fail if value is not hashable. We can't add the value # to the map, and by-value lookups for this value will be # linear. enum_class . _value2member_map_ [ value ] = enum_member except TypeError : pass # double check that repr and friends are not the mixin's or various # things break (such as pickle) for name in ( '__repr__' , '__str__' , '__format__' , '__reduce_ex__' ): class_method = getattr ( enum_class , name ) obj_method = getattr ( member_type , name , None ) enum_method = getattr ( first_enum , name , None ) if obj_method is not None and obj_method is class_method : setattr ( enum_class , name , enum_method ) # replace any other __new__ with our own (as long as Enum is not None, # anyway) -- again, this is to support pickle if Enum is not None : # if the user defined their own __new__, save it before it gets # clobbered in case they subclass later if save_new : enum_class . __new_member__ = __new__ enum_class . __new__ = Enum . __new__ # py3 support for definition order (helps keep py2/py3 code in sync) if _order_ is not None : if isinstance ( _order_ , str ): _order_ = _order_ . replace ( ',' , ' ' ) . split () if _order_ != enum_class . _member_names_ : raise TypeError ( 'member order does not match _order_' ) return enum_class","title":"__new__()"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__prepare__","text":"prepare () -> dict used to create the namespace for the class statement Source code in elegy/nn/embedding.py 119 120 121 122 123 124 125 126 127 @classmethod def __prepare__ ( metacls , cls , bases ): # create the namespace dict enum_dict = _EnumDict () # inherit previous flags and _generate_next_value_ function member_type , first_enum = metacls . _get_mixins_ ( bases ) if first_enum is not None : enum_dict [ '_generate_next_value_' ] = getattr ( first_enum , '_generate_next_value_' , None ) return enum_dict","title":"__prepare__()"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__setattr__","text":"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. Source code in elegy/nn/embedding.py 368 369 370 371 372 373 374 375 376 377 378 379 def __setattr__ ( cls , name , value ): \"\"\"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. \"\"\" member_map = cls . __dict__ . get ( '_member_map_' , {}) if name in member_map : raise AttributeError ( 'Cannot reassign members.' ) super () . __setattr__ ( name , value )","title":"__setattr__()"},{"location":"api/nn/Embedding/","text":"elegy.nn.Embedding Module for embedding tokens in a low-dimensional space. __init__ ( self , vocab_size = None , embed_dim = None , embedding_matrix = None , w_init = None , lookup_style = 'ARRAY_INDEX' , name = None ) special Constructs an Embed module. Parameters: Name Type Description Default vocab_size Optional[int] The number of unique tokens to embed. If not provided, an existing vocabulary matrix from which vocab_size can be inferred must be provided as existing_vocab . None embed_dim Optional[int] Number of dimensions to assign to each embedding. If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. None embedding_matrix Optional[jax._src.numpy.lax_numpy.ndarray] A matrix-like object equivalent in size to [vocab_size, embed_dim] . If given, it is used as the initial value for the embedding matrix and neither vocab_size or embed_dim need be given. If they are given, their values are checked to be consistent with the dimensions of embedding_matrix . None w_init Optional[elegy.types.Initializer] An initializer for the embeddings matrix. As a default, embeddings are initialized via a truncated normal distribution. None lookup_style Union[str, elegy.nn.embedding.EmbedLookupStyle] One of the enum values of :class: EmbedLookupStyle determining how to access the value of the embbeddings given an ID. Regardless the input should be a dense array of integer values representing ids. This setting changes how internally this module maps those ides to embeddings. The result is the same, but the speed and memory tradeoffs are different. It default to using numpy-style array indexing. This value is only the default for the module, and at any given invocation can be overriden in :meth: __call__ . 'ARRAY_INDEX' name Optional[str] Optional name for this module. None Exceptions: Type Description ValueError If none of embed_dim , embedding_matrix and vocab_size are supplied, or if embedding_matrix is supplied and embed_dim or vocab_size is not consistent with the supplied matrix. Source code in elegy/nn/embedding.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def __init__ ( self , vocab_size : Optional [ int ] = None , embed_dim : Optional [ int ] = None , embedding_matrix : Optional [ jnp . ndarray ] = None , w_init : Optional [ initializers . Initializer ] = None , lookup_style : Union [ str , EmbedLookupStyle ] = \"ARRAY_INDEX\" , name : Optional [ str ] = None , ): \"\"\" Constructs an Embed module. Args: vocab_size: The number of unique tokens to embed. If not provided, an existing vocabulary matrix from which ``vocab_size`` can be inferred must be provided as ``existing_vocab``. embed_dim: Number of dimensions to assign to each embedding. If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. embedding_matrix: A matrix-like object equivalent in size to ``[vocab_size, embed_dim]``. If given, it is used as the initial value for the embedding matrix and neither ``vocab_size`` or ``embed_dim`` need be given. If they are given, their values are checked to be consistent with the dimensions of ``embedding_matrix``. w_init: An initializer for the embeddings matrix. As a default, embeddings are initialized via a truncated normal distribution. lookup_style: One of the enum values of :class:`EmbedLookupStyle` determining how to access the value of the embbeddings given an ID. Regardless the input should be a dense array of integer values representing ids. This setting changes how internally this module maps those ides to embeddings. The result is the same, but the speed and memory tradeoffs are different. It default to using numpy-style array indexing. This value is only the default for the module, and at any given invocation can be overriden in :meth:`__call__`. name: Optional name for this module. Raises: ValueError: If none of ``embed_dim``, ``embedding_matrix`` and ``vocab_size`` are supplied, or if ``embedding_matrix`` is supplied and ``embed_dim`` or ``vocab_size`` is not consistent with the supplied matrix. \"\"\" super () . __init__ ( name = name ) if embedding_matrix is None and not ( vocab_size and embed_dim ): raise ValueError ( \"Embedding must be supplied either with an initial `embedding_matrix` \" \"or with `embed_dim` and `vocab_size`.\" ) if embedding_matrix is not None : embedding_matrix = jnp . asarray ( embedding_matrix ) if vocab_size and embedding_matrix . shape [ 0 ] != vocab_size : raise ValueError ( \"An `embedding_matrix` was supplied but the `vocab_size` of \" f \" { vocab_size } was not consistent with its shape \" f \" { embedding_matrix . shape } .\" ) if embed_dim and embedding_matrix . shape [ 1 ] != embed_dim : raise ValueError ( \"An `embedding_matrix` was supplied but the `embed_dim` of \" f \" { embed_dim } was not consistent with its shape \" f \" { embedding_matrix . shape } .\" ) self . embeddings = self . add_parameter ( \"embeddings\" , embedding_matrix . shape , initializer = lambda _ , __ : embedding_matrix , ) else : assert embed_dim is not None assert vocab_size is not None w_init = w_init or initializers . TruncatedNormal () self . embeddings = self . add_parameter ( \"embeddings\" , [ vocab_size , embed_dim ], initializer = w_init ) self . vocab_size = vocab_size or embedding_matrix . shape [ 0 ] self . embed_dim = embed_dim or embedding_matrix . shape [ 1 ] self . lookup_style = lookup_style add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/embedding.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value call ( self , ids , lookup_style = None ) Lookup embeddings. Looks up an embedding vector for each value in ids . All ids must be within [0, vocab_size) to prevent NaN \\ s from propagating. Parameters: Name Type Description Default ids ndarray integer array. required lookup_style Optional[Union[str, elegy.nn.embedding.EmbedLookupStyle]] Overrides the lookup_style given in the constructor. None Returns: Type Description ndarray Tensor of ids.shape + [embedding_dim] . Exceptions: Type Description AttributeError If lookup_style is not valid. ValueError If ids is not an integer array. Source code in elegy/nn/embedding.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def call ( self , ids : jnp . ndarray , lookup_style : Optional [ Union [ str , EmbedLookupStyle ]] = None , ) -> jnp . ndarray : r \"\"\" Lookup embeddings. Looks up an embedding vector for each value in ``ids``. All ids must be within ``[0, vocab_size)`` to prevent ``NaN``\\ s from propagating. Args: ids: integer array. lookup_style: Overrides the ``lookup_style`` given in the constructor. Returns: Tensor of ``ids.shape + [embedding_dim]``. Raises: AttributeError: If ``lookup_style`` is not valid. ValueError: If ``ids`` is not an integer array. \"\"\" # TODO(tomhennigan) Consider removing asarray here. ids = jnp . asarray ( ids ) if not jnp . issubdtype ( ids . dtype , jnp . integer ): raise ValueError ( \"Embedding's __call__ method must take an array of \" \"integer dtype but was called with an array of \" f \" { ids . dtype } \" ) lookup_style = lookup_style or self . lookup_style if isinstance ( lookup_style , str ): lookup_style = getattr ( EmbedLookupStyle , lookup_style . upper ()) if lookup_style == EmbedLookupStyle . ARRAY_INDEX : return self . embeddings [( ids ,)] elif lookup_style == EmbedLookupStyle . ONE_HOT : one_hot_ids = jax . nn . one_hot ( ids , self . vocab_size )[ ... , None ] return ( self . embeddings * one_hot_ids ) . sum ( axis =- 2 ) else : raise NotImplementedError ( f \" { lookup_style } is not supported by Embedding.\" ) get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/embedding.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/embedding.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/embedding.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/embedding.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"Embedding"},{"location":"api/nn/Embedding/#elegynnembedding","text":"","title":"elegy.nn.Embedding"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding","text":"Module for embedding tokens in a low-dimensional space.","title":"elegy.nn.embedding.Embedding"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.__init__","text":"Constructs an Embed module. Parameters: Name Type Description Default vocab_size Optional[int] The number of unique tokens to embed. If not provided, an existing vocabulary matrix from which vocab_size can be inferred must be provided as existing_vocab . None embed_dim Optional[int] Number of dimensions to assign to each embedding. If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. None embedding_matrix Optional[jax._src.numpy.lax_numpy.ndarray] A matrix-like object equivalent in size to [vocab_size, embed_dim] . If given, it is used as the initial value for the embedding matrix and neither vocab_size or embed_dim need be given. If they are given, their values are checked to be consistent with the dimensions of embedding_matrix . None w_init Optional[elegy.types.Initializer] An initializer for the embeddings matrix. As a default, embeddings are initialized via a truncated normal distribution. None lookup_style Union[str, elegy.nn.embedding.EmbedLookupStyle] One of the enum values of :class: EmbedLookupStyle determining how to access the value of the embbeddings given an ID. Regardless the input should be a dense array of integer values representing ids. This setting changes how internally this module maps those ides to embeddings. The result is the same, but the speed and memory tradeoffs are different. It default to using numpy-style array indexing. This value is only the default for the module, and at any given invocation can be overriden in :meth: __call__ . 'ARRAY_INDEX' name Optional[str] Optional name for this module. None Exceptions: Type Description ValueError If none of embed_dim , embedding_matrix and vocab_size are supplied, or if embedding_matrix is supplied and embed_dim or vocab_size is not consistent with the supplied matrix. Source code in elegy/nn/embedding.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def __init__ ( self , vocab_size : Optional [ int ] = None , embed_dim : Optional [ int ] = None , embedding_matrix : Optional [ jnp . ndarray ] = None , w_init : Optional [ initializers . Initializer ] = None , lookup_style : Union [ str , EmbedLookupStyle ] = \"ARRAY_INDEX\" , name : Optional [ str ] = None , ): \"\"\" Constructs an Embed module. Args: vocab_size: The number of unique tokens to embed. If not provided, an existing vocabulary matrix from which ``vocab_size`` can be inferred must be provided as ``existing_vocab``. embed_dim: Number of dimensions to assign to each embedding. If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. embedding_matrix: A matrix-like object equivalent in size to ``[vocab_size, embed_dim]``. If given, it is used as the initial value for the embedding matrix and neither ``vocab_size`` or ``embed_dim`` need be given. If they are given, their values are checked to be consistent with the dimensions of ``embedding_matrix``. w_init: An initializer for the embeddings matrix. As a default, embeddings are initialized via a truncated normal distribution. lookup_style: One of the enum values of :class:`EmbedLookupStyle` determining how to access the value of the embbeddings given an ID. Regardless the input should be a dense array of integer values representing ids. This setting changes how internally this module maps those ides to embeddings. The result is the same, but the speed and memory tradeoffs are different. It default to using numpy-style array indexing. This value is only the default for the module, and at any given invocation can be overriden in :meth:`__call__`. name: Optional name for this module. Raises: ValueError: If none of ``embed_dim``, ``embedding_matrix`` and ``vocab_size`` are supplied, or if ``embedding_matrix`` is supplied and ``embed_dim`` or ``vocab_size`` is not consistent with the supplied matrix. \"\"\" super () . __init__ ( name = name ) if embedding_matrix is None and not ( vocab_size and embed_dim ): raise ValueError ( \"Embedding must be supplied either with an initial `embedding_matrix` \" \"or with `embed_dim` and `vocab_size`.\" ) if embedding_matrix is not None : embedding_matrix = jnp . asarray ( embedding_matrix ) if vocab_size and embedding_matrix . shape [ 0 ] != vocab_size : raise ValueError ( \"An `embedding_matrix` was supplied but the `vocab_size` of \" f \" { vocab_size } was not consistent with its shape \" f \" { embedding_matrix . shape } .\" ) if embed_dim and embedding_matrix . shape [ 1 ] != embed_dim : raise ValueError ( \"An `embedding_matrix` was supplied but the `embed_dim` of \" f \" { embed_dim } was not consistent with its shape \" f \" { embedding_matrix . shape } .\" ) self . embeddings = self . add_parameter ( \"embeddings\" , embedding_matrix . shape , initializer = lambda _ , __ : embedding_matrix , ) else : assert embed_dim is not None assert vocab_size is not None w_init = w_init or initializers . TruncatedNormal () self . embeddings = self . add_parameter ( \"embeddings\" , [ vocab_size , embed_dim ], initializer = w_init ) self . vocab_size = vocab_size or embedding_matrix . shape [ 0 ] self . embed_dim = embed_dim or embedding_matrix . shape [ 1 ] self . lookup_style = lookup_style","title":"__init__()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/embedding.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.call","text":"Lookup embeddings. Looks up an embedding vector for each value in ids . All ids must be within [0, vocab_size) to prevent NaN \\ s from propagating. Parameters: Name Type Description Default ids ndarray integer array. required lookup_style Optional[Union[str, elegy.nn.embedding.EmbedLookupStyle]] Overrides the lookup_style given in the constructor. None Returns: Type Description ndarray Tensor of ids.shape + [embedding_dim] . Exceptions: Type Description AttributeError If lookup_style is not valid. ValueError If ids is not an integer array. Source code in elegy/nn/embedding.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def call ( self , ids : jnp . ndarray , lookup_style : Optional [ Union [ str , EmbedLookupStyle ]] = None , ) -> jnp . ndarray : r \"\"\" Lookup embeddings. Looks up an embedding vector for each value in ``ids``. All ids must be within ``[0, vocab_size)`` to prevent ``NaN``\\ s from propagating. Args: ids: integer array. lookup_style: Overrides the ``lookup_style`` given in the constructor. Returns: Tensor of ``ids.shape + [embedding_dim]``. Raises: AttributeError: If ``lookup_style`` is not valid. ValueError: If ``ids`` is not an integer array. \"\"\" # TODO(tomhennigan) Consider removing asarray here. ids = jnp . asarray ( ids ) if not jnp . issubdtype ( ids . dtype , jnp . integer ): raise ValueError ( \"Embedding's __call__ method must take an array of \" \"integer dtype but was called with an array of \" f \" { ids . dtype } \" ) lookup_style = lookup_style or self . lookup_style if isinstance ( lookup_style , str ): lookup_style = getattr ( EmbedLookupStyle , lookup_style . upper ()) if lookup_style == EmbedLookupStyle . ARRAY_INDEX : return self . embeddings [( ids ,)] elif lookup_style == EmbedLookupStyle . ONE_HOT : one_hot_ids = jax . nn . one_hot ( ids , self . vocab_size )[ ... , None ] return ( self . embeddings * one_hot_ids ) . sum ( axis =- 2 ) else : raise NotImplementedError ( f \" { lookup_style } is not supported by Embedding.\" )","title":"call()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/embedding.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.init","text":"Initializes the module, Source code in elegy/nn/embedding.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/embedding.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/embedding.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/Flatten/","text":"elegy.nn.Flatten Flattens the input, preserving the batch dimension(s). By default, Flatten combines all dimensions except the first. Additional leading dimensions can be preserved by setting preserve_dims. x = jnp . ones ([ 3 , 2 , 4 ]) flat = elegy . nn . Flatten () assert flat ( x ) . shape == ( 3 , 8 ) When the input to flatten has fewer than preserve_dims dimensions it is returned unchanged: x = jnp . ones ([ 3 ]) assert flat ( x ) . shape == ( 3 ,) add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/flatten.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value call ( self , inputs ) inherited Parameters: Name Type Description Default inputs ndarray the array to be reshaped. required Returns: Type Description ndarray A reshaped array. Source code in elegy/nn/flatten.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Arguments: inputs: the array to be reshaped. Returns: A reshaped array. \"\"\" if inputs . ndim <= self . preserve_dims : return inputs if - 1 in self . output_shape : reshaped_shape = _infer_shape ( self . output_shape , inputs . shape [ self . preserve_dims :] ) else : reshaped_shape = self . output_shape shape = inputs . shape [: self . preserve_dims ] + reshaped_shape return jnp . reshape ( inputs , shape ) get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/flatten.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/flatten.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/flatten.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/flatten.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"Flatten"},{"location":"api/nn/Flatten/#elegynnflatten","text":"","title":"elegy.nn.Flatten"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten","text":"Flattens the input, preserving the batch dimension(s). By default, Flatten combines all dimensions except the first. Additional leading dimensions can be preserved by setting preserve_dims. x = jnp . ones ([ 3 , 2 , 4 ]) flat = elegy . nn . Flatten () assert flat ( x ) . shape == ( 3 , 8 ) When the input to flatten has fewer than preserve_dims dimensions it is returned unchanged: x = jnp . ones ([ 3 ]) assert flat ( x ) . shape == ( 3 ,)","title":"elegy.nn.flatten.Flatten"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/flatten.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.call","text":"Parameters: Name Type Description Default inputs ndarray the array to be reshaped. required Returns: Type Description ndarray A reshaped array. Source code in elegy/nn/flatten.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Arguments: inputs: the array to be reshaped. Returns: A reshaped array. \"\"\" if inputs . ndim <= self . preserve_dims : return inputs if - 1 in self . output_shape : reshaped_shape = _infer_shape ( self . output_shape , inputs . shape [ self . preserve_dims :] ) else : reshaped_shape = self . output_shape shape = inputs . shape [: self . preserve_dims ] + reshaped_shape return jnp . reshape ( inputs , shape )","title":"call()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/flatten.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.init","text":"Initializes the module, Source code in elegy/nn/flatten.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/flatten.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/flatten.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/InstanceNormalization/","text":"elegy.nn.InstanceNormalization Normalizes inputs along the spatial dimensions. See LayerNorm for more details. __init__ ( self , create_scale = True , create_offset = True , eps = 1e-05 , scale_init = None , offset_init = None , data_format = 'channels_last' , ** kwargs ) special Constructs an InstanceNormalization module. This method creates a module which normalizes over the spatial dimensions. Parameters: Name Type Description Default create_scale bool bool representing whether to create a trainable scale per channel applied after the normalization. True create_offset bool bool representing whether to create a trainable offset per channel applied after normalization and scaling. True eps float Small epsilon to avoid division by zero variance. Defaults to 1e-5 . 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for the scale variable. Can only be set if create_scale=True . By default scale is initialized to 1 . None offset_init Optional[elegy.types.Initializer] Optional initializer for the offset variable. Can only be set if create_offset=True . By default offset is initialized to 0 . None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default it is channels_last . 'channels_last' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/layer_normalization.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , create_scale : bool = True , create_offset : bool = True , eps : float = 1e-5 , scale_init : Optional [ initializers . Initializer ] = None , offset_init : Optional [ initializers . Initializer ] = None , data_format : str = \"channels_last\" , ** kwargs ): \"\"\"Constructs an `InstanceNormalization` module. This method creates a module which normalizes over the spatial dimensions. Args: create_scale: ``bool`` representing whether to create a trainable scale per channel applied after the normalization. create_offset: ``bool`` representing whether to create a trainable offset per channel applied after normalization and scaling. eps: Small epsilon to avoid division by zero variance. Defaults to ``1e-5``. scale_init: Optional initializer for the scale variable. Can only be set if ``create_scale=True``. By default scale is initialized to ``1``. offset_init: Optional initializer for the offset variable. Can only be set if ``create_offset=True``. By default offset is initialized to ``0``. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default it is ``channels_last``. kwargs: Additional keyword arguments passed to Module. \"\"\" if hk_utils . get_channel_index ( data_format ) == 1 : axis = slice ( 2 , None ) else : # channel_index = -1 axis = slice ( 1 , - 1 ) super () . __init__ ( axis = axis , create_scale = create_scale , create_offset = create_offset , eps = eps , scale_init = scale_init , offset_init = offset_init , ** kwargs ) add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/layer_normalization.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value call ( self , inputs , scale = None , offset = None ) inherited Connects the layer norm. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [N, ..., C] . required scale Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized. Source code in elegy/nn/layer_normalization.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def call ( self , inputs : jnp . ndarray , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Connects the layer norm. Args: inputs: An array, where the data format is ``[N, ..., C]``. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized. \"\"\" if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) axis = self . axis if isinstance ( axis , slice ): axis = tuple ( range ( inputs . ndim )[ axis ]) mean = jnp . mean ( inputs , axis = axis , keepdims = True ) variance = jnp . var ( inputs , axis = axis , keepdims = True ) param_shape = inputs . shape [ - 1 :] if self . create_scale : scale = self . add_parameter ( \"scale\" , param_shape , jnp . float32 , initializer = self . scale_init ) elif scale is None : scale = np . array ( 1.0 , dtype = inputs . dtype ) if self . create_offset : offset = self . add_parameter ( \"offset\" , param_shape , jnp . float32 , initializer = self . offset_init ) elif offset is None : offset = np . array ( 0.0 , dtype = inputs . dtype ) scale = jnp . broadcast_to ( scale , inputs . shape ) offset = jnp . broadcast_to ( offset , inputs . shape ) mean = jnp . broadcast_to ( mean , inputs . shape ) inv = scale * jax . lax . rsqrt ( variance + self . eps ) return inv * ( inputs - mean ) + offset get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/layer_normalization.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/layer_normalization.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/layer_normalization.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/layer_normalization.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"InstanceNormalization"},{"location":"api/nn/InstanceNormalization/#elegynninstancenormalization","text":"","title":"elegy.nn.InstanceNormalization"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization","text":"Normalizes inputs along the spatial dimensions. See LayerNorm for more details.","title":"elegy.nn.layer_normalization.InstanceNormalization"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.__init__","text":"Constructs an InstanceNormalization module. This method creates a module which normalizes over the spatial dimensions. Parameters: Name Type Description Default create_scale bool bool representing whether to create a trainable scale per channel applied after the normalization. True create_offset bool bool representing whether to create a trainable offset per channel applied after normalization and scaling. True eps float Small epsilon to avoid division by zero variance. Defaults to 1e-5 . 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for the scale variable. Can only be set if create_scale=True . By default scale is initialized to 1 . None offset_init Optional[elegy.types.Initializer] Optional initializer for the offset variable. Can only be set if create_offset=True . By default offset is initialized to 0 . None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default it is channels_last . 'channels_last' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/layer_normalization.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , create_scale : bool = True , create_offset : bool = True , eps : float = 1e-5 , scale_init : Optional [ initializers . Initializer ] = None , offset_init : Optional [ initializers . Initializer ] = None , data_format : str = \"channels_last\" , ** kwargs ): \"\"\"Constructs an `InstanceNormalization` module. This method creates a module which normalizes over the spatial dimensions. Args: create_scale: ``bool`` representing whether to create a trainable scale per channel applied after the normalization. create_offset: ``bool`` representing whether to create a trainable offset per channel applied after normalization and scaling. eps: Small epsilon to avoid division by zero variance. Defaults to ``1e-5``. scale_init: Optional initializer for the scale variable. Can only be set if ``create_scale=True``. By default scale is initialized to ``1``. offset_init: Optional initializer for the offset variable. Can only be set if ``create_offset=True``. By default offset is initialized to ``0``. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default it is ``channels_last``. kwargs: Additional keyword arguments passed to Module. \"\"\" if hk_utils . get_channel_index ( data_format ) == 1 : axis = slice ( 2 , None ) else : # channel_index = -1 axis = slice ( 1 , - 1 ) super () . __init__ ( axis = axis , create_scale = create_scale , create_offset = create_offset , eps = eps , scale_init = scale_init , offset_init = offset_init , ** kwargs )","title":"__init__()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/layer_normalization.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.call","text":"Connects the layer norm. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [N, ..., C] . required scale Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized. Source code in elegy/nn/layer_normalization.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def call ( self , inputs : jnp . ndarray , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Connects the layer norm. Args: inputs: An array, where the data format is ``[N, ..., C]``. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized. \"\"\" if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) axis = self . axis if isinstance ( axis , slice ): axis = tuple ( range ( inputs . ndim )[ axis ]) mean = jnp . mean ( inputs , axis = axis , keepdims = True ) variance = jnp . var ( inputs , axis = axis , keepdims = True ) param_shape = inputs . shape [ - 1 :] if self . create_scale : scale = self . add_parameter ( \"scale\" , param_shape , jnp . float32 , initializer = self . scale_init ) elif scale is None : scale = np . array ( 1.0 , dtype = inputs . dtype ) if self . create_offset : offset = self . add_parameter ( \"offset\" , param_shape , jnp . float32 , initializer = self . offset_init ) elif offset is None : offset = np . array ( 0.0 , dtype = inputs . dtype ) scale = jnp . broadcast_to ( scale , inputs . shape ) offset = jnp . broadcast_to ( offset , inputs . shape ) mean = jnp . broadcast_to ( mean , inputs . shape ) inv = scale * jax . lax . rsqrt ( variance + self . eps ) return inv * ( inputs - mean ) + offset","title":"call()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/layer_normalization.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.init","text":"Initializes the module, Source code in elegy/nn/layer_normalization.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/layer_normalization.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/layer_normalization.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/LayerNormalization/","text":"elegy.nn.LayerNormalization LayerNorm module. See: https://arxiv.org/abs/1607.06450. __init__ ( self , axis =- 1 , create_scale = True , create_offset = True , eps = 1e-05 , scale_init = None , offset_init = None , ** kwargs ) special Constructs a LayerNorm module. Parameters: Name Type Description Default axis Union[int, Sequence[int], slice] Integer, list of integers, or slice indicating which axes to normalize over. -1 create_scale bool Bool, defines whether to create a trainable scale per channel applied after the normalization. True create_offset bool Bool, defines whether to create a trainable offset per channel applied after normalization and scaling. True eps float Small epsilon to avoid division by zero variance. Defaults 1e-5 , as in the paper and Sonnet. 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for gain (aka scale). By default, one. None offset_init Optional[elegy.types.Initializer] Optional initializer for bias (aka offset). By default, zero. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/layer_normalization.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , axis : Union [ int , Sequence [ int ], slice ] = - 1 , create_scale : bool = True , create_offset : bool = True , eps : float = 1e-5 , scale_init : Optional [ initializers . Initializer ] = None , offset_init : Optional [ initializers . Initializer ] = None , ** kwargs ): \"\"\"Constructs a LayerNorm module. Args: axis: Integer, list of integers, or slice indicating which axes to normalize over. create_scale: Bool, defines whether to create a trainable scale per channel applied after the normalization. create_offset: Bool, defines whether to create a trainable offset per channel applied after normalization and scaling. eps: Small epsilon to avoid division by zero variance. Defaults ``1e-5``, as in the paper and Sonnet. scale_init: Optional initializer for gain (aka scale). By default, one. offset_init: Optional initializer for bias (aka offset). By default, zero. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if not create_scale and scale_init is not None : raise ValueError ( \"Cannot set `scale_init` if `create_scale=False`.\" ) if not create_offset and offset_init is not None : raise ValueError ( \"Cannot set `offset_init` if `create_offset=False`.\" ) if isinstance ( axis , slice ): self . axis = axis elif isinstance ( axis , int ): self . axis = ( axis ,) elif isinstance ( axis , collections . Iterable ) and all ( isinstance ( ax , int ) for ax in axis ): self . axis = tuple ( axis ) else : raise ValueError ( \"`axis` should be an int, slice or iterable of ints.\" ) self . eps = eps self . create_scale = create_scale self . create_offset = create_offset self . scale_init = scale_init or jnp . ones self . offset_init = offset_init or jnp . zeros add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/layer_normalization.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value call ( self , inputs , scale = None , offset = None ) Connects the layer norm. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [N, ..., C] . required scale Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized. Source code in elegy/nn/layer_normalization.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def call ( self , inputs : jnp . ndarray , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Connects the layer norm. Args: inputs: An array, where the data format is ``[N, ..., C]``. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized. \"\"\" if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) axis = self . axis if isinstance ( axis , slice ): axis = tuple ( range ( inputs . ndim )[ axis ]) mean = jnp . mean ( inputs , axis = axis , keepdims = True ) variance = jnp . var ( inputs , axis = axis , keepdims = True ) param_shape = inputs . shape [ - 1 :] if self . create_scale : scale = self . add_parameter ( \"scale\" , param_shape , jnp . float32 , initializer = self . scale_init ) elif scale is None : scale = np . array ( 1.0 , dtype = inputs . dtype ) if self . create_offset : offset = self . add_parameter ( \"offset\" , param_shape , jnp . float32 , initializer = self . offset_init ) elif offset is None : offset = np . array ( 0.0 , dtype = inputs . dtype ) scale = jnp . broadcast_to ( scale , inputs . shape ) offset = jnp . broadcast_to ( offset , inputs . shape ) mean = jnp . broadcast_to ( mean , inputs . shape ) inv = scale * jax . lax . rsqrt ( variance + self . eps ) return inv * ( inputs - mean ) + offset get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/layer_normalization.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/layer_normalization.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/layer_normalization.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/layer_normalization.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"LayerNormalization"},{"location":"api/nn/LayerNormalization/#elegynnlayernormalization","text":"","title":"elegy.nn.LayerNormalization"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization","text":"LayerNorm module. See: https://arxiv.org/abs/1607.06450.","title":"elegy.nn.layer_normalization.LayerNormalization"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.__init__","text":"Constructs a LayerNorm module. Parameters: Name Type Description Default axis Union[int, Sequence[int], slice] Integer, list of integers, or slice indicating which axes to normalize over. -1 create_scale bool Bool, defines whether to create a trainable scale per channel applied after the normalization. True create_offset bool Bool, defines whether to create a trainable offset per channel applied after normalization and scaling. True eps float Small epsilon to avoid division by zero variance. Defaults 1e-5 , as in the paper and Sonnet. 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for gain (aka scale). By default, one. None offset_init Optional[elegy.types.Initializer] Optional initializer for bias (aka offset). By default, zero. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/layer_normalization.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , axis : Union [ int , Sequence [ int ], slice ] = - 1 , create_scale : bool = True , create_offset : bool = True , eps : float = 1e-5 , scale_init : Optional [ initializers . Initializer ] = None , offset_init : Optional [ initializers . Initializer ] = None , ** kwargs ): \"\"\"Constructs a LayerNorm module. Args: axis: Integer, list of integers, or slice indicating which axes to normalize over. create_scale: Bool, defines whether to create a trainable scale per channel applied after the normalization. create_offset: Bool, defines whether to create a trainable offset per channel applied after normalization and scaling. eps: Small epsilon to avoid division by zero variance. Defaults ``1e-5``, as in the paper and Sonnet. scale_init: Optional initializer for gain (aka scale). By default, one. offset_init: Optional initializer for bias (aka offset). By default, zero. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if not create_scale and scale_init is not None : raise ValueError ( \"Cannot set `scale_init` if `create_scale=False`.\" ) if not create_offset and offset_init is not None : raise ValueError ( \"Cannot set `offset_init` if `create_offset=False`.\" ) if isinstance ( axis , slice ): self . axis = axis elif isinstance ( axis , int ): self . axis = ( axis ,) elif isinstance ( axis , collections . Iterable ) and all ( isinstance ( ax , int ) for ax in axis ): self . axis = tuple ( axis ) else : raise ValueError ( \"`axis` should be an int, slice or iterable of ints.\" ) self . eps = eps self . create_scale = create_scale self . create_offset = create_offset self . scale_init = scale_init or jnp . ones self . offset_init = offset_init or jnp . zeros","title":"__init__()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/layer_normalization.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.call","text":"Connects the layer norm. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [N, ..., C] . required scale Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized. Source code in elegy/nn/layer_normalization.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def call ( self , inputs : jnp . ndarray , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Connects the layer norm. Args: inputs: An array, where the data format is ``[N, ..., C]``. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized. \"\"\" if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) axis = self . axis if isinstance ( axis , slice ): axis = tuple ( range ( inputs . ndim )[ axis ]) mean = jnp . mean ( inputs , axis = axis , keepdims = True ) variance = jnp . var ( inputs , axis = axis , keepdims = True ) param_shape = inputs . shape [ - 1 :] if self . create_scale : scale = self . add_parameter ( \"scale\" , param_shape , jnp . float32 , initializer = self . scale_init ) elif scale is None : scale = np . array ( 1.0 , dtype = inputs . dtype ) if self . create_offset : offset = self . add_parameter ( \"offset\" , param_shape , jnp . float32 , initializer = self . offset_init ) elif offset is None : offset = np . array ( 0.0 , dtype = inputs . dtype ) scale = jnp . broadcast_to ( scale , inputs . shape ) offset = jnp . broadcast_to ( offset , inputs . shape ) mean = jnp . broadcast_to ( mean , inputs . shape ) inv = scale * jax . lax . rsqrt ( variance + self . eps ) return inv * ( inputs - mean ) + offset","title":"call()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/layer_normalization.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.init","text":"Initializes the module, Source code in elegy/nn/layer_normalization.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/layer_normalization.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/layer_normalization.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/Linear/","text":"elegy.nn.Linear Linear module. __init__ ( self , output_size , with_bias = True , w_init = None , b_init = None , ** kwargs ) special Constructs the Linear module. Parameters: Name Type Description Default output_size int Output dimensionality. required with_bias bool Whether to add a bias to the output. True w_init Optional[elegy.types.Initializer] Optional initializer for weights. By default, uses random values from truncated normal, with stddev 1 / sqrt(fan_in) . See https://arxiv.org/abs/1502.03167v3. None b_init Optional[elegy.types.Initializer] Optional initializer for bias. By default, zero. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/linear.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , output_size : int , with_bias : bool = True , w_init : tp . Optional [ Initializer ] = None , b_init : tp . Optional [ Initializer ] = None , ** kwargs ): \"\"\" Constructs the Linear module. Arguments: output_size: Output dimensionality. with_bias: Whether to add a bias to the output. w_init: Optional initializer for weights. By default, uses random values from truncated normal, with stddev `1 / sqrt(fan_in)`. See https://arxiv.org/abs/1502.03167v3. b_init: Optional initializer for bias. By default, zero. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) self . input_size = None self . output_size = output_size self . with_bias = with_bias self . w_init = w_init self . b_init = b_init or jnp . zeros add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/linear.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/linear.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/linear.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/linear.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/linear.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"Linear"},{"location":"api/nn/Linear/#elegynnlinear","text":"","title":"elegy.nn.Linear"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear","text":"Linear module.","title":"elegy.nn.linear.Linear"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.__init__","text":"Constructs the Linear module. Parameters: Name Type Description Default output_size int Output dimensionality. required with_bias bool Whether to add a bias to the output. True w_init Optional[elegy.types.Initializer] Optional initializer for weights. By default, uses random values from truncated normal, with stddev 1 / sqrt(fan_in) . See https://arxiv.org/abs/1502.03167v3. None b_init Optional[elegy.types.Initializer] Optional initializer for bias. By default, zero. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/linear.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , output_size : int , with_bias : bool = True , w_init : tp . Optional [ Initializer ] = None , b_init : tp . Optional [ Initializer ] = None , ** kwargs ): \"\"\" Constructs the Linear module. Arguments: output_size: Output dimensionality. with_bias: Whether to add a bias to the output. w_init: Optional initializer for weights. By default, uses random values from truncated normal, with stddev `1 / sqrt(fan_in)`. See https://arxiv.org/abs/1502.03167v3. b_init: Optional initializer for bias. By default, zero. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) self . input_size = None self . output_size = output_size self . with_bias = with_bias self . w_init = w_init self . b_init = b_init or jnp . zeros","title":"__init__()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/linear.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/linear.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.init","text":"Initializes the module, Source code in elegy/nn/linear.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/linear.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/linear.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/MaxPool/","text":"elegy.nn.MaxPool Max pool. Equivalent to partial application of :func: max_pool . __init__ ( self , window_shape , strides , padding , channel_axis =- 1 , name = None ) special Max pool. Parameters: Name Type Description Default window_shape Union[int, Sequence[int]] Shape of window to pool over. Same rank as value or int . required strides Union[int, Sequence[int]] Strides for the window. Same rank as value or int . required padding str Padding algorithm. Either VALID or SAME . required channel_axis Optional[int] Axis of the spatial channels for which pooling is skipped. -1 Source code in elegy/nn/pool.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def __init__ ( self , window_shape : Union [ int , Sequence [ int ]], strides : Union [ int , Sequence [ int ]], padding : str , channel_axis : Optional [ int ] = - 1 , name : Optional [ str ] = None , ): \"\"\"Max pool. Args: window_shape: Shape of window to pool over. Same rank as value or ``int``. strides: Strides for the window. Same rank as value or ``int``. padding: Padding algorithm. Either ``VALID`` or ``SAME``. channel_axis: Axis of the spatial channels for which pooling is skipped. \"\"\" super () . __init__ ( name = name ) self . window_shape = window_shape self . strides = strides self . padding = padding self . channel_axis = channel_axis add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/pool.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/pool.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/pool.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/pool.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/pool.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"MaxPool"},{"location":"api/nn/MaxPool/#elegynnmaxpool","text":"","title":"elegy.nn.MaxPool"},{"location":"api/nn/MaxPool/#elegy.nn.pool.MaxPool","text":"Max pool. Equivalent to partial application of :func: max_pool .","title":"elegy.nn.pool.MaxPool"},{"location":"api/nn/MaxPool/#elegy.nn.pool.MaxPool.__init__","text":"Max pool. Parameters: Name Type Description Default window_shape Union[int, Sequence[int]] Shape of window to pool over. Same rank as value or int . required strides Union[int, Sequence[int]] Strides for the window. Same rank as value or int . required padding str Padding algorithm. Either VALID or SAME . required channel_axis Optional[int] Axis of the spatial channels for which pooling is skipped. -1 Source code in elegy/nn/pool.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def __init__ ( self , window_shape : Union [ int , Sequence [ int ]], strides : Union [ int , Sequence [ int ]], padding : str , channel_axis : Optional [ int ] = - 1 , name : Optional [ str ] = None , ): \"\"\"Max pool. Args: window_shape: Shape of window to pool over. Same rank as value or ``int``. strides: Strides for the window. Same rank as value or ``int``. padding: Padding algorithm. Either ``VALID`` or ``SAME``. channel_axis: Axis of the spatial channels for which pooling is skipped. \"\"\" super () . __init__ ( name = name ) self . window_shape = window_shape self . strides = strides self . padding = padding self . channel_axis = channel_axis","title":"__init__()"},{"location":"api/nn/MaxPool/#elegy.nn.pool.MaxPool.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/pool.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/MaxPool/#elegy.nn.pool.MaxPool.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/pool.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/MaxPool/#elegy.nn.pool.MaxPool.init","text":"Initializes the module, Source code in elegy/nn/pool.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/MaxPool/#elegy.nn.pool.MaxPool.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/pool.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/MaxPool/#elegy.nn.pool.MaxPool.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/pool.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/Reshape/","text":"elegy.nn.Reshape Reshapes input Tensor, preserving the batch dimension. For example, given an input tensor with shape [B, H, W, C, D] : B , H , W , C , D = range ( 1 , 6 ) x = jnp . ones ([ B , H , W , C , D ]) The default behavior when output_shape is (-1, D) is to flatten all dimensions between B and D : mod = elegy . nn . Reshape ( output_shape = ( - 1 , D )) assert mod ( x ) . shape == ( B , H * W * C , D ) You can change the number of preserved leading dimensions via preserve_dims : mod = elegy . nn . Reshape ( output_shape = ( - 1 , D ), preserve_dims = 2 ) assert mod ( x ) . shape == ( B , H , W * C , D ) mod = elegy . nn . Reshape ( output_shape = ( - 1 , D ), preserve_dims = 3 ) assert mod ( x ) . shape == ( B , H , W , C , D ) mod = elegy . nn . Reshape ( output_shape = ( - 1 , D ), preserve_dims = 4 ) assert mod ( x ) . shape == ( B , H , W , C , 1 , D ) __init__ ( self , output_shape , preserve_dims = 1 , ** kwargs ) special Constructs a Reshape module. Parameters: Name Type Description Default output_shape Sequence[int] Shape to reshape the input tensor to while preserving its first preserve_dims dimensions. When the special value -1 appears in output_shape the corresponding size is automatically inferred. Note that -1 can only appear once in output_shape . To flatten all non-batch dimensions use Flatten . required preserve_dims int Number of leading dimensions that will not be reshaped. 1 kwargs Additional keyword arguments passed to Module. {} Exceptions: Type Description ValueError If preserve_dims is not positive. Source code in elegy/nn/flatten.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , output_shape : types . Shape , preserve_dims : int = 1 , ** kwargs ): \"\"\" Constructs a `Reshape` module. Args: output_shape: Shape to reshape the input tensor to while preserving its first `preserve_dims` dimensions. When the special value -1 appears in `output_shape` the corresponding size is automatically inferred. Note that -1 can only appear once in `output_shape`. To flatten all non-batch dimensions use `Flatten`. preserve_dims: Number of leading dimensions that will not be reshaped. kwargs: Additional keyword arguments passed to Module. Raises: ValueError: If `preserve_dims` is not positive. \"\"\" super () . __init__ ( ** kwargs ) if preserve_dims <= 0 : raise ValueError ( \"Argument preserve_dims should be >= 1.\" ) if output_shape . count ( - 1 ) > 1 : raise ValueError ( \"-1 can only occur once in `output_shape`.\" ) self . output_shape = tuple ( output_shape ) self . preserve_dims = preserve_dims add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/flatten.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value call ( self , inputs ) Parameters: Name Type Description Default inputs ndarray the array to be reshaped. required Returns: Type Description ndarray A reshaped array. Source code in elegy/nn/flatten.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Arguments: inputs: the array to be reshaped. Returns: A reshaped array. \"\"\" if inputs . ndim <= self . preserve_dims : return inputs if - 1 in self . output_shape : reshaped_shape = _infer_shape ( self . output_shape , inputs . shape [ self . preserve_dims :] ) else : reshaped_shape = self . output_shape shape = inputs . shape [: self . preserve_dims ] + reshaped_shape return jnp . reshape ( inputs , shape ) get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/flatten.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/flatten.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/flatten.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/flatten.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"Reshape"},{"location":"api/nn/Reshape/#elegynnreshape","text":"","title":"elegy.nn.Reshape"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape","text":"Reshapes input Tensor, preserving the batch dimension. For example, given an input tensor with shape [B, H, W, C, D] : B , H , W , C , D = range ( 1 , 6 ) x = jnp . ones ([ B , H , W , C , D ]) The default behavior when output_shape is (-1, D) is to flatten all dimensions between B and D : mod = elegy . nn . Reshape ( output_shape = ( - 1 , D )) assert mod ( x ) . shape == ( B , H * W * C , D ) You can change the number of preserved leading dimensions via preserve_dims : mod = elegy . nn . Reshape ( output_shape = ( - 1 , D ), preserve_dims = 2 ) assert mod ( x ) . shape == ( B , H , W * C , D ) mod = elegy . nn . Reshape ( output_shape = ( - 1 , D ), preserve_dims = 3 ) assert mod ( x ) . shape == ( B , H , W , C , D ) mod = elegy . nn . Reshape ( output_shape = ( - 1 , D ), preserve_dims = 4 ) assert mod ( x ) . shape == ( B , H , W , C , 1 , D )","title":"elegy.nn.flatten.Reshape"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.__init__","text":"Constructs a Reshape module. Parameters: Name Type Description Default output_shape Sequence[int] Shape to reshape the input tensor to while preserving its first preserve_dims dimensions. When the special value -1 appears in output_shape the corresponding size is automatically inferred. Note that -1 can only appear once in output_shape . To flatten all non-batch dimensions use Flatten . required preserve_dims int Number of leading dimensions that will not be reshaped. 1 kwargs Additional keyword arguments passed to Module. {} Exceptions: Type Description ValueError If preserve_dims is not positive. Source code in elegy/nn/flatten.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , output_shape : types . Shape , preserve_dims : int = 1 , ** kwargs ): \"\"\" Constructs a `Reshape` module. Args: output_shape: Shape to reshape the input tensor to while preserving its first `preserve_dims` dimensions. When the special value -1 appears in `output_shape` the corresponding size is automatically inferred. Note that -1 can only appear once in `output_shape`. To flatten all non-batch dimensions use `Flatten`. preserve_dims: Number of leading dimensions that will not be reshaped. kwargs: Additional keyword arguments passed to Module. Raises: ValueError: If `preserve_dims` is not positive. \"\"\" super () . __init__ ( ** kwargs ) if preserve_dims <= 0 : raise ValueError ( \"Argument preserve_dims should be >= 1.\" ) if output_shape . count ( - 1 ) > 1 : raise ValueError ( \"-1 can only occur once in `output_shape`.\" ) self . output_shape = tuple ( output_shape ) self . preserve_dims = preserve_dims","title":"__init__()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/flatten.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.call","text":"Parameters: Name Type Description Default inputs ndarray the array to be reshaped. required Returns: Type Description ndarray A reshaped array. Source code in elegy/nn/flatten.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Arguments: inputs: the array to be reshaped. Returns: A reshaped array. \"\"\" if inputs . ndim <= self . preserve_dims : return inputs if - 1 in self . output_shape : reshaped_shape = _infer_shape ( self . output_shape , inputs . shape [ self . preserve_dims :] ) else : reshaped_shape = self . output_shape shape = inputs . shape [: self . preserve_dims ] + reshaped_shape return jnp . reshape ( inputs , shape )","title":"call()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/flatten.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.init","text":"Initializes the module, Source code in elegy/nn/flatten.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/flatten.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/flatten.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/Sequential/","text":"elegy.nn.Sequential Creates a Module from a zero argument lambda that produces a list of Modules or function to be executed sequentially. The lambda is necessary so that all sub-modules are instantiated inside the context of the Sequential module. mlp = elegy . nn . Sequential ( lambda : [ elegy . nn . Linear ( 64 ), jax . nn . relu , elegy . nn . Linear ( 32 ), jax . nn . relu , elegy . nn . Linear ( 10 ), jax . nn . softmax , ] ) y = mlp ( x ) add_parameter ( self , name , shape = (), dtype = None , initializer =< function zeros at 0x7fb715333dc0 > , trainable = True ) inherited A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/sequential_module.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value call ( self , * args , ** kwargs ) Connects all layers. *args and **kwargs are passed to the first layer. Source code in elegy/nn/sequential_module.py 98 99 100 def call ( self , * args , ** kwargs ): \"\"\"Connects all layers. `*args` and `**kwargs` are passed to the first layer.\"\"\" return sequential ( * self . layers )( * args , ** kwargs ) get_parameters ( self , trainable = None ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/sequential_module.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params init ( self , * args , ** kwargs ) inherited Initializes the module, Source code in elegy/nn/sequential_module.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/sequential_module.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/sequential_module.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"Sequential"},{"location":"api/nn/Sequential/#elegynnsequential","text":"","title":"elegy.nn.Sequential"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential","text":"Creates a Module from a zero argument lambda that produces a list of Modules or function to be executed sequentially. The lambda is necessary so that all sub-modules are instantiated inside the context of the Sequential module. mlp = elegy . nn . Sequential ( lambda : [ elegy . nn . Linear ( 64 ), jax . nn . relu , elegy . nn . Linear ( 32 ), jax . nn . relu , elegy . nn . Linear ( 10 ), jax . nn . softmax , ] ) y = mlp ( x )","title":"elegy.nn.sequential_module.Sequential"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.add_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fb715333dc0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/nn/sequential_module.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def add_parameter ( self , name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , trainable : bool = True , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if not hasattr ( self , name ): if not is_initializing (): raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) self . _params [ name ] = trainable if dtype is None : dtype = self . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( self , name , initial_value ) elif name not in self . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( self , name ) return value","title":"add_parameter()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.call","text":"Connects all layers. *args and **kwargs are passed to the first layer. Source code in elegy/nn/sequential_module.py 98 99 100 def call ( self , * args , ** kwargs ): \"\"\"Connects all layers. `*args` and `**kwargs` are passed to the first layer.\"\"\" return sequential ( * self . layers )( * args , ** kwargs )","title":"call()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/sequential_module.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def get_parameters ( self , trainable : tp . Optional [ bool ] = None , ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key , param_is_trainable in module . _params . items () if hasattr ( module , key ) and ( trainable is None or trainable == ( param_is_trainable and module . trainable ) ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.init","text":"Initializes the module, Source code in elegy/nn/sequential_module.py 354 355 356 357 358 359 360 361 362 def init ( self , * args , ** kwargs ) -> None : \"\"\" Initializes the module, \"\"\" with init_context ( can_update = False ): self ( * args , ** kwargs ) self . initialized = True","title":"init()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/sequential_module.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/sequential_module.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" def f ( module : Module , values : tp . Dict [ str , tp . Any ]): for key , value in values . items (): if key in module . _params : setattr ( module , key , value ) tree_apply ( f , self , values )","title":"set_parameters()"},{"location":"api/nn/sequential/","text":"elegy.nn.sequential Connects all layers. *args and **kwargs are passed to the first layer. def call ( self , x ): mlp = elegy . nn . sequential ( elegy . nn . Linear ( 64 ), jax . nn . relu , elegy . nn . Linear ( 32 ), jax . nn . relu , elegy . nn . Linear ( 10 ), jax . nn . softmax , ) y = mlp ( x ) ... Note sequential is not a Module , that is, it wont create a scope over the layers it runs, in constrast to Sequential layers are eagerly instantiate outside of sequential and just passed to it to automate the execution. Parameters: Name Type Description Default layers Callable[..., Any] Modules or functions passed as *args () Returns: Type Description Callable[..., Any] A callable that waits for the inputs and applies the layers sequentially. Source code in elegy/nn/sequential_module.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def sequential ( * layers : tp . Callable [ ... , tp . Any ]) -> tp . Callable [ ... , tp . Any ]: \"\"\" Connects all layers. `*args` and `**kwargs` are passed to the first layer. ```python def call(self, x): mlp = elegy.nn.sequential( elegy.nn.Linear(64), jax.nn.relu, elegy.nn.Linear(32), jax.nn.relu, elegy.nn.Linear(10), jax.nn.softmax, ) y = mlp(x) ... ``` !!! Note `sequential` is not a `Module`, that is, it wont create a scope over the layers it runs, in constrast to `Sequential` layers are eagerly instantiate outside of `sequential` and just passed to it to automate the execution. Arguments: layers: Modules or functions passed as `*args` Returns: A callable that waits for the inputs and applies the layers sequentially. \"\"\" def call ( inputs , * args , ** kwargs ): out = inputs for i , layer in enumerate ( layers ): if i == 0 : out = layer ( out , * args , ** kwargs ) else : out = layer ( out ) if not isinstance ( layer , Module ): name = ( layer . __name__ if hasattr ( layer , \"__name__\" ) else layer . __class__ . __name__ ) add_summary ( name , out ) return out return call","title":"sequential"},{"location":"api/nn/sequential/#elegynnsequential","text":"","title":"elegy.nn.sequential"},{"location":"api/nn/sequential/#elegy.nn.sequential_module.sequential","text":"Connects all layers. *args and **kwargs are passed to the first layer. def call ( self , x ): mlp = elegy . nn . sequential ( elegy . nn . Linear ( 64 ), jax . nn . relu , elegy . nn . Linear ( 32 ), jax . nn . relu , elegy . nn . Linear ( 10 ), jax . nn . softmax , ) y = mlp ( x ) ... Note sequential is not a Module , that is, it wont create a scope over the layers it runs, in constrast to Sequential layers are eagerly instantiate outside of sequential and just passed to it to automate the execution. Parameters: Name Type Description Default layers Callable[..., Any] Modules or functions passed as *args () Returns: Type Description Callable[..., Any] A callable that waits for the inputs and applies the layers sequentially. Source code in elegy/nn/sequential_module.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def sequential ( * layers : tp . Callable [ ... , tp . Any ]) -> tp . Callable [ ... , tp . Any ]: \"\"\" Connects all layers. `*args` and `**kwargs` are passed to the first layer. ```python def call(self, x): mlp = elegy.nn.sequential( elegy.nn.Linear(64), jax.nn.relu, elegy.nn.Linear(32), jax.nn.relu, elegy.nn.Linear(10), jax.nn.softmax, ) y = mlp(x) ... ``` !!! Note `sequential` is not a `Module`, that is, it wont create a scope over the layers it runs, in constrast to `Sequential` layers are eagerly instantiate outside of `sequential` and just passed to it to automate the execution. Arguments: layers: Modules or functions passed as `*args` Returns: A callable that waits for the inputs and applies the layers sequentially. \"\"\" def call ( inputs , * args , ** kwargs ): out = inputs for i , layer in enumerate ( layers ): if i == 0 : out = layer ( out , * args , ** kwargs ) else : out = layer ( out ) if not isinstance ( layer , Module ): name = ( layer . __name__ if hasattr ( layer , \"__name__\" ) else layer . __class__ . __name__ ) add_summary ( name , out ) return out return call","title":"elegy.nn.sequential_module.sequential"},{"location":"api/regularizers/GlobalL1/","text":"elegy.regularizers.GlobalL1 Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . regularizers . GlobalL1 ( l = 1e-5 ) ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L1 regularization factor. 0.01 kwargs Additional keyword arguments passed to Module. {} Returns: Type Description GlobalL1L2 An L1 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l1.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def GlobalL1 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l1_regularization\" , ** kwargs ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.regularizers.GlobalL1(l=1e-5) ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L1 regularization factor. kwargs: Additional keyword arguments passed to Module. Returns: An L1 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l1 = l , reduction = reduction , name = name , ** kwargs )","title":"GlobalL1"},{"location":"api/regularizers/GlobalL1/#elegyregularizersgloball1","text":"","title":"elegy.regularizers.GlobalL1"},{"location":"api/regularizers/GlobalL1/#elegy.regularizers.global_l1.GlobalL1","text":"Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . regularizers . GlobalL1 ( l = 1e-5 ) ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L1 regularization factor. 0.01 kwargs Additional keyword arguments passed to Module. {} Returns: Type Description GlobalL1L2 An L1 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l1.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def GlobalL1 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l1_regularization\" , ** kwargs ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.regularizers.GlobalL1(l=1e-5) ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L1 regularization factor. kwargs: Additional keyword arguments passed to Module. Returns: An L1 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l1 = l , reduction = reduction , name = name , ** kwargs )","title":"elegy.regularizers.global_l1.GlobalL1"},{"location":"api/regularizers/GlobalL1L2/","text":"elegy.regularizers.GlobalL1L2 A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . regularizers . GlobalL1L2 ( l1 = 1e-5 , l2 = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Attributes: Name Type Description l1 L1 regularization factor. l2 L2 regularization factor. call ( self , parameters ) Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default parameters Mapping[str, Mapping[str, jax._src.numpy.lax_numpy.ndarray]] A structure with all the parameters of the model. required Source code in elegy/regularizers/global_l1l2.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def call ( self , parameters : hk . Params ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: parameters: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in jax . tree_leaves ( parameters ) ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in jax . tree_leaves ( parameters ) ) return regularization","title":"GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegyregularizersgloball1l2","text":"","title":"elegy.regularizers.GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2","text":"A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . regularizers . GlobalL1L2 ( l1 = 1e-5 , l2 = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Attributes: Name Type Description l1 L1 regularization factor. l2 L2 regularization factor.","title":"elegy.regularizers.global_l1l2.GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.call","text":"Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default parameters Mapping[str, Mapping[str, jax._src.numpy.lax_numpy.ndarray]] A structure with all the parameters of the model. required Source code in elegy/regularizers/global_l1l2.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def call ( self , parameters : hk . Params ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: parameters: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in jax . tree_leaves ( parameters ) ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in jax . tree_leaves ( parameters ) ) return regularization","title":"call()"},{"location":"api/regularizers/GlobalL2/","text":"elegy.regularizers.GlobalL2 Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . losses . GlobaL2Regularization ( l = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L2 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L2 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l2.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def GlobalL2 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l2_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.losses.GlobaL2Regularization(l=1e-4), ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L2 regularization factor. Returns: An L2 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l2 = l , reduction = reduction , name = name )","title":"GlobalL2"},{"location":"api/regularizers/GlobalL2/#elegyregularizersgloball2","text":"","title":"elegy.regularizers.GlobalL2"},{"location":"api/regularizers/GlobalL2/#elegy.regularizers.global_l2.GlobalL2","text":"Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . losses . GlobaL2Regularization ( l = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L2 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L2 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l2.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def GlobalL2 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l2_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.losses.GlobaL2Regularization(l=1e-4), ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L2 regularization factor. Returns: An L2 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l2 = l , reduction = reduction , name = name )","title":"elegy.regularizers.global_l2.GlobalL2"},{"location":"guides/contributing/","text":"Contributing This is a short guide on how to start contributing to Elegy along with some best practices for the project. Setup We use poetry >= 1.1.4 , the easiest way to setup a development environment is run: poetry config virtualenvs.in-project true --local poetry install In order for Jax to recognize your GPU, you will probably have to install it again using the command below. PYTHON_VERSION = cp38 CUDA_VERSION = cuda101 # alternatives: cuda100, cuda101, cuda102, cuda110, check your cuda version PLATFORM = manylinux2010_x86_64 # alternatives: manylinux2010_x86_64 BASE_URL = 'https://storage.googleapis.com/jax-releases' pip install --upgrade $BASE_URL / $CUDA_VERSION /jaxlib-0.1.55- $PYTHON_VERSION -none- $PLATFORM .whl pip install --upgrade jax Creating Losses and Metrics For this you can follow these guidelines: Each loss / metric should be defined in its own file. Inherit from either elegy.losses.loss.Loss or elegy.metrics.metric.Metric or an existing class that inherits from them. Try to use an existing metric or loss as a template You must provide documentation for the following: The class definition. The __init__ method. The call method. Try to port the documentation + signature from its Keras counter part. If so you must give credits to the original source file. You must include tests. If you there exists an equivalent loss/metric in Keras you must test numerical equivalence between both. Testing To execute all the tests just run pytest Documentation We use mkdocs . If you create a new object that requires documentation please do the following: Add a markdown file inside /docs/api in the appropriate location according to the project's structure. This file must: Contain the path of function / class as header Use mkdocstring to render the API information. Example: # elegy . losses . BinaryCrossentropy ::: elegy . losses . BinaryCrossentropy selection : inherited_members : true members : - call - __init__ Add and entry to mkdocs.yml inside nav pointing to this file. Checkout mkdocs.yml . To build and visualize the documentation locally run mkdocs serve Creating a PR Before sending a pull request make sure all test run and code is formatted with black : black .","title":"Contributing"},{"location":"guides/contributing/#contributing","text":"This is a short guide on how to start contributing to Elegy along with some best practices for the project.","title":"Contributing"},{"location":"guides/contributing/#setup","text":"We use poetry >= 1.1.4 , the easiest way to setup a development environment is run: poetry config virtualenvs.in-project true --local poetry install In order for Jax to recognize your GPU, you will probably have to install it again using the command below. PYTHON_VERSION = cp38 CUDA_VERSION = cuda101 # alternatives: cuda100, cuda101, cuda102, cuda110, check your cuda version PLATFORM = manylinux2010_x86_64 # alternatives: manylinux2010_x86_64 BASE_URL = 'https://storage.googleapis.com/jax-releases' pip install --upgrade $BASE_URL / $CUDA_VERSION /jaxlib-0.1.55- $PYTHON_VERSION -none- $PLATFORM .whl pip install --upgrade jax","title":"Setup"},{"location":"guides/contributing/#creating-losses-and-metrics","text":"For this you can follow these guidelines: Each loss / metric should be defined in its own file. Inherit from either elegy.losses.loss.Loss or elegy.metrics.metric.Metric or an existing class that inherits from them. Try to use an existing metric or loss as a template You must provide documentation for the following: The class definition. The __init__ method. The call method. Try to port the documentation + signature from its Keras counter part. If so you must give credits to the original source file. You must include tests. If you there exists an equivalent loss/metric in Keras you must test numerical equivalence between both.","title":"Creating Losses and Metrics"},{"location":"guides/contributing/#testing","text":"To execute all the tests just run pytest","title":"Testing"},{"location":"guides/contributing/#documentation","text":"We use mkdocs . If you create a new object that requires documentation please do the following: Add a markdown file inside /docs/api in the appropriate location according to the project's structure. This file must: Contain the path of function / class as header Use mkdocstring to render the API information. Example: # elegy . losses . BinaryCrossentropy ::: elegy . losses . BinaryCrossentropy selection : inherited_members : true members : - call - __init__ Add and entry to mkdocs.yml inside nav pointing to this file. Checkout mkdocs.yml . To build and visualize the documentation locally run mkdocs serve","title":"Documentation"},{"location":"guides/contributing/#creating-a-pr","text":"Before sending a pull request make sure all test run and code is formatted with black : black .","title":"Creating a PR"},{"location":"guides/module-system/","text":"The Module System This is a guide to Elegy's underlying Module System. It will help get a better understanding of how Elegy interacts with Jax at the lower level, certain details about the hooks system and how it differs from other Deep Learning frameworks. Traditional Object Oriented Style We will begin by exploring how other frameworks define Modules / Layers. It is very common to use Object Oriented architectures as backbones of Module systems as it helps frameworks keep track of the parameters and states each module might require. Here we will create some very basic Linear and MLP modules which will seem very familiar: class Linear ( elegy . Module ): def __init__ ( self , n_in , n_out ): super () . __init__ () self . w = self . add_parameter ( \"w\" , [ x . shape [ - 1 ], self . n_out ], initializer = elegy . initializers . RandomUniform (), ) self . b = self . add_parameter ( \"b\" , [ n_out ], initializer = elegy . initializers . RandomUniform ()) def call ( self , x ): return jnp . dot ( x , self . w ) + self . b class MLP ( elegy . Module ): def __init__ ( self , n_in ): self . linear1 = Linear ( n_in , 64 ) self . linear2 = Linear ( 64 , 32 ) self . linear3 = Linear ( 32 , 1 ) def call ( self , x ): x = self . linear1 ( x ) x = jax . nn . relu ( x ) x = self . linear2 ( x ) x = jax . nn . relu ( x ) x = self . linear3 ( x ) return x Here we just defined a simple linear layer and used it inside a MLP with 3 layers. Pytorch and Keras users should feel very familiar with this type of code: we define parameters or other submodules in the __init__ method, and use them during the call (forward) method. Keras users might complain that if we do things this way we loose the ability to do shape inference, but don't worry, we will fix that latter. Fow now it is important to notice that here we use our first hook: add_parameter . Elegy Hooks Hooks are a way in which we can manage state while preserving functional purity (in the end). Elegy's hook system is ported and expanded from Haiku, but hook-based functional architectures in other areas like web development have proven valuable, React Hooks being a recent notable success. In Elegy we have the following list of hooks: Hook Description self.add_parameter Gives us access to trainable and non-trainable parameters. elegy.add_loss Lets us declare a loss from some intermediate module. elegy.add_metric Lets us declare a metric in some intermediate module. elegy.add_summary Lets us declare a summary in some intermediate module. elegy.training Tells us whether training is currently happening or not. elegy.next_rng_key Gives us access to a unique PRNGKey we can pass to functions like jax.random.uniform and friends. Note If you use existing Module s you might not need to worry much about these hooks, but keep them in mind if you are developing your own custom modules. Module Hooks: Functional Style In the initial example we used hooks in a very shy manner to replicate the behavior of of other frameworks, now we will go beyond. The first thing we need to know is that: Quote Modules are hooks This means that module instantiation taps into the hook system, and that hooks are aware of the module in which they are executing in. In practice this will mean that we will be able to move a lot of the code defined on the __init__ method to the call method: class Linear ( elegy . Module ): def __init__ ( self , n_out ): super () . __init__ () self . n_out = n_out def call ( self , x ): w = self . add_parameter ( \"w\" , [ x . shape [ - 1 ], self . n_out ], initializer = elegy . initializers . RandomUniform (), ) b = self . add_parameter ( \"b\" , [ self . n_out ], initializer = jnp . zeros ) return jnp . dot ( x , w ) + b class MLP ( elegy . Module ): def call ( self , x ): x = Linear ( 64 )( x ) x = jax . nn . relu ( x ) x = Linear ( 32 )( x ) x = jax . nn . relu ( x ) x = Linear ( 1 )( x ) return x What happened here? Lets decompose it into two parts. First we moved the add_parameter definitions on the Linear module to the call method: class Linear ( elegy . Module ): def __init__ ( self , n_out ): super () . __init__ () self . n_out = n_out def call ( self , x ): w = self . add_parameter ( \"w\" , [ x . shape [ - 1 ], self . n_out ], initializer = elegy . initializers . RandomUniform (), ) b = self . add_parameter ( \"b\" , [ self . n_out ], initializer = jnp . zeros ) return jnp . dot ( x , w ) + b As you see this allows us to do shape inference since we have access to the inputs when defining our parameter's shape. Second, we also moved the instantiation of the Linear modules in MLP from __init__ to call : class MLP ( elegy . Module ): def call ( self , x ): x = Linear ( 64 )( x ) x = jax . nn . relu ( x ) x = Linear ( 32 )( x ) x = jax . nn . relu ( x ) x = Linear ( 1 )( x ) return x Here we are using Modules as hooks. While it may appear as if we are instantiating 3 new Linear modules on every call , Elegy is actually caching them behind the scenes with the help of Python metaclasses. There is one important rule you have to follow: Quote You must use hooks unconditionally This motto comes from React and it means that the module always has to call the same amount of hooks, and for module hooks specifically they must be called in the same order. For example the following code is invalid: def call ( self , x ): if x . shape [ 0 ] > 5 : x = elegy . nn . Conv2D ( 32 , [ 3 , 3 ])( x ) x = elegy . nn . Linear ( 48 , [ 3 , 3 ])( x ) else : x = elegy . nn . Linear ( 48 , [ 3 , 3 ])( x ) x = elegy . nn . Conv2D ( 32 , [ 3 , 3 ])( x ) return x Here Linear and Conv2D are dangerously swapped based on some condition. If you want to do this you can just clare them unconditionally and use them inside the condition: def call ( self , x ): linear = elegy . nn . Linear ( 48 , [ 3 , 3 ]) conv2d = elegy . nn . Conv2D ( 32 , [ 3 , 3 ]) if x . shape [ 0 ] > 5 : x = conv2d ( x ) x = linear ( x ) else : x = linear ( x ) x = conv2d ( x ) return x Hooks Preserve References In our MLP class we where able to create the Linear modules at their call site, this simplified our code a lot but we've seem to lost the reference to these modules. Having reference to other modules is critical for being able to e.g. easily compose modules that might be trained separately like in transfer learning, or being able to easily decompose / extract a sub-module and use it separately like when using the decoder of a VAE by itself to generate new samples. Because of this, Elegy actually assigns all submodules, parameters, and states as properties of the module: x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () mlp ( x ) linear , linear_1 , linear_2 = mlp . linear , mlp . linear_1 , mlp . linear_2 y_pred = mlp ( x ) assert linear is mlp . linear and linear_1 is mlp . linear_1 and linear_2 is mlp . linear_2 As you see we were able to access all the linear layer references. More over, we verified that these reference don't change during execution. Each submodule gets assigned to a a unique field name based on its class name and order of creation. You can customize this name by using the name argument available in the Module 's constructor. Low-level Training Loop A big theme in Jax is that state and computation are separate, this is a requirement because in order for combinators like jax.grad and jax.jit to work you need pure functions. Elegy as you've seen is object oriented so additional effort ir required to properly convert all the global states and Module parameters an inputs to a function so Jax can track them. To achieve Elegy implements its own jit and value_and_grad function wrappers that handle this for you. Lets create a low level training loop using the previous definition MLP along with these functions: x = np . random . uniform ( size = ( 15 , 3 )) y = np . random . uniform ( size = ( 15 , 1 )) mlp = MLP () def loss_fn ( x , y ): y_pred = mlp ( x ) return jnp . mean ( jnp . square ( y - y_pred )) def update ( x , y ): loss , gradients = elegy . value_and_grad ( loss_fn , modules = mlp )( x , y ) parameters = mlp . get_parameters ( trainable = True ) new_parameters = jax . tree_multimap ( lambda p , g : p - 0.01 * g , parameters , gradients ) mlp . set_parameters ( new_parameters ) return loss update_jit = elegy . jit ( update , modules = mlp ) for step in range ( 1000 ): loss = update_jit ( x , y ) print ( step , loss ) Here we created the functions loss_fn and update , plus a minimal training loop. Loss loss_fn calculate the Mean Square Error while update uses value_and_grad to calculate the gradient of the loss with respect to the trainable parameters of mlp . def update ( x , y ): loss , gradients = elegy . value_and_grad ( loss_fn , modules = mlp )( x , y ) parameters = mlp . get_parameters ( trainable = True ) new_parameters = jax . tree_multimap ( lambda p , g : p - 0.01 * g , parameters , gradients ) mlp . set_parameters ( new_parameters ) return loss After that we just use tree_multimap to implement Gradient Descent and get our new_parameters and then use the set_parameters method our Module to update its state. def update ( x , y ): loss , gradients = elegy . value_and_grad ( loss_fn , modules = mlp )( x , y ) parameters = mlp . get_parameters ( trainable = True ) new_parameters = jax . tree_multimap ( lambda p , g : p - 0.01 * g , parameters , gradients ) mlp . set_parameters ( new_parameters ) return loss Having our update function we can use elegy.jit to create an optimized version of our computation and create a minimal training loop. update_jit = elegy . jit ( update , modules = mlp ) for step in range ( 1000 ): loss = update_jit ( x , y ) print ( step , loss ) Notice that even though we are jitting update which has the set_parameters side effect (normally forbidden in Jax), learning is happening because update_jit automatically keeps track of changes to the parameters of mlp and updates them for us. Something similar is done in elegy.value_and_grad as you saw previously. Note Elegy has 2 types states: module state for the parameters of models and global state where Elegy keeps track of certain variables like an RNG for convenience. Elegy's jit behaves just like its Jax counterpart except that its aware of the changes in state such that: Jax properly recompiles if something changes. The jitted function behaves similar to its eager version in that it propagates changes in state inwards and outwards (this only applies to Elegy states, not arbitrary side effects). High Level Equivalent If all this seems a bit too manual for you don't worry, you can can easily express all the previous in a few lines of code using an elegy.Model : model = elegy . Model ( module = elegy . nn . Sequential ( lambda : [ elegy . nn . Linear ( 64 ), jax . nn . relu , elegy . nn . Linear ( 32 ), jax . nn . relu , elegy . nn . Linear ( 1 ), ] ), loss = elegy . losses . MeanSquaredError (), ) model . fit ( x = np . random . uniform ( size = ( 15 , 3 )), y = np . random . uniform ( size = ( 15 , 1 )), batch_size = 15 , epochs = 1000 , )","title":"The Module System"},{"location":"guides/module-system/#the-module-system","text":"This is a guide to Elegy's underlying Module System. It will help get a better understanding of how Elegy interacts with Jax at the lower level, certain details about the hooks system and how it differs from other Deep Learning frameworks.","title":"The Module System"},{"location":"guides/module-system/#traditional-object-oriented-style","text":"We will begin by exploring how other frameworks define Modules / Layers. It is very common to use Object Oriented architectures as backbones of Module systems as it helps frameworks keep track of the parameters and states each module might require. Here we will create some very basic Linear and MLP modules which will seem very familiar: class Linear ( elegy . Module ): def __init__ ( self , n_in , n_out ): super () . __init__ () self . w = self . add_parameter ( \"w\" , [ x . shape [ - 1 ], self . n_out ], initializer = elegy . initializers . RandomUniform (), ) self . b = self . add_parameter ( \"b\" , [ n_out ], initializer = elegy . initializers . RandomUniform ()) def call ( self , x ): return jnp . dot ( x , self . w ) + self . b class MLP ( elegy . Module ): def __init__ ( self , n_in ): self . linear1 = Linear ( n_in , 64 ) self . linear2 = Linear ( 64 , 32 ) self . linear3 = Linear ( 32 , 1 ) def call ( self , x ): x = self . linear1 ( x ) x = jax . nn . relu ( x ) x = self . linear2 ( x ) x = jax . nn . relu ( x ) x = self . linear3 ( x ) return x Here we just defined a simple linear layer and used it inside a MLP with 3 layers. Pytorch and Keras users should feel very familiar with this type of code: we define parameters or other submodules in the __init__ method, and use them during the call (forward) method. Keras users might complain that if we do things this way we loose the ability to do shape inference, but don't worry, we will fix that latter. Fow now it is important to notice that here we use our first hook: add_parameter .","title":"Traditional Object Oriented Style"},{"location":"guides/module-system/#elegy-hooks","text":"Hooks are a way in which we can manage state while preserving functional purity (in the end). Elegy's hook system is ported and expanded from Haiku, but hook-based functional architectures in other areas like web development have proven valuable, React Hooks being a recent notable success. In Elegy we have the following list of hooks: Hook Description self.add_parameter Gives us access to trainable and non-trainable parameters. elegy.add_loss Lets us declare a loss from some intermediate module. elegy.add_metric Lets us declare a metric in some intermediate module. elegy.add_summary Lets us declare a summary in some intermediate module. elegy.training Tells us whether training is currently happening or not. elegy.next_rng_key Gives us access to a unique PRNGKey we can pass to functions like jax.random.uniform and friends. Note If you use existing Module s you might not need to worry much about these hooks, but keep them in mind if you are developing your own custom modules.","title":"Elegy Hooks"},{"location":"guides/module-system/#module-hooks-functional-style","text":"In the initial example we used hooks in a very shy manner to replicate the behavior of of other frameworks, now we will go beyond. The first thing we need to know is that: Quote Modules are hooks This means that module instantiation taps into the hook system, and that hooks are aware of the module in which they are executing in. In practice this will mean that we will be able to move a lot of the code defined on the __init__ method to the call method: class Linear ( elegy . Module ): def __init__ ( self , n_out ): super () . __init__ () self . n_out = n_out def call ( self , x ): w = self . add_parameter ( \"w\" , [ x . shape [ - 1 ], self . n_out ], initializer = elegy . initializers . RandomUniform (), ) b = self . add_parameter ( \"b\" , [ self . n_out ], initializer = jnp . zeros ) return jnp . dot ( x , w ) + b class MLP ( elegy . Module ): def call ( self , x ): x = Linear ( 64 )( x ) x = jax . nn . relu ( x ) x = Linear ( 32 )( x ) x = jax . nn . relu ( x ) x = Linear ( 1 )( x ) return x What happened here? Lets decompose it into two parts. First we moved the add_parameter definitions on the Linear module to the call method: class Linear ( elegy . Module ): def __init__ ( self , n_out ): super () . __init__ () self . n_out = n_out def call ( self , x ): w = self . add_parameter ( \"w\" , [ x . shape [ - 1 ], self . n_out ], initializer = elegy . initializers . RandomUniform (), ) b = self . add_parameter ( \"b\" , [ self . n_out ], initializer = jnp . zeros ) return jnp . dot ( x , w ) + b As you see this allows us to do shape inference since we have access to the inputs when defining our parameter's shape. Second, we also moved the instantiation of the Linear modules in MLP from __init__ to call : class MLP ( elegy . Module ): def call ( self , x ): x = Linear ( 64 )( x ) x = jax . nn . relu ( x ) x = Linear ( 32 )( x ) x = jax . nn . relu ( x ) x = Linear ( 1 )( x ) return x Here we are using Modules as hooks. While it may appear as if we are instantiating 3 new Linear modules on every call , Elegy is actually caching them behind the scenes with the help of Python metaclasses. There is one important rule you have to follow: Quote You must use hooks unconditionally This motto comes from React and it means that the module always has to call the same amount of hooks, and for module hooks specifically they must be called in the same order. For example the following code is invalid: def call ( self , x ): if x . shape [ 0 ] > 5 : x = elegy . nn . Conv2D ( 32 , [ 3 , 3 ])( x ) x = elegy . nn . Linear ( 48 , [ 3 , 3 ])( x ) else : x = elegy . nn . Linear ( 48 , [ 3 , 3 ])( x ) x = elegy . nn . Conv2D ( 32 , [ 3 , 3 ])( x ) return x Here Linear and Conv2D are dangerously swapped based on some condition. If you want to do this you can just clare them unconditionally and use them inside the condition: def call ( self , x ): linear = elegy . nn . Linear ( 48 , [ 3 , 3 ]) conv2d = elegy . nn . Conv2D ( 32 , [ 3 , 3 ]) if x . shape [ 0 ] > 5 : x = conv2d ( x ) x = linear ( x ) else : x = linear ( x ) x = conv2d ( x ) return x","title":"Module Hooks: Functional Style"},{"location":"guides/module-system/#hooks-preserve-references","text":"In our MLP class we where able to create the Linear modules at their call site, this simplified our code a lot but we've seem to lost the reference to these modules. Having reference to other modules is critical for being able to e.g. easily compose modules that might be trained separately like in transfer learning, or being able to easily decompose / extract a sub-module and use it separately like when using the decoder of a VAE by itself to generate new samples. Because of this, Elegy actually assigns all submodules, parameters, and states as properties of the module: x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () mlp ( x ) linear , linear_1 , linear_2 = mlp . linear , mlp . linear_1 , mlp . linear_2 y_pred = mlp ( x ) assert linear is mlp . linear and linear_1 is mlp . linear_1 and linear_2 is mlp . linear_2 As you see we were able to access all the linear layer references. More over, we verified that these reference don't change during execution. Each submodule gets assigned to a a unique field name based on its class name and order of creation. You can customize this name by using the name argument available in the Module 's constructor.","title":"Hooks Preserve References"},{"location":"guides/module-system/#low-level-training-loop","text":"A big theme in Jax is that state and computation are separate, this is a requirement because in order for combinators like jax.grad and jax.jit to work you need pure functions. Elegy as you've seen is object oriented so additional effort ir required to properly convert all the global states and Module parameters an inputs to a function so Jax can track them. To achieve Elegy implements its own jit and value_and_grad function wrappers that handle this for you. Lets create a low level training loop using the previous definition MLP along with these functions: x = np . random . uniform ( size = ( 15 , 3 )) y = np . random . uniform ( size = ( 15 , 1 )) mlp = MLP () def loss_fn ( x , y ): y_pred = mlp ( x ) return jnp . mean ( jnp . square ( y - y_pred )) def update ( x , y ): loss , gradients = elegy . value_and_grad ( loss_fn , modules = mlp )( x , y ) parameters = mlp . get_parameters ( trainable = True ) new_parameters = jax . tree_multimap ( lambda p , g : p - 0.01 * g , parameters , gradients ) mlp . set_parameters ( new_parameters ) return loss update_jit = elegy . jit ( update , modules = mlp ) for step in range ( 1000 ): loss = update_jit ( x , y ) print ( step , loss ) Here we created the functions loss_fn and update , plus a minimal training loop. Loss loss_fn calculate the Mean Square Error while update uses value_and_grad to calculate the gradient of the loss with respect to the trainable parameters of mlp . def update ( x , y ): loss , gradients = elegy . value_and_grad ( loss_fn , modules = mlp )( x , y ) parameters = mlp . get_parameters ( trainable = True ) new_parameters = jax . tree_multimap ( lambda p , g : p - 0.01 * g , parameters , gradients ) mlp . set_parameters ( new_parameters ) return loss After that we just use tree_multimap to implement Gradient Descent and get our new_parameters and then use the set_parameters method our Module to update its state. def update ( x , y ): loss , gradients = elegy . value_and_grad ( loss_fn , modules = mlp )( x , y ) parameters = mlp . get_parameters ( trainable = True ) new_parameters = jax . tree_multimap ( lambda p , g : p - 0.01 * g , parameters , gradients ) mlp . set_parameters ( new_parameters ) return loss Having our update function we can use elegy.jit to create an optimized version of our computation and create a minimal training loop. update_jit = elegy . jit ( update , modules = mlp ) for step in range ( 1000 ): loss = update_jit ( x , y ) print ( step , loss ) Notice that even though we are jitting update which has the set_parameters side effect (normally forbidden in Jax), learning is happening because update_jit automatically keeps track of changes to the parameters of mlp and updates them for us. Something similar is done in elegy.value_and_grad as you saw previously. Note Elegy has 2 types states: module state for the parameters of models and global state where Elegy keeps track of certain variables like an RNG for convenience. Elegy's jit behaves just like its Jax counterpart except that its aware of the changes in state such that: Jax properly recompiles if something changes. The jitted function behaves similar to its eager version in that it propagates changes in state inwards and outwards (this only applies to Elegy states, not arbitrary side effects).","title":"Low-level Training Loop"},{"location":"guides/module-system/#high-level-equivalent","text":"If all this seems a bit too manual for you don't worry, you can can easily express all the previous in a few lines of code using an elegy.Model : model = elegy . Model ( module = elegy . nn . Sequential ( lambda : [ elegy . nn . Linear ( 64 ), jax . nn . relu , elegy . nn . Linear ( 32 ), jax . nn . relu , elegy . nn . Linear ( 1 ), ] ), loss = elegy . losses . MeanSquaredError (), ) model . fit ( x = np . random . uniform ( size = ( 15 , 3 )), y = np . random . uniform ( size = ( 15 , 1 )), batch_size = 15 , epochs = 1000 , )","title":"High Level Equivalent"},{"location":"guides/modules-losses-metrics/","text":"Modules, Losses, and Metrics This guide goes into depth on how modules, losses and metrics work in Elegy when used with an elegy.Model . For more in-depth explanation on how they work internally check out the Module System guide. Keras Limitations One of our goals with Elegy was to solve Keras restrictions around the type of losses and metrics you can define. When creating a complex model with multiple outputs in Keras, say output_a and output_b , you are forced to define losses and metrics per-output only: model . compile ( loss = { \"output_a\" : keras . losses . BinaryCrossentropy ( from_logits = True ), \"output_b\" : keras . losses . CategoricalCrossentropy ( from_logits = True ), }, metrics = { \"output_a\" : keras . losses . BinaryAccuracy ( from_logits = True ), \"output_b\" : keras . losses . CategoricalAccuracy ( from_logits = True ), }, ... ) This is very restrictive, in particular it doesn't allow the following: Losses and metrics that combine multiple outputs with multiple labels. A single loss/metric based on multiple outputs (a especial case of the previous). Losses and metrics that depend on other variables such as inputs, parameters, states, etc. Most of these are usually solvable by tricks such as: Concatenating the outputs / labels Passing the inputs and other kind of information as labels. Using the functional API which is more flexible (but it only runs on graph mode making it very painful to debug). It is clear that these solution are hacky. Sometimes they are non-obvious, and depending on the problem they can be insufficient. Dependency Injection Elegy solves the previous problems by introducing a dependency injection mechanism that allows the user to express complex functions by simply declaring the variables to use by their name . The following parameters are available for the different callables you pass to Elegy: parameter Description Module Metric Loss x Inputs of the model corresponding to the x argument of fit * x x y_true The input labels corresponding to the y argument of fit x x y_pred Outputs of the model x x sample_weight Importance of each sample x x class_weight Importance of each class x x training Whether training is currently in progress x x x parameters The learnable parameters of the model x x states The non-learnable parameters of the model x x Note The content of x is technically passed to the model's Module but the parameter name \"x\" will bare no special meaning in that context. Modules Modules define the architecture of the network, their primary task (in Elegy terms) is transforming the inputs x into outputs y_pred . To make it easy to consume the content of x , Elegy has some special but very simple rules on how the signature of any Module can be structured: 1. If x is simply an array it will be passed directly: class SomeModule ( elegy . Module ): def call ( self , m ): ... model = elegy . Model ( SomeModule (), ... ) a = get_inputs () model . fit ( x = a , ... ) In this case a is passed as m . 2. If x is a tuple , then x will be expanded positional arguments a.k.a. *args , this means that the module will have define exactly as many arguments as there are inputs. For example: class SomeModule ( elegy . Module ): def call ( self , m , n ): ... model = elegy . Model ( SomeModule (), ... ) a , b = get_inputs () model . fit ( x = ( a , b ), ... ) In this case a is passed as m and b is passed as n . 3. If x is a dict , then x will be expanded as keyword arguments a.k.a. **kwargs , in this case the module can optionally request any key defined in x as an argument. For example: class SomeModule ( elegy . Module ): def call ( self , n , o ): ... model = elegy . Model ( SomeModule (), ... ) a , b , c = get_inputs () model . fit ( x = { \"m\" : a , \"n\" : b , \"o\" : c }, ... ) Here only n and o are requested by name and you get as input its values b and c , the variable m with the content of a is safely ignored. If you want to request all the avaiable inputs you can use **kwargs . Losses Losses can request all the available parameters that Elegy provides for dependency injection. A typical loss will request the y_true and y_pred values (as its common / enforced in Keras). The Mean Squared Error loss for example is easily defined in these terms: class MSE ( elegy . Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 ) ... X_train , y_train = get_inputs () model . fit ( x = X_train , y = y_train , loss = MSE (), ) Here the input y is passed as y_true to MSE . For an auto-encoder however, it makes perfect sense to define the loss only in terms of x (according to the math) and Elegy lets you do exactly that: class AutoEncoderLoss ( elegy . Loss ): def call ( self , x , y_pred ): return jnp . mean ( jnp . square ( x - y_pred ), axis =- 1 ) ... X_train , _ = get_inputs () model . fit ( x = X_train , loss = AutoEncoderLoss (), ) Notice thanks to this we didn't have to define y on the fit method. Note An alternative here is to just use the previous definition of MSE and define y=X_train . However, avoiding the creation of redundant information is good in general and being explicit about dependencies helps documenting the behaviour of the model. Partitioning a loss If you have a complex loss function that is just a sum of different parts that have to be compute together you might define something like this: class SomeComplexFunction ( elegy . Loss ): def call ( self , x , y_true , y_pred , parameters , ... ): ... return a + b + c Elegy lets you return a dict specifying the name of each part: class SomeComplexFunction ( elegy . Loss ): def call ( self , x , y_true , y_pred , parameters , ... ): ... return { \"a\" : a , \"b\" : b , \"c\" : c , } Elegy will use this information to show you each loss separate in the logs / Tensorboard / History with the names: some_complex_function_loss/a some_complex_function_loss/b some_complex_function_loss/c Each individual loss will still be subject to the sample_weight and reduction behavior as specified to SomeComplexFunction . Multiple Outputs + Labels The Model 's constructor loss argument can accept a single Loss , a list or dict of losses, and even nested structures of the previous. However, in Elegy the form of loss is not strictly related to structure of input labels and outputs of the model. This is very different to Keras where each loss has to be matched with exactly one (label, output) pair. Elegy's method of dealing with multiple outputs and labels is super simple: Quote y_true will contain the entire structure passed to y . y_pred will contain the entire structure output by the Module . This means there are no restrictions on how you structure the loss function. According to this rule Keras and Elegy behave the same when there is only one output and one label because there is no structure. Both frameworks will allow you to define something like: model = Model ( ... loss = elegy . losses . CategoricalCrossentropy ( from_logits = True ) ) However, if you have many outputs and many labels, Elegy will just pass their structures to your loss and you will be able to do whatever you want by e.g. indexing these structures: class MyLoss ( Elegy . Loss ): def call ( self , y_true , y_pred ): return some_function ( y_true [ \"label_a\" ], y_pred [ \"output_a\" ], y_true [ \"label_b\" ] ) model = Model ( ... loss = elegy . losses . MyLoss () ) This example assumes the y_true and y_pred are dictionaries but they can also be tuples or nested structures. This strategy gives you maximal flexibility but come with the additional cost of having to implement your own loss function. Keras-like behavior While having this flexibility is good, there is a common scenario that Keras covers really well: what if you really just need one loss per (label, output) pair? In other words, how can we achieve equivalent behaviour of the following Keras code? class MyModel ( keras . Model ): def call ( self , x ): ... return { \"key_a\" : key_a , \"key_b\" : key_b , ... } ... model . compile ( loss = { \"key_a\" : keras . losses . BinaryCrossentropy (), \"key_b\" : keras . losses . MeanSquaredError (), ... }, loss_weights = { \"key_a\" : 10.0 , \"key_b\" : 1.0 , ... }, ) To do this Elegy lets each Loss optionally filter / index the y_true and y_pred arguments based on a string key (for dict s) or integer key (for tuple s) in the constructor's on parameter: class MyModule ( elegy . Module ): def call ( self , x ): ... return { \"key_a\" : key_a , \"key_b\" : key_b , ... } ... model = elegy . Model ( module = MyModule (), loss = [ elegy . losses . BinaryCrossentropy ( on = \"key_a\" , weight = 10.0 ), elegy . losses . MeanSquaredError ( on = \"key_b\" , weight = 1.0 ), ... ] ) This is almost exactly how Keras behaves except each loss is explicitly aware of which part of the output / label its supposed to attend to. The previous is roughly equivalent to manually indexing y_true and y_pred and passing the resulting value to the loss in question like this: model = elegy . Model ( module = MyModule (), loss = [ lambda y_true , y_pred : elegy . losses . BinaryCrossentropy ( weight = 10.0 )( y_true = y_true [ \"key_a\" ], y_pred = y_pred [ \"key_a\" ], ), lambda y_true , y_pred : elegy . losses . MeanSquaredError ( weight = 1.0 )( y_true = y_true [ \"key_b\" ], y_pred = y_pred [ \"key_b\" ], ), ... ] ) Note For the same reasons Elegy doesn't support the loss_weights parameter as defined in keras.compile . Nonetheless, each loss accepts a weight argument directly, as seen in the examples above, which you can use to recover this behavior. Metrics Metrics behave exactly like losses except for one thing: Quote Metrics can hold state. As in Keras, Elegy metrics are cumulative so they update their internal state on every step. From a user's perspective this means that you should use the self.add_parameter and self.update_parameter hooks when implementing your own metrics. Here is an example of a simple cumulative implementation of Accuracy which uses state hooks: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = self . add_parameter ( \"total\" , initializer = 0 , trainable = False ) count = self . add_parameter ( \"count\" , initializer = 0 , trainable = False ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) self . update_parameter ( \"total\" , total ) self . update_parameter ( \"count\" , count ) return total / count For a more in-depth description of how Elegy's hook system works check out the Module System guide.","title":"Modules, Losses, and Metrics"},{"location":"guides/modules-losses-metrics/#modules-losses-and-metrics","text":"This guide goes into depth on how modules, losses and metrics work in Elegy when used with an elegy.Model . For more in-depth explanation on how they work internally check out the Module System guide.","title":"Modules, Losses, and Metrics"},{"location":"guides/modules-losses-metrics/#keras-limitations","text":"One of our goals with Elegy was to solve Keras restrictions around the type of losses and metrics you can define. When creating a complex model with multiple outputs in Keras, say output_a and output_b , you are forced to define losses and metrics per-output only: model . compile ( loss = { \"output_a\" : keras . losses . BinaryCrossentropy ( from_logits = True ), \"output_b\" : keras . losses . CategoricalCrossentropy ( from_logits = True ), }, metrics = { \"output_a\" : keras . losses . BinaryAccuracy ( from_logits = True ), \"output_b\" : keras . losses . CategoricalAccuracy ( from_logits = True ), }, ... ) This is very restrictive, in particular it doesn't allow the following: Losses and metrics that combine multiple outputs with multiple labels. A single loss/metric based on multiple outputs (a especial case of the previous). Losses and metrics that depend on other variables such as inputs, parameters, states, etc. Most of these are usually solvable by tricks such as: Concatenating the outputs / labels Passing the inputs and other kind of information as labels. Using the functional API which is more flexible (but it only runs on graph mode making it very painful to debug). It is clear that these solution are hacky. Sometimes they are non-obvious, and depending on the problem they can be insufficient.","title":"Keras Limitations"},{"location":"guides/modules-losses-metrics/#dependency-injection","text":"Elegy solves the previous problems by introducing a dependency injection mechanism that allows the user to express complex functions by simply declaring the variables to use by their name . The following parameters are available for the different callables you pass to Elegy: parameter Description Module Metric Loss x Inputs of the model corresponding to the x argument of fit * x x y_true The input labels corresponding to the y argument of fit x x y_pred Outputs of the model x x sample_weight Importance of each sample x x class_weight Importance of each class x x training Whether training is currently in progress x x x parameters The learnable parameters of the model x x states The non-learnable parameters of the model x x Note The content of x is technically passed to the model's Module but the parameter name \"x\" will bare no special meaning in that context.","title":"Dependency Injection"},{"location":"guides/modules-losses-metrics/#modules","text":"Modules define the architecture of the network, their primary task (in Elegy terms) is transforming the inputs x into outputs y_pred . To make it easy to consume the content of x , Elegy has some special but very simple rules on how the signature of any Module can be structured: 1. If x is simply an array it will be passed directly: class SomeModule ( elegy . Module ): def call ( self , m ): ... model = elegy . Model ( SomeModule (), ... ) a = get_inputs () model . fit ( x = a , ... ) In this case a is passed as m . 2. If x is a tuple , then x will be expanded positional arguments a.k.a. *args , this means that the module will have define exactly as many arguments as there are inputs. For example: class SomeModule ( elegy . Module ): def call ( self , m , n ): ... model = elegy . Model ( SomeModule (), ... ) a , b = get_inputs () model . fit ( x = ( a , b ), ... ) In this case a is passed as m and b is passed as n . 3. If x is a dict , then x will be expanded as keyword arguments a.k.a. **kwargs , in this case the module can optionally request any key defined in x as an argument. For example: class SomeModule ( elegy . Module ): def call ( self , n , o ): ... model = elegy . Model ( SomeModule (), ... ) a , b , c = get_inputs () model . fit ( x = { \"m\" : a , \"n\" : b , \"o\" : c }, ... ) Here only n and o are requested by name and you get as input its values b and c , the variable m with the content of a is safely ignored. If you want to request all the avaiable inputs you can use **kwargs .","title":"Modules"},{"location":"guides/modules-losses-metrics/#losses","text":"Losses can request all the available parameters that Elegy provides for dependency injection. A typical loss will request the y_true and y_pred values (as its common / enforced in Keras). The Mean Squared Error loss for example is easily defined in these terms: class MSE ( elegy . Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 ) ... X_train , y_train = get_inputs () model . fit ( x = X_train , y = y_train , loss = MSE (), ) Here the input y is passed as y_true to MSE . For an auto-encoder however, it makes perfect sense to define the loss only in terms of x (according to the math) and Elegy lets you do exactly that: class AutoEncoderLoss ( elegy . Loss ): def call ( self , x , y_pred ): return jnp . mean ( jnp . square ( x - y_pred ), axis =- 1 ) ... X_train , _ = get_inputs () model . fit ( x = X_train , loss = AutoEncoderLoss (), ) Notice thanks to this we didn't have to define y on the fit method. Note An alternative here is to just use the previous definition of MSE and define y=X_train . However, avoiding the creation of redundant information is good in general and being explicit about dependencies helps documenting the behaviour of the model.","title":"Losses"},{"location":"guides/modules-losses-metrics/#partitioning-a-loss","text":"If you have a complex loss function that is just a sum of different parts that have to be compute together you might define something like this: class SomeComplexFunction ( elegy . Loss ): def call ( self , x , y_true , y_pred , parameters , ... ): ... return a + b + c Elegy lets you return a dict specifying the name of each part: class SomeComplexFunction ( elegy . Loss ): def call ( self , x , y_true , y_pred , parameters , ... ): ... return { \"a\" : a , \"b\" : b , \"c\" : c , } Elegy will use this information to show you each loss separate in the logs / Tensorboard / History with the names: some_complex_function_loss/a some_complex_function_loss/b some_complex_function_loss/c Each individual loss will still be subject to the sample_weight and reduction behavior as specified to SomeComplexFunction .","title":"Partitioning a loss"},{"location":"guides/modules-losses-metrics/#multiple-outputs-labels","text":"The Model 's constructor loss argument can accept a single Loss , a list or dict of losses, and even nested structures of the previous. However, in Elegy the form of loss is not strictly related to structure of input labels and outputs of the model. This is very different to Keras where each loss has to be matched with exactly one (label, output) pair. Elegy's method of dealing with multiple outputs and labels is super simple: Quote y_true will contain the entire structure passed to y . y_pred will contain the entire structure output by the Module . This means there are no restrictions on how you structure the loss function. According to this rule Keras and Elegy behave the same when there is only one output and one label because there is no structure. Both frameworks will allow you to define something like: model = Model ( ... loss = elegy . losses . CategoricalCrossentropy ( from_logits = True ) ) However, if you have many outputs and many labels, Elegy will just pass their structures to your loss and you will be able to do whatever you want by e.g. indexing these structures: class MyLoss ( Elegy . Loss ): def call ( self , y_true , y_pred ): return some_function ( y_true [ \"label_a\" ], y_pred [ \"output_a\" ], y_true [ \"label_b\" ] ) model = Model ( ... loss = elegy . losses . MyLoss () ) This example assumes the y_true and y_pred are dictionaries but they can also be tuples or nested structures. This strategy gives you maximal flexibility but come with the additional cost of having to implement your own loss function.","title":"Multiple Outputs + Labels"},{"location":"guides/modules-losses-metrics/#keras-like-behavior","text":"While having this flexibility is good, there is a common scenario that Keras covers really well: what if you really just need one loss per (label, output) pair? In other words, how can we achieve equivalent behaviour of the following Keras code? class MyModel ( keras . Model ): def call ( self , x ): ... return { \"key_a\" : key_a , \"key_b\" : key_b , ... } ... model . compile ( loss = { \"key_a\" : keras . losses . BinaryCrossentropy (), \"key_b\" : keras . losses . MeanSquaredError (), ... }, loss_weights = { \"key_a\" : 10.0 , \"key_b\" : 1.0 , ... }, ) To do this Elegy lets each Loss optionally filter / index the y_true and y_pred arguments based on a string key (for dict s) or integer key (for tuple s) in the constructor's on parameter: class MyModule ( elegy . Module ): def call ( self , x ): ... return { \"key_a\" : key_a , \"key_b\" : key_b , ... } ... model = elegy . Model ( module = MyModule (), loss = [ elegy . losses . BinaryCrossentropy ( on = \"key_a\" , weight = 10.0 ), elegy . losses . MeanSquaredError ( on = \"key_b\" , weight = 1.0 ), ... ] ) This is almost exactly how Keras behaves except each loss is explicitly aware of which part of the output / label its supposed to attend to. The previous is roughly equivalent to manually indexing y_true and y_pred and passing the resulting value to the loss in question like this: model = elegy . Model ( module = MyModule (), loss = [ lambda y_true , y_pred : elegy . losses . BinaryCrossentropy ( weight = 10.0 )( y_true = y_true [ \"key_a\" ], y_pred = y_pred [ \"key_a\" ], ), lambda y_true , y_pred : elegy . losses . MeanSquaredError ( weight = 1.0 )( y_true = y_true [ \"key_b\" ], y_pred = y_pred [ \"key_b\" ], ), ... ] ) Note For the same reasons Elegy doesn't support the loss_weights parameter as defined in keras.compile . Nonetheless, each loss accepts a weight argument directly, as seen in the examples above, which you can use to recover this behavior.","title":"Keras-like behavior"},{"location":"guides/modules-losses-metrics/#metrics","text":"Metrics behave exactly like losses except for one thing: Quote Metrics can hold state. As in Keras, Elegy metrics are cumulative so they update their internal state on every step. From a user's perspective this means that you should use the self.add_parameter and self.update_parameter hooks when implementing your own metrics. Here is an example of a simple cumulative implementation of Accuracy which uses state hooks: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = self . add_parameter ( \"total\" , initializer = 0 , trainable = False ) count = self . add_parameter ( \"count\" , initializer = 0 , trainable = False ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) self . update_parameter ( \"total\" , total ) self . update_parameter ( \"count\" , count ) return total / count For a more in-depth description of how Elegy's hook system works check out the Module System guide.","title":"Metrics"}]}