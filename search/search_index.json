{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Elegy Elegy is a Neural Networks framework based on Jax inspired by Keras and Haiku. Elegy implements the Keras API but makes changes to play better with Jax and gives more flexibility around losses and metrics , it also ports Haiku's excellent module system and makes it easier to use. Elegy is in an early stage, feel free to send us your feedback! Main Features Familiar : Elegy should feel very familiar to Keras users. Flexible : Elegy improves upon the basic Keras API by letting users optionally take more control over the definition of losses and metrics. Easy-to-use : Elegy maintains all the simplicity and ease of use that Keras brings with it. Compatible : Elegy strives to be compatible with the rest of the Jax ecosystem. For more information take a look at the Documentation . Installation Install Elegy using pip: pip install elegy For Windows users we recommend the Windows subsystem for linux 2 WSL2 since jax does not support it yet. Quick Start Elegy greatly simplifies the training of Deep Learning models compared to pure Jax where, due to Jax's functional nature, users have to do a lot of book keeping around the state of the model. In Elegy you just have to follow 3 basic steps: 1. Define the architecture inside an elegy.Module : class MLP ( elegy . Module ): def call ( self , x : jnp . ndarray ) -> jnp . ndarray : x = elegy . nn . Linear ( 300 )( x ) x = jax . nn . relu ( x ) x = elegy . nn . Linear ( 10 )( x ) return x Note that we can define sub-modules on-the-fly directly in the call (forward) method. 2. Create a Model from this module and specify additional things like losses, metrics, and optimizers: model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . rmsprop ( 1e-3 ), ) 3. Train the model using the fit method: model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . TensorBoard ( \"summaries\" )] ) And you are done! For more information check out: Our Getting Started tutorial. Elegy's Documentation . The examples directory. What is Jax? Why Jax & Elegy? Given all the well-stablished Deep Learning framework like TensorFlow + Keras or Pytorch + Pytorch-Lightning/Skorch, it is fair to ask why we need something like Jax + Elegy? Here are some of the reasons why this framework exists. Why Jax? Jax is a linear algebra library with the perfect recipe: * Numpy's familiar API * The speed and hardware support of XLA * Automatic Differentiation The awesome thing about Jax is that Deep Learning is just a use-case that it happens to excel at but you can use it for most task you would use NumPy for. Jax is so compatible with Numpy that is array type actually inherits from np.ndarray . In a sense, Jax takes the best of both TensorFlow and Pytorch in a principled manner: while both TF and Pytorch historically converged to the same set of features, their APIs still contain quirks they have to keep for compatibility. Why Elegy? We believe that Elegy can offer the best experience for coding Deep Learning applications by leveraging the power and familiarity of Jax API, an easy-to-use and succinct Module system, and packaging everything on top of a convenient Keras-like API. Elegy improves upon other Deep Learning frameworks in the following ways: Its hook-based Module System makes it easier (less verbose) to write model code compared to Keras & Pytorch since it lets you declare sub-modules, parameters, and states directly on your call (forward) method. Thanks to this you get shape inference for free so there is no need for a build method (Keras) or propagating shape information all over the place (Pytorch). A naive implementation of Linear could be as simple as: class Linear ( elegy . Module ): def __init__ ( self , units ): super () . __init__ () self . units = units def call ( self , x ): w = elegy . get_parameter ( \"w\" , [ x . shape [ - 1 ], self . units ], initializer = jnp . ones ) b = elegy . get_parameter ( \"b\" , [ self . units ], initializer = jnp . ones ) return jnp . dot ( x , w ) + b It has a very flexible system for defining the inputs for losses and metrics based on dependency injection in opposition to Keras rigid requirement to have matching (output, label) pairs, and being unable to use additional information like inputs, parameters, and states in the definition of losses and metrics. Its hook system preserve's reference information from a module to its sub-modules, parameters, and states while maintaining a functional API. This is crucial since most Jax-based frameworks like Flax and Haiku tend to loose this information which makes it very tricky to perform tasks like transfer learning where you need to mix a pre-trained models into a new model (easier to do if you keep references). Features Model estimator class losses module metrics module regularizers module callbacks module nn layers module For more information checkout the Reference API section in the Documentation . Contributing Deep Learning is evolving at an incredible pace, there is so much to do and so few hands. If you wish to contibute anything from a loss or metric to a new awesome feature for Elegy just open an issue or send a PR! For more information check out our Contibuting Guide . About Us We are some friends passionate about ML. License Apache Citing Elegy To cite this project: BibTeX @software{elegy2020repository, author = {PoetsAI}, title = {Elegy: A Keras-like deep learning framework based on Jax}, url = {https://github.com/poets-ai/elegy}, version = {0.2.2}, year = {2020}, } Where the current version may be retrieved either from the Release tag or the file elegy/__init__.py and the year corresponds to the project's release year.","title":"Introduction"},{"location":"#elegy","text":"Elegy is a Neural Networks framework based on Jax inspired by Keras and Haiku. Elegy implements the Keras API but makes changes to play better with Jax and gives more flexibility around losses and metrics , it also ports Haiku's excellent module system and makes it easier to use. Elegy is in an early stage, feel free to send us your feedback!","title":"Elegy"},{"location":"#main-features","text":"Familiar : Elegy should feel very familiar to Keras users. Flexible : Elegy improves upon the basic Keras API by letting users optionally take more control over the definition of losses and metrics. Easy-to-use : Elegy maintains all the simplicity and ease of use that Keras brings with it. Compatible : Elegy strives to be compatible with the rest of the Jax ecosystem. For more information take a look at the Documentation .","title":"Main Features"},{"location":"#installation","text":"Install Elegy using pip: pip install elegy For Windows users we recommend the Windows subsystem for linux 2 WSL2 since jax does not support it yet.","title":"Installation"},{"location":"#quick-start","text":"Elegy greatly simplifies the training of Deep Learning models compared to pure Jax where, due to Jax's functional nature, users have to do a lot of book keeping around the state of the model. In Elegy you just have to follow 3 basic steps: 1. Define the architecture inside an elegy.Module : class MLP ( elegy . Module ): def call ( self , x : jnp . ndarray ) -> jnp . ndarray : x = elegy . nn . Linear ( 300 )( x ) x = jax . nn . relu ( x ) x = elegy . nn . Linear ( 10 )( x ) return x Note that we can define sub-modules on-the-fly directly in the call (forward) method. 2. Create a Model from this module and specify additional things like losses, metrics, and optimizers: model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . rmsprop ( 1e-3 ), ) 3. Train the model using the fit method: model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . TensorBoard ( \"summaries\" )] ) And you are done! For more information check out: Our Getting Started tutorial. Elegy's Documentation . The examples directory. What is Jax?","title":"Quick Start"},{"location":"#why-jax-elegy","text":"Given all the well-stablished Deep Learning framework like TensorFlow + Keras or Pytorch + Pytorch-Lightning/Skorch, it is fair to ask why we need something like Jax + Elegy? Here are some of the reasons why this framework exists.","title":"Why Jax &amp; Elegy?"},{"location":"#why-jax","text":"Jax is a linear algebra library with the perfect recipe: * Numpy's familiar API * The speed and hardware support of XLA * Automatic Differentiation The awesome thing about Jax is that Deep Learning is just a use-case that it happens to excel at but you can use it for most task you would use NumPy for. Jax is so compatible with Numpy that is array type actually inherits from np.ndarray . In a sense, Jax takes the best of both TensorFlow and Pytorch in a principled manner: while both TF and Pytorch historically converged to the same set of features, their APIs still contain quirks they have to keep for compatibility.","title":"Why Jax?"},{"location":"#why-elegy","text":"We believe that Elegy can offer the best experience for coding Deep Learning applications by leveraging the power and familiarity of Jax API, an easy-to-use and succinct Module system, and packaging everything on top of a convenient Keras-like API. Elegy improves upon other Deep Learning frameworks in the following ways: Its hook-based Module System makes it easier (less verbose) to write model code compared to Keras & Pytorch since it lets you declare sub-modules, parameters, and states directly on your call (forward) method. Thanks to this you get shape inference for free so there is no need for a build method (Keras) or propagating shape information all over the place (Pytorch). A naive implementation of Linear could be as simple as: class Linear ( elegy . Module ): def __init__ ( self , units ): super () . __init__ () self . units = units def call ( self , x ): w = elegy . get_parameter ( \"w\" , [ x . shape [ - 1 ], self . units ], initializer = jnp . ones ) b = elegy . get_parameter ( \"b\" , [ self . units ], initializer = jnp . ones ) return jnp . dot ( x , w ) + b It has a very flexible system for defining the inputs for losses and metrics based on dependency injection in opposition to Keras rigid requirement to have matching (output, label) pairs, and being unable to use additional information like inputs, parameters, and states in the definition of losses and metrics. Its hook system preserve's reference information from a module to its sub-modules, parameters, and states while maintaining a functional API. This is crucial since most Jax-based frameworks like Flax and Haiku tend to loose this information which makes it very tricky to perform tasks like transfer learning where you need to mix a pre-trained models into a new model (easier to do if you keep references).","title":"Why Elegy?"},{"location":"#features","text":"Model estimator class losses module metrics module regularizers module callbacks module nn layers module For more information checkout the Reference API section in the Documentation .","title":"Features"},{"location":"#contributing","text":"Deep Learning is evolving at an incredible pace, there is so much to do and so few hands. If you wish to contibute anything from a loss or metric to a new awesome feature for Elegy just open an issue or send a PR! For more information check out our Contibuting Guide .","title":"Contributing"},{"location":"#about-us","text":"We are some friends passionate about ML.","title":"About Us"},{"location":"#license","text":"Apache","title":"License"},{"location":"#citing-elegy","text":"To cite this project: BibTeX @software{elegy2020repository, author = {PoetsAI}, title = {Elegy: A Keras-like deep learning framework based on Jax}, url = {https://github.com/poets-ai/elegy}, version = {0.2.2}, year = {2020}, } Where the current version may be retrieved either from the Release tag or the file elegy/__init__.py and the year corresponds to the project's release year.","title":"Citing Elegy"},{"location":"getting-started/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); In this tutorial we will explore the basic features of Elegy . If you are a Keras user you should feel at home, if you are currently using Jax things will appear much more streamlined. To get started you will first need to install the following dependencies: In [ ]: ! pip install elegy dataget matplotlib ! pip install jax jaxlib Note that Elegy doesn't depend on jax since there are both cpu and gpu versions you can choose from, the previous block will install jax-cpu , if you want jax to run on gpu you will need to install it separately. If you are running this example on colab you are good to go since it comes with a GPU/TPU-enabled version of jax preinstalled. Loading the Data \u00b6 In this tutorial we will train a Neural Network on the MNIST dataset, for this we will first need to download and load the data into memory. Here we will use dataget for simplicity but you can use you favorite datasets library. In [1]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () print ( \"X_train:\" , X_train . shape , X_train . dtype ) print ( \"y_train:\" , y_train . shape , y_train . dtype ) print ( \"X_test:\" , X_test . shape , X_test . dtype ) print ( \"y_test:\" , y_test . shape , y_test . dtype ) X_train: (60000, 28, 28) uint8 y_train: (60000,) uint8 X_test: (10000, 28, 28) uint8 y_test: (10000,) uint8 In this case dataget loads the data from Yann LeCun's website. Defining the Architecture \u00b6 Now that we have the data we can define our model. In Elegy you can do this by inheriting from elegy.Module and defining a call method. This method should take in some inputs, perform a series of transformation using Jax, and returns the outputs of the network. In this example we will create a simple 2 layer MLP using Elegy modules: In [2]: import jax.numpy as jnp import jax import elegy class MLP ( elegy . Module ): \"\"\"Standard LeNet-300-100 MLP network.\"\"\" def __init__ ( self , n1 : int = 300 , n2 : int = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . n1 = n1 self . n2 = n2 def call ( self , image : jnp . ndarray ) -> jnp . ndarray : image = image . astype ( jnp . float32 ) / 255.0 mlp = elegy . nn . sequential ( elegy . nn . Flatten (), elegy . nn . Linear ( self . n1 ), jax . nn . relu , elegy . nn . Linear ( self . n2 ), jax . nn . relu , elegy . nn . Linear ( 10 ), ) return mlp ( image ) Here we are using sequential to stack two layers with relu activations and a final Linear layer with 10 units that represents the logits of the network. This code should feel familiar to most Keras / PyTorch users, the main difference here is that thanks to elegy's hooks system you can (uncoditionally) declare modules, parameters, and states right in your call (forward) function without having to explicitly assign them to properties, this tends to produce much more readable code and reduce boilerplate. Creating the Model \u00b6 Now that we have this module we can create an Elegy Model . In [3]: import optax model = elegy . Model ( module = MLP ( n1 = 300 , n2 = 100 ), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-4 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), ) Much like keras.Model , an Elegy Model is tasked with performing training, evalaution, and inference. The constructor of this class accepts most of the arguments accepted by keras.Model.compile as you might have seen but there are some notable differences: It requires you to pass a module as first argument. loss can be a list even if we don't have multiple corresponding outputs/labels, this is because Elegy exposes a more flexible system for defining losses and metrics based on Dependency Injection. As in Keras, you can get a rich description of the model by calling Model.summary with a sample input: In [4]: model . summary ( X_train [: 64 ]) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Layer \u2502 Outputs Shape \u2502 Trainable \u2502 Non-trainable \u2502 \u2502 \u2502 \u2502 Parameters \u2502 Parameters \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 Inputs \u2502 (64, 28, 28) uint8 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 flatten (Flatten) \u2502 (64, 784) float32 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear (Linear) \u2502 (64, 300) float32 \u2502 235,500 942.0 KB \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 relu \u2502 (64, 300) float32 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear_1 (Linear) \u2502 (64, 100) float32 \u2502 30,100 120.4 KB \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 relu_1 \u2502 (64, 100) float32 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear_2 (Linear) \u2502 (64, 10) float32 \u2502 1,010 4.0 KB \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Outputs (MLP) \u2502 (64, 10) float32 \u2502 0 \u2502 0 \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Total Parameters: 266,610 1.1 MB Trainable Parameters: 266,610 1.1 MB Non-trainable Parameters: 0 Training the Model \u00b6 Having our model instance we are now ready need to pass it some data to start training. Like in Keras this is done via the fit method which contains more or less the same signature. We try to be as compatible with Keras as possible here but also remove a lot of the Tensorflow specific stuff. The following code will train our model for 100 epochs while limiting each epoch to 200 steps and using a batch size of 64 : In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . ModelCheckpoint ( \"model\" , save_best_only = True )], ) ... Epoch 99/100 200/200 [==============================] - 1s 4ms/step - l2_regularization_loss: 0.0452 - loss: 0.0662 - sparse_categorical_accuracy: 0.9928 - sparse_categorical_crossentropy_loss: 0.0210 - val_l2_regularization_loss: 0.0451 - val_loss: 0.1259 - val_sparse_categorical_accuracy: 0.9766 - val_sparse_categorical_crossentropy_loss: 0.0808 Epoch 100/100 200/200 [==============================] - 1s 4ms/step - l2_regularization_loss: 0.0450 - loss: 0.0610 - sparse_categorical_accuracy: 0.9953 - sparse_categorical_crossentropy_loss: 0.0161 - val_l2_regularization_loss: 0.0447 - val_loss: 0.1093 - val_sparse_categorical_accuracy: 0.9795 - val_sparse_categorical_crossentropy_loss: 0.0646 As you see we've ported Keras progress bar and also implemented its Callback and History APIs. fit returns a history object which we will use next to visualize how the metrics and losses evolved during training. In [6]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history ) Generating Predictions \u00b6 Having our trained model we can now get some samples from the test set and generate some predictions. First we will just pick some random samples using numpy : In [7]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) x_sample = X_test [ idxs ] Here we selected 9 random images. Now we can use the predict method to get their labels: In [8]: y_pred = model . predict ( x = x_sample ) Easy right? Finally lets plot the results to see if they are accurate. In [9]: plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( x_sample [ k ], cmap = \"gray\" ) Perfect! Serialization \u00b6 To serialize the Model you can just use the model.save(...) method, this will create a folder with some files that contain the model's code plus all parameters and states. However, here we don't need to do that since we previously added the elegy.callbacks.ModelCheckpoint callback on fit which periodically does this for us during training. We configured ModelCheckpoint to save our model to a folder called \"model\" so we can just load it from there using elegy.model.load . Lets get a new model reference containing the same weights and call its evaluate method to verify everything loaded correctly: In [10]: # current model reference print ( \"current model id:\" , id ( model )) # load model from disk model = elegy . model . load ( \"model\" ) # new model reference print ( \"new model id: \" , id ( model )) # check that it works! model . evaluate ( x = X_test , y = y_test ) current model id: 140137340602160 new model id: 140136071352432 /data/cristian/elegy/.venv/lib/python3.8/site-packages/jax/numpy/lax_numpy.py:1531: FutureWarning: jax.numpy reductions won't accept lists and tuples in future versions, only scalars and ndarrays warnings.warn(msg, category=FutureWarning) 313/313 [==============================] - 1s 3ms/step - l2_regularization_loss: 0.0450 - loss: 0.1013 - sparse_categorical_accuracy: 0.9825 - sparse_categorical_crossentropy_loss: 0.0563 Out[10]: {'l2_regularization_loss': array(0.04499801, dtype=float32), 'loss': array(0.10126135, dtype=float32), 'sparse_categorical_accuracy': array(0.9825001, dtype=float32), 'sparse_categorical_crossentropy_loss': array(0.05626357, dtype=float32), 'size': 32} Excellent! We hope you've enjoyed this tutorial. Next Steps \u00b6 Elegy is still in a very early stage, there are probably tons of bugs and missing features but we will get there. If you have some ideas or feedback on the current design we are eager to hear from you, feel free to open an issue.","title":"Getting Started"},{"location":"getting-started/#loading-the-data","text":"In this tutorial we will train a Neural Network on the MNIST dataset, for this we will first need to download and load the data into memory. Here we will use dataget for simplicity but you can use you favorite datasets library. In [1]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () print ( \"X_train:\" , X_train . shape , X_train . dtype ) print ( \"y_train:\" , y_train . shape , y_train . dtype ) print ( \"X_test:\" , X_test . shape , X_test . dtype ) print ( \"y_test:\" , y_test . shape , y_test . dtype ) X_train: (60000, 28, 28) uint8 y_train: (60000,) uint8 X_test: (10000, 28, 28) uint8 y_test: (10000,) uint8 In this case dataget loads the data from Yann LeCun's website.","title":"Loading the Data"},{"location":"getting-started/#defining-the-architecture","text":"Now that we have the data we can define our model. In Elegy you can do this by inheriting from elegy.Module and defining a call method. This method should take in some inputs, perform a series of transformation using Jax, and returns the outputs of the network. In this example we will create a simple 2 layer MLP using Elegy modules: In [2]: import jax.numpy as jnp import jax import elegy class MLP ( elegy . Module ): \"\"\"Standard LeNet-300-100 MLP network.\"\"\" def __init__ ( self , n1 : int = 300 , n2 : int = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . n1 = n1 self . n2 = n2 def call ( self , image : jnp . ndarray ) -> jnp . ndarray : image = image . astype ( jnp . float32 ) / 255.0 mlp = elegy . nn . sequential ( elegy . nn . Flatten (), elegy . nn . Linear ( self . n1 ), jax . nn . relu , elegy . nn . Linear ( self . n2 ), jax . nn . relu , elegy . nn . Linear ( 10 ), ) return mlp ( image ) Here we are using sequential to stack two layers with relu activations and a final Linear layer with 10 units that represents the logits of the network. This code should feel familiar to most Keras / PyTorch users, the main difference here is that thanks to elegy's hooks system you can (uncoditionally) declare modules, parameters, and states right in your call (forward) function without having to explicitly assign them to properties, this tends to produce much more readable code and reduce boilerplate.","title":"Defining the Architecture"},{"location":"getting-started/#creating-the-model","text":"Now that we have this module we can create an Elegy Model . In [3]: import optax model = elegy . Model ( module = MLP ( n1 = 300 , n2 = 100 ), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-4 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), ) Much like keras.Model , an Elegy Model is tasked with performing training, evalaution, and inference. The constructor of this class accepts most of the arguments accepted by keras.Model.compile as you might have seen but there are some notable differences: It requires you to pass a module as first argument. loss can be a list even if we don't have multiple corresponding outputs/labels, this is because Elegy exposes a more flexible system for defining losses and metrics based on Dependency Injection. As in Keras, you can get a rich description of the model by calling Model.summary with a sample input: In [4]: model . summary ( X_train [: 64 ]) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Layer \u2502 Outputs Shape \u2502 Trainable \u2502 Non-trainable \u2502 \u2502 \u2502 \u2502 Parameters \u2502 Parameters \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 Inputs \u2502 (64, 28, 28) uint8 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 flatten (Flatten) \u2502 (64, 784) float32 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear (Linear) \u2502 (64, 300) float32 \u2502 235,500 942.0 KB \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 relu \u2502 (64, 300) float32 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear_1 (Linear) \u2502 (64, 100) float32 \u2502 30,100 120.4 KB \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 relu_1 \u2502 (64, 100) float32 \u2502 0 \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear_2 (Linear) \u2502 (64, 10) float32 \u2502 1,010 4.0 KB \u2502 0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Outputs (MLP) \u2502 (64, 10) float32 \u2502 0 \u2502 0 \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Total Parameters: 266,610 1.1 MB Trainable Parameters: 266,610 1.1 MB Non-trainable Parameters: 0","title":"Creating the Model"},{"location":"getting-started/#training-the-model","text":"Having our model instance we are now ready need to pass it some data to start training. Like in Keras this is done via the fit method which contains more or less the same signature. We try to be as compatible with Keras as possible here but also remove a lot of the Tensorflow specific stuff. The following code will train our model for 100 epochs while limiting each epoch to 200 steps and using a batch size of 64 : In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . ModelCheckpoint ( \"model\" , save_best_only = True )], ) ... Epoch 99/100 200/200 [==============================] - 1s 4ms/step - l2_regularization_loss: 0.0452 - loss: 0.0662 - sparse_categorical_accuracy: 0.9928 - sparse_categorical_crossentropy_loss: 0.0210 - val_l2_regularization_loss: 0.0451 - val_loss: 0.1259 - val_sparse_categorical_accuracy: 0.9766 - val_sparse_categorical_crossentropy_loss: 0.0808 Epoch 100/100 200/200 [==============================] - 1s 4ms/step - l2_regularization_loss: 0.0450 - loss: 0.0610 - sparse_categorical_accuracy: 0.9953 - sparse_categorical_crossentropy_loss: 0.0161 - val_l2_regularization_loss: 0.0447 - val_loss: 0.1093 - val_sparse_categorical_accuracy: 0.9795 - val_sparse_categorical_crossentropy_loss: 0.0646 As you see we've ported Keras progress bar and also implemented its Callback and History APIs. fit returns a history object which we will use next to visualize how the metrics and losses evolved during training. In [6]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history )","title":"Training the Model"},{"location":"getting-started/#generating-predictions","text":"Having our trained model we can now get some samples from the test set and generate some predictions. First we will just pick some random samples using numpy : In [7]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) x_sample = X_test [ idxs ] Here we selected 9 random images. Now we can use the predict method to get their labels: In [8]: y_pred = model . predict ( x = x_sample ) Easy right? Finally lets plot the results to see if they are accurate. In [9]: plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( x_sample [ k ], cmap = \"gray\" ) Perfect!","title":"Generating Predictions"},{"location":"getting-started/#serialization","text":"To serialize the Model you can just use the model.save(...) method, this will create a folder with some files that contain the model's code plus all parameters and states. However, here we don't need to do that since we previously added the elegy.callbacks.ModelCheckpoint callback on fit which periodically does this for us during training. We configured ModelCheckpoint to save our model to a folder called \"model\" so we can just load it from there using elegy.model.load . Lets get a new model reference containing the same weights and call its evaluate method to verify everything loaded correctly: In [10]: # current model reference print ( \"current model id:\" , id ( model )) # load model from disk model = elegy . model . load ( \"model\" ) # new model reference print ( \"new model id: \" , id ( model )) # check that it works! model . evaluate ( x = X_test , y = y_test ) current model id: 140137340602160 new model id: 140136071352432 /data/cristian/elegy/.venv/lib/python3.8/site-packages/jax/numpy/lax_numpy.py:1531: FutureWarning: jax.numpy reductions won't accept lists and tuples in future versions, only scalars and ndarrays warnings.warn(msg, category=FutureWarning) 313/313 [==============================] - 1s 3ms/step - l2_regularization_loss: 0.0450 - loss: 0.1013 - sparse_categorical_accuracy: 0.9825 - sparse_categorical_crossentropy_loss: 0.0563 Out[10]: {'l2_regularization_loss': array(0.04499801, dtype=float32), 'loss': array(0.10126135, dtype=float32), 'sparse_categorical_accuracy': array(0.9825001, dtype=float32), 'sparse_categorical_crossentropy_loss': array(0.05626357, dtype=float32), 'size': 32} Excellent! We hope you've enjoyed this tutorial.","title":"Serialization"},{"location":"getting-started/#next-steps","text":"Elegy is still in a very early stage, there are probably tons of bugs and missing features but we will get there. If you have some ideas or feedback on the current design we are eager to hear from you, feel free to open an issue.","title":"Next Steps"},{"location":"api/Loss/","text":"elegy.Loss Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/loss.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/loss.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/loss.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/loss.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/loss.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/loss.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/loss.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/loss.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Loss"},{"location":"api/Loss/#elegyloss","text":"","title":"elegy.Loss"},{"location":"api/Loss/#elegy.losses.loss.Loss","text":"Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this.","title":"elegy.losses.loss.Loss"},{"location":"api/Loss/#elegy.losses.loss.Loss.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/Loss/#elegy.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/loss.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"__init__()"},{"location":"api/Loss/#elegy.losses.loss.Loss.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/loss.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/Loss/#elegy.losses.loss.Loss.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/loss.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/Loss/#elegy.losses.loss.Loss.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/loss.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/Loss/#elegy.losses.loss.Loss.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/loss.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/Loss/#elegy.losses.loss.Loss.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/loss.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/Loss/#elegy.losses.loss.Loss.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/loss.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/Loss/#elegy.losses.loss.Loss.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/loss.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/Metric/","text":"elegy.Metric Encapsulates metric logic and state. Usage: m = SomeMetric ( ... ) for input in ... : result = m ( input ) print ( 'Final result: ' , result ) Usage with the Model API: class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), ], metrics = [ elegy . metrics . SparseCategoricalAccuracy () ], optimizer = optax . rmsprop ( 1e-3 ), ) To be implemented by subclasses: call() : All state variables should be created in this method by calling haiku.get_state() , update this state by calling haiku.set_state(...) , and return a result based on these states. Example subclass implementation: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = hk . get_state ( \"total\" , [], jnp . zeros ) count = hk . get_state ( \"count\" , [], jnp . zeros ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) hk . set_state ( \"total\" , total ) hk . set_state ( \"count\" , count ) return total / count submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , on = None , ** kwargs ) special Base Metric constructor. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/metrics/metric.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Base Metric constructor. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/metric.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/metric.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/metric.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/metric.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/metric.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/metric.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/metric.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Metric"},{"location":"api/Metric/#elegymetric","text":"","title":"elegy.Metric"},{"location":"api/Metric/#elegy.metrics.metric.Metric","text":"Encapsulates metric logic and state. Usage: m = SomeMetric ( ... ) for input in ... : result = m ( input ) print ( 'Final result: ' , result ) Usage with the Model API: class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), ], metrics = [ elegy . metrics . SparseCategoricalAccuracy () ], optimizer = optax . rmsprop ( 1e-3 ), ) To be implemented by subclasses: call() : All state variables should be created in this method by calling haiku.get_state() , update this state by calling haiku.set_state(...) , and return a result based on these states. Example subclass implementation: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = hk . get_state ( \"total\" , [], jnp . zeros ) count = hk . get_state ( \"count\" , [], jnp . zeros ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) hk . set_state ( \"total\" , total ) hk . set_state ( \"count\" , count ) return total / count","title":"elegy.metrics.metric.Metric"},{"location":"api/Metric/#elegy.metrics.metric.Metric.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/Metric/#elegy.metrics.metric.Metric.__init__","text":"Base Metric constructor. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/metrics/metric.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Base Metric constructor. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"__init__()"},{"location":"api/Metric/#elegy.metrics.metric.Metric.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/metric.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/Metric/#elegy.metrics.metric.Metric.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/metric.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/Metric/#elegy.metrics.metric.Metric.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/metric.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/Metric/#elegy.metrics.metric.Metric.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/metric.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/Metric/#elegy.metrics.metric.Metric.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/metric.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/Metric/#elegy.metrics.metric.Metric.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/metric.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/Metric/#elegy.metrics.metric.Metric.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/metric.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/Model/","text":"elegy.Model Model is tasked with performing training, evaluation, and inference for a given elegy.Module or haiku.Module . To create a Model you first have to define its architecture in a Module : class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) Then you can pass this Module to the Model 's constructor and specify additional things like losses, metrics, optimizer, and callbacks: model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . rmsprop ( 1e-3 ), ) Once the model is created, you can train the model with model.fit() , or use the model to do prediction with model.predict() . Checkout Getting Started for additional details. Attributes: Name Type Description parameters A haiku.Params structure with the weights of the model. states A haiku.State structure with non-trainable parameters of the model. optimizer_state Optional[NamedTuple] A optax.OptState structure with states of the optimizer. metrics_states A haiku.State structure with the states of the metrics. initial_metrics_state Optional[Dict] A haiku.State structure with the initial states of the metrics. run_eagerly bool Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code, instead of using Jax's jit to optimize the computation. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. seed: ndarray property writable Current random states of the model. evaluate ( self , x , y = None , verbose = 1 , batch_size = None , sample_weight = None , steps = None , callbacks = None ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required y Optional[Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 1 batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None steps Optional[int] Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . This argument is not supported with array inputs. None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.Model.fit]. Returns: Type Description Dict[str, numpy.ndarray] A dictionary for mapping the losses and metrics names to the values obtained. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model.py 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 def evaluate ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], y : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. This argument is not supported with array inputs. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.Model.fit]. Returns: A dictionary for mapping the losses and metrics names to the values obtained. Raises: ValueError: in case of invalid arguments. \"\"\" data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , training = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . test_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 1 , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 ) Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable]] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for generator type is given below. None y Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator. 0.0 validation_data Optional[Union[Tuple, Iterable]] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy/Jax arrays, list of arrays or mappings tuple (x_val, y_val, val_sample_weights) of Numpy/Jax arrays, list of arrays or mappings generator For the first two cases, batch_size must be provided. For the last case, validation_steps should be provided, and should follow the same convention for yielding data as x . Note that validation_data does not support all the data types that are supported in x , eg, dict. None shuffle bool Boolean (whether to shuffle the training data before each epoch). This argument is ignored when x is a generator. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of generators (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description History A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model.py 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 def fit ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , None , ] = None , y : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ np . ndarray ] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , ) -> History : \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for generator type is given below. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a generator. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy/Jax arrays, list of arrays or mappings - tuple `(x_val, y_val, val_sample_weights)` of Numpy/Jax arrays, list of arrays or mappings - generator For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` should be provided, and should follow the same convention for yielding data as `x`. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict. shuffle: Boolean (whether to shuffle the training data before each epoch). This argument is ignored when `x` is a generator. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of generators (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if x is None : x = {} if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) # sample_weight = batch[2] if len(batch) == 3 else None x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . train_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) epoch_logs = copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) # print( # f\"epoch: {epoch} - \" # + \" - \".join(f\"{key}: {value:.3f}\" for key, value in epoch_logs.items()) # ) if self . stop_training : break callbacks . on_train_end () return self . history load ( self , path ) Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the haiku.Params + haiku.State structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Source code in elegy/model.py 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 def load ( self , path : tp . Union [ str , Path ]) -> None : \"\"\" Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the `haiku.Params` + `haiku.State` structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Arguments: path: path to a saved model's directory. \"\"\" if isinstance ( path , str ): path = Path ( path ) states : tp . Dict = deepdish . io . load ( path / \"parameters.h5\" ) optimizer_state_path = path / \"optimizer_state.pkl\" if optimizer_state_path . exists (): with open ( optimizer_state_path , \"rb\" ) as f : states [ \"optimizer_state\" ] = pickle . load ( f ) self . full_state = states predict ( self , x , verbose = 0 , batch_size = None , steps = None , callbacks = None ) Generates output predictions for the input samples. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generators (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.Model.fit]. Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description ndarray Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model.py 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 def predict ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> np . ndarray : \"\"\"Generates output predictions for the input samples. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generators (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.Model.fit]. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs , ) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. required Returns: Type Description Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Jax array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between given number of inputs and expectations of the model. Source code in elegy/model.py 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 def predict_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ] ) -> tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ]: \"\"\" Returns predictions for a single batch of samples. Arguments: x: Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. Returns: Jax array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. \"\"\" self . _maybe_initialize ( mode = Mode . predict , x = x , y = None , sample_weight = None , class_weight = None ) assert self . parameters is not None assert self . states is not None y_pred , context = self . _predict ( False , # training False , # get_summaries x = x , parameters = self . parameters , states = self . states , rng = next ( self . _rngs ), ) return y_pred save ( self , path , include_optimizer = True ) Saves the model to disk. It creates a directory that includes: The Model object instance serialized with pickle as as {path}/model.pkl , this allows you to re-instantiate the model later. The model parameters + states serialized into HDF5 as {path}/parameters.h5 . The states of the optimizer serialized with pickle as as {path}/optimizer_state.pkl , allowing to resume training exactly where you left off. We hope to use HDF5 in the future but optax states is incompatible with deepdish . This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via Model.load if the model is already instiated or elegy.model.load to load the model instance from its pickled version. import elegy model . save ( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load ( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path where model structure will be saved. required include_optimizer bool If True, save optimizer's states together. True Source code in elegy/model.py 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 def save ( self , path : tp . Union [ str , Path ], include_optimizer : bool = True ) -> None : \"\"\" Saves the model to disk. It creates a directory that includes: - The `Model` object instance serialized with `pickle` as as `{path}/model.pkl`, this allows you to re-instantiate the model later. - The model parameters + states serialized into HDF5 as `{path}/parameters.h5`. - The states of the optimizer serialized with `pickle` as as `{path}/optimizer_state.pkl`, allowing to resume training exactly where you left off. We hope to use HDF5 in the future but `optax` states is incompatible with `deepdish`. This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via `Model.load` if the model is already instiated or `elegy.model.load` to load the model instance from its pickled version. ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path where model structure will be saved. include_optimizer: If True, save optimizer's states together. \"\"\" if isinstance ( path , str ): path = Path ( path ) path . mkdir ( parents = True , exist_ok = True ) states = self . full_state original_state = copy ( states ) states . pop ( \"metrics_states\" , None ) states . pop ( \"initial_metrics_state\" , None ) optimizer_state = states . pop ( \"optimizer_state\" , None ) deepdish . io . save ( path / \"parameters.h5\" , states ) if include_optimizer and optimizer_state is not None : with open ( path / \"optimizer_state.pkl\" , \"wb\" ) as f : pickle . dump ( optimizer_state , f ) # getting pickle errors self . reset () try : path = path / \"model.pkl\" with open ( path , \"wb\" ) as f : cloudpickle . dump ( self , f ) except BaseException as e : print ( f \"Error occurred saving the model object at { path } \\n Continuing....\" ) self . full_state = original_state summary ( self , x , depth = 2 , tablefmt = 'fancy_grid' , ** tablulate_kwargs ) Prints a summary of the network. Parameters: Name Type Description Default x A sample of inputs to the network. required depth int The level number of nested level which will be showed. Information about summaries from modules deeper than depth will be aggregated together. 2 tablefmt str A string represeting the style of the table generated by tabulate . See python-tabulate for more options. 'fancy_grid' tablulate_kwargs Additional keyword arguments passed to tabulate . See python-tabulate for more options. {} Source code in elegy/model.py 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 def summary ( self , x , depth : int = 2 , tablefmt : str = \"fancy_grid\" , ** tablulate_kwargs ): \"\"\" Prints a summary of the network. Arguments: x: A sample of inputs to the network. depth: The level number of nested level which will be showed. Information about summaries from modules deeper than `depth` will be aggregated together. tablefmt: A string represeting the style of the table generated by `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. tablulate_kwargs: Additional keyword arguments passed to `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. \"\"\" self . _maybe_initialize ( mode = Mode . predict , x = x , y = None , sample_weight = None , class_weight = None , ) assert self . parameters is not None assert self . states is not None y_pred , context = self . _predict ( training = False , get_summaries = True , x = x , parameters = self . parameters , states = self . states , rng = next ( self . _rngs ), ) def format_output ( outputs ) -> str : file = StringIO () outputs = jax . tree_map ( lambda x : f \" { x . shape } {{ pad }} { x . dtype } \" , outputs ) yaml . safe_dump ( outputs , file , default_flow_style = False , indent = 2 , explicit_end = False ) return file . getvalue () . replace ( \" \\n ...\" , \"\" ) def format_size ( size ): return ( f \" { size / 1e9 : ,.1f } GB\" if size > 1e9 else f \" { size / 1e6 : ,.1f } MB\" if size > 1e6 else f \" { size / 1e3 : ,.1f } KB\" if size > 1e3 else f \" { size : , } B\" ) table : tp . List = [[ \"Inputs\" , format_output ( x ), \"0\" , \"0\" ]] for module , base_name , value in context . summaries : base_name_parts = base_name . split ( \"/\" )[ 1 :] module_depth = len ( base_name_parts ) if module_depth > depth : continue include_submodules = module_depth == depth params_count = ( module . parameters_size ( include_submodules ) if module is not None else 0 ) params_size = ( module . parameters_bytes ( include_submodules ) if module is not None else 0 ) states_count = ( module . states_size ( include_submodules ) if module is not None else 0 ) states_size = ( module . states_bytes ( include_submodules ) if module is not None else 0 ) class_name = f \"( { module . __class__ . __name__ } )\" if module is not None else \"\" base_name = \"/\" . join ( base_name_parts ) if not base_name : base_name = \"Outputs\" table . append ( [ f \" { base_name } {{ pad }} { class_name } \" , format_output ( value ), f \" { params_count : , } {{ pad }} { format_size ( params_size ) } \" if params_count > 0 else \"0\" , f \" { states_count : , } {{ pad }} { format_size ( states_size ) } \" if states_count > 0 else \"0\" , ] ) # add papdding for col in range ( 4 ): max_length = max ( len ( line . split ( \" {pad} \" )[ 0 ]) for row in table for line in row [ col ] . split ( \" \\n \" ) ) for row in table : row [ col ] = \" \\n \" . join ( line . format ( pad = \" \" * ( max_length - len ( line . rstrip () . split ( \" {pad} \" )[ 0 ])) ) for line in row [ col ] . rstrip () . split ( \" \\n \" ) ) print ( \" \\n \" + tabulate ( table , headers = [ \"Layer\" , \"Outputs Shape\" , \"Trainable \\n Parameters\" , \"Non-trainable \\n Parameters\" , ], tablefmt = tablefmt , ** tablulate_kwargs , ) ) params_count = self . module . parameters_size () params_size = self . module . parameters_bytes () states_count = self . module . states_size () states_size = self . module . states_bytes () total_count = params_count + states_count total_size = params_size + states_size print ( tabulate ( [ [ f \"Total Parameters:\" , f \" { total_count : , } \" , f \" { format_size ( total_size ) } \" if total_count > 0 else \"\" , ], [ f \"Trainable Parameters:\" , f \" { params_count : , } \" , f \" { format_size ( params_size ) } \" if params_count > 0 else \"\" , ], [ f \"Non-trainable Parameters:\" , f \" { states_count : , } \" , f \" { format_size ( states_size ) } \" if states_count > 0 else \"\" , ], ], tablefmt = \"plain\" , ) + \" \\n \" ) test_on_batch ( self , x , y = None , sample_weight = None , class_weight = None ) Test the model on a single batch of samples. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). None sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None Returns: Type Description Dict[str, jax.numpy.lax_numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model.py 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 def test_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ jnp . ndarray ] = None , class_weight : tp . Optional [ jnp . ndarray ] = None , ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . _maybe_initialize ( mode = Mode . test , x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , ) assert self . parameters is not None assert self . states is not None ( logs , metrics_states ) = self . _test ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , parameters = self . parameters , states = self . states , metrics_states = self . metrics_states , rng = next ( self . _rngs ), ) if metrics_states is not None : self . metrics_states = metrics_states # logs = jax.tree_map(np.asarray, logs) return logs train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None ) Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). It should be consistent with x (you cannot have Numpy inputs and array targets, or inversely). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight Optional[numpy.ndarray] Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None Returns: Type Description Dict[str, numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model.py 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def train_on_batch ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ np . ndarray ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\" Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). It should be consistent with `x` (you cannot have Numpy inputs and array targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . _maybe_initialize ( mode = Mode . train , x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , ) assert self . parameters is not None assert self . states is not None assert self . optimizer_state is not None ( logs , self . parameters , self . states , self . optimizer_state , metrics_states , ) = self . _update ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , parameters = self . parameters , states = self . states , optimizer_state = self . optimizer_state , metrics_states = self . metrics_states , rng = next ( self . _rngs ), ) if metrics_states is not None : self . metrics_states = metrics_states # logs = jax.tree_map(np.asarray, logs) return logs","title":"Model"},{"location":"api/Model/#elegymodel","text":"","title":"elegy.Model"},{"location":"api/Model/#elegy.model.Model","text":"Model is tasked with performing training, evaluation, and inference for a given elegy.Module or haiku.Module . To create a Model you first have to define its architecture in a Module : class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) Then you can pass this Module to the Model 's constructor and specify additional things like losses, metrics, optimizer, and callbacks: model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . rmsprop ( 1e-3 ), ) Once the model is created, you can train the model with model.fit() , or use the model to do prediction with model.predict() . Checkout Getting Started for additional details. Attributes: Name Type Description parameters A haiku.Params structure with the weights of the model. states A haiku.State structure with non-trainable parameters of the model. optimizer_state Optional[NamedTuple] A optax.OptState structure with states of the optimizer. metrics_states A haiku.State structure with the states of the metrics. initial_metrics_state Optional[Dict] A haiku.State structure with the initial states of the metrics. run_eagerly bool Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code, instead of using Jax's jit to optimize the computation. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls.","title":"elegy.model.Model"},{"location":"api/Model/#elegy.model.Model.seed","text":"Current random states of the model.","title":"seed"},{"location":"api/Model/#elegy.model.Model.evaluate","text":"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required y Optional[Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 1 batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None steps Optional[int] Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . This argument is not supported with array inputs. None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.Model.fit]. Returns: Type Description Dict[str, numpy.ndarray] A dictionary for mapping the losses and metrics names to the values obtained. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model.py 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 def evaluate ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], y : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. This argument is not supported with array inputs. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.Model.fit]. Returns: A dictionary for mapping the losses and metrics names to the values obtained. Raises: ValueError: in case of invalid arguments. \"\"\" data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , training = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . test_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs","title":"evaluate()"},{"location":"api/Model/#elegy.model.Model.fit","text":"Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable]] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for generator type is given below. None y Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator. 0.0 validation_data Optional[Union[Tuple, Iterable]] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy/Jax arrays, list of arrays or mappings tuple (x_val, y_val, val_sample_weights) of Numpy/Jax arrays, list of arrays or mappings generator For the first two cases, batch_size must be provided. For the last case, validation_steps should be provided, and should follow the same convention for yielding data as x . Note that validation_data does not support all the data types that are supported in x , eg, dict. None shuffle bool Boolean (whether to shuffle the training data before each epoch). This argument is ignored when x is a generator. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of generators (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description History A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model.py 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 def fit ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , None , ] = None , y : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ np . ndarray ] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , ) -> History : \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for generator type is given below. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a generator. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy/Jax arrays, list of arrays or mappings - tuple `(x_val, y_val, val_sample_weights)` of Numpy/Jax arrays, list of arrays or mappings - generator For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` should be provided, and should follow the same convention for yielding data as `x`. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict. shuffle: Boolean (whether to shuffle the training data before each epoch). This argument is ignored when `x` is a generator. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of generators (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if x is None : x = {} if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) # sample_weight = batch[2] if len(batch) == 3 else None x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . train_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) epoch_logs = copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) # print( # f\"epoch: {epoch} - \" # + \" - \".join(f\"{key}: {value:.3f}\" for key, value in epoch_logs.items()) # ) if self . stop_training : break callbacks . on_train_end () return self . history","title":"fit()"},{"location":"api/Model/#elegy.model.Model.load","text":"Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the haiku.Params + haiku.State structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Source code in elegy/model.py 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 def load ( self , path : tp . Union [ str , Path ]) -> None : \"\"\" Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the `haiku.Params` + `haiku.State` structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Arguments: path: path to a saved model's directory. \"\"\" if isinstance ( path , str ): path = Path ( path ) states : tp . Dict = deepdish . io . load ( path / \"parameters.h5\" ) optimizer_state_path = path / \"optimizer_state.pkl\" if optimizer_state_path . exists (): with open ( optimizer_state_path , \"rb\" ) as f : states [ \"optimizer_state\" ] = pickle . load ( f ) self . full_state = states","title":"load()"},{"location":"api/Model/#elegy.model.Model.predict","text":"Generates output predictions for the input samples. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generators (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.Model.fit]. Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description ndarray Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model.py 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 def predict ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> np . ndarray : \"\"\"Generates output predictions for the input samples. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generators (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.Model.fit]. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs , ) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs","title":"predict()"},{"location":"api/Model/#elegy.model.Model.predict_on_batch","text":"Returns predictions for a single batch of samples. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. required Returns: Type Description Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Jax array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between given number of inputs and expectations of the model. Source code in elegy/model.py 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 def predict_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ] ) -> tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ]: \"\"\" Returns predictions for a single batch of samples. Arguments: x: Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. Returns: Jax array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. \"\"\" self . _maybe_initialize ( mode = Mode . predict , x = x , y = None , sample_weight = None , class_weight = None ) assert self . parameters is not None assert self . states is not None y_pred , context = self . _predict ( False , # training False , # get_summaries x = x , parameters = self . parameters , states = self . states , rng = next ( self . _rngs ), ) return y_pred","title":"predict_on_batch()"},{"location":"api/Model/#elegy.model.Model.save","text":"Saves the model to disk. It creates a directory that includes: The Model object instance serialized with pickle as as {path}/model.pkl , this allows you to re-instantiate the model later. The model parameters + states serialized into HDF5 as {path}/parameters.h5 . The states of the optimizer serialized with pickle as as {path}/optimizer_state.pkl , allowing to resume training exactly where you left off. We hope to use HDF5 in the future but optax states is incompatible with deepdish . This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via Model.load if the model is already instiated or elegy.model.load to load the model instance from its pickled version. import elegy model . save ( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load ( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path where model structure will be saved. required include_optimizer bool If True, save optimizer's states together. True Source code in elegy/model.py 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 def save ( self , path : tp . Union [ str , Path ], include_optimizer : bool = True ) -> None : \"\"\" Saves the model to disk. It creates a directory that includes: - The `Model` object instance serialized with `pickle` as as `{path}/model.pkl`, this allows you to re-instantiate the model later. - The model parameters + states serialized into HDF5 as `{path}/parameters.h5`. - The states of the optimizer serialized with `pickle` as as `{path}/optimizer_state.pkl`, allowing to resume training exactly where you left off. We hope to use HDF5 in the future but `optax` states is incompatible with `deepdish`. This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via `Model.load` if the model is already instiated or `elegy.model.load` to load the model instance from its pickled version. ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path where model structure will be saved. include_optimizer: If True, save optimizer's states together. \"\"\" if isinstance ( path , str ): path = Path ( path ) path . mkdir ( parents = True , exist_ok = True ) states = self . full_state original_state = copy ( states ) states . pop ( \"metrics_states\" , None ) states . pop ( \"initial_metrics_state\" , None ) optimizer_state = states . pop ( \"optimizer_state\" , None ) deepdish . io . save ( path / \"parameters.h5\" , states ) if include_optimizer and optimizer_state is not None : with open ( path / \"optimizer_state.pkl\" , \"wb\" ) as f : pickle . dump ( optimizer_state , f ) # getting pickle errors self . reset () try : path = path / \"model.pkl\" with open ( path , \"wb\" ) as f : cloudpickle . dump ( self , f ) except BaseException as e : print ( f \"Error occurred saving the model object at { path } \\n Continuing....\" ) self . full_state = original_state","title":"save()"},{"location":"api/Model/#elegy.model.Model.summary","text":"Prints a summary of the network. Parameters: Name Type Description Default x A sample of inputs to the network. required depth int The level number of nested level which will be showed. Information about summaries from modules deeper than depth will be aggregated together. 2 tablefmt str A string represeting the style of the table generated by tabulate . See python-tabulate for more options. 'fancy_grid' tablulate_kwargs Additional keyword arguments passed to tabulate . See python-tabulate for more options. {} Source code in elegy/model.py 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 def summary ( self , x , depth : int = 2 , tablefmt : str = \"fancy_grid\" , ** tablulate_kwargs ): \"\"\" Prints a summary of the network. Arguments: x: A sample of inputs to the network. depth: The level number of nested level which will be showed. Information about summaries from modules deeper than `depth` will be aggregated together. tablefmt: A string represeting the style of the table generated by `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. tablulate_kwargs: Additional keyword arguments passed to `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. \"\"\" self . _maybe_initialize ( mode = Mode . predict , x = x , y = None , sample_weight = None , class_weight = None , ) assert self . parameters is not None assert self . states is not None y_pred , context = self . _predict ( training = False , get_summaries = True , x = x , parameters = self . parameters , states = self . states , rng = next ( self . _rngs ), ) def format_output ( outputs ) -> str : file = StringIO () outputs = jax . tree_map ( lambda x : f \" { x . shape } {{ pad }} { x . dtype } \" , outputs ) yaml . safe_dump ( outputs , file , default_flow_style = False , indent = 2 , explicit_end = False ) return file . getvalue () . replace ( \" \\n ...\" , \"\" ) def format_size ( size ): return ( f \" { size / 1e9 : ,.1f } GB\" if size > 1e9 else f \" { size / 1e6 : ,.1f } MB\" if size > 1e6 else f \" { size / 1e3 : ,.1f } KB\" if size > 1e3 else f \" { size : , } B\" ) table : tp . List = [[ \"Inputs\" , format_output ( x ), \"0\" , \"0\" ]] for module , base_name , value in context . summaries : base_name_parts = base_name . split ( \"/\" )[ 1 :] module_depth = len ( base_name_parts ) if module_depth > depth : continue include_submodules = module_depth == depth params_count = ( module . parameters_size ( include_submodules ) if module is not None else 0 ) params_size = ( module . parameters_bytes ( include_submodules ) if module is not None else 0 ) states_count = ( module . states_size ( include_submodules ) if module is not None else 0 ) states_size = ( module . states_bytes ( include_submodules ) if module is not None else 0 ) class_name = f \"( { module . __class__ . __name__ } )\" if module is not None else \"\" base_name = \"/\" . join ( base_name_parts ) if not base_name : base_name = \"Outputs\" table . append ( [ f \" { base_name } {{ pad }} { class_name } \" , format_output ( value ), f \" { params_count : , } {{ pad }} { format_size ( params_size ) } \" if params_count > 0 else \"0\" , f \" { states_count : , } {{ pad }} { format_size ( states_size ) } \" if states_count > 0 else \"0\" , ] ) # add papdding for col in range ( 4 ): max_length = max ( len ( line . split ( \" {pad} \" )[ 0 ]) for row in table for line in row [ col ] . split ( \" \\n \" ) ) for row in table : row [ col ] = \" \\n \" . join ( line . format ( pad = \" \" * ( max_length - len ( line . rstrip () . split ( \" {pad} \" )[ 0 ])) ) for line in row [ col ] . rstrip () . split ( \" \\n \" ) ) print ( \" \\n \" + tabulate ( table , headers = [ \"Layer\" , \"Outputs Shape\" , \"Trainable \\n Parameters\" , \"Non-trainable \\n Parameters\" , ], tablefmt = tablefmt , ** tablulate_kwargs , ) ) params_count = self . module . parameters_size () params_size = self . module . parameters_bytes () states_count = self . module . states_size () states_size = self . module . states_bytes () total_count = params_count + states_count total_size = params_size + states_size print ( tabulate ( [ [ f \"Total Parameters:\" , f \" { total_count : , } \" , f \" { format_size ( total_size ) } \" if total_count > 0 else \"\" , ], [ f \"Trainable Parameters:\" , f \" { params_count : , } \" , f \" { format_size ( params_size ) } \" if params_count > 0 else \"\" , ], [ f \"Non-trainable Parameters:\" , f \" { states_count : , } \" , f \" { format_size ( states_size ) } \" if states_count > 0 else \"\" , ], ], tablefmt = \"plain\" , ) + \" \\n \" )","title":"summary()"},{"location":"api/Model/#elegy.model.Model.test_on_batch","text":"Test the model on a single batch of samples. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). None sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None Returns: Type Description Dict[str, jax.numpy.lax_numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model.py 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 def test_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ jnp . ndarray ] = None , class_weight : tp . Optional [ jnp . ndarray ] = None , ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . _maybe_initialize ( mode = Mode . test , x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , ) assert self . parameters is not None assert self . states is not None ( logs , metrics_states ) = self . _test ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , parameters = self . parameters , states = self . states , metrics_states = self . metrics_states , rng = next ( self . _rngs ), ) if metrics_states is not None : self . metrics_states = metrics_states # logs = jax.tree_map(np.asarray, logs) return logs","title":"test_on_batch()"},{"location":"api/Model/#elegy.model.Model.train_on_batch","text":"Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). It should be consistent with x (you cannot have Numpy inputs and array targets, or inversely). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight Optional[numpy.ndarray] Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None Returns: Type Description Dict[str, numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model.py 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def train_on_batch ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ np . ndarray ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\" Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). It should be consistent with `x` (you cannot have Numpy inputs and array targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . _maybe_initialize ( mode = Mode . train , x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , ) assert self . parameters is not None assert self . states is not None assert self . optimizer_state is not None ( logs , self . parameters , self . states , self . optimizer_state , metrics_states , ) = self . _update ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , parameters = self . parameters , states = self . states , optimizer_state = self . optimizer_state , metrics_states = self . metrics_states , rng = next ( self . _rngs ), ) if metrics_states is not None : self . metrics_states = metrics_states # logs = jax.tree_map(np.asarray, logs) return logs","title":"train_on_batch()"},{"location":"api/Module/","text":"elegy.Module Basic Elegy Module. Its a thin wrapper around hk.Module that add custom functionalities related to Elegy. submodules: Dict [ str , Any ] property readonly A dictionary with all submodules contained in this Module. __init__ ( self , name = None , dtype =< class ' jax . numpy . lax_numpy . float32 '>) special Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Parameters: Name Type Description Default name Optional[str] An optional string name for the class. Must be a valid elsePython identifier. If name is not provided then the class name for the current instance is converted to lower_snake_case and used instead. None Source code in elegy/module.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : np . dtype = jnp . float32 ): \"\"\" Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Arguments: name: An optional string name for the class. Must be a valid elsePython identifier. If ``name`` is not provided then the class name for the current instance is converted to ``lower_snake_case`` and used instead. \"\"\" self . name = name if name else utils . lower_snake_case ( self . __class__ . __name__ ) self . dtype = dtype self . _params = set () self . _states = set () self . _submodules = set () self . _dynamic_submodules = [] self . _ignore = set () apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/module.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn get_parameters ( self ) Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/module.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/module.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/module.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/module.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/module.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/module.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Module"},{"location":"api/Module/#elegymodule","text":"","title":"elegy.Module"},{"location":"api/Module/#elegy.module.Module","text":"Basic Elegy Module. Its a thin wrapper around hk.Module that add custom functionalities related to Elegy.","title":"elegy.module.Module"},{"location":"api/Module/#elegy.module.Module.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/Module/#elegy.module.Module.__init__","text":"Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Parameters: Name Type Description Default name Optional[str] An optional string name for the class. Must be a valid elsePython identifier. If name is not provided then the class name for the current instance is converted to lower_snake_case and used instead. None Source code in elegy/module.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : np . dtype = jnp . float32 ): \"\"\" Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Arguments: name: An optional string name for the class. Must be a valid elsePython identifier. If ``name`` is not provided then the class name for the current instance is converted to ``lower_snake_case`` and used instead. \"\"\" self . name = name if name else utils . lower_snake_case ( self . __class__ . __name__ ) self . dtype = dtype self . _params = set () self . _states = set () self . _submodules = set () self . _dynamic_submodules = [] self . _ignore = set ()","title":"__init__()"},{"location":"api/Module/#elegy.module.Module.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/module.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/Module/#elegy.module.Module.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/module.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/Module/#elegy.module.Module.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/module.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/Module/#elegy.module.Module.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/module.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/Module/#elegy.module.Module.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/module.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/Module/#elegy.module.Module.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/module.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/Module/#elegy.module.Module.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/module.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/PRNGSequence/","text":"elegy.PRNGSequence","title":"PRNGSequence"},{"location":"api/PRNGSequence/#elegyprngsequence","text":"","title":"elegy.PRNGSequence"},{"location":"api/PRNGSequence/#elegy.module.PRNGSequence","text":"","title":"elegy.module.PRNGSequence"},{"location":"api/add_loss/","text":"elegy.add_loss A hook that lets you define a loss within a [ module ][elegy.module.Module]. w = elegy . get_parameter ( \"w\" , [ 3 , 5 ], initializer = jnp . ones ) # L2 regularization penalty elegy . add_loss ( \"l2_regularization\" , 0.01 * jnp . mean ( w ** 2 )) The loss will be aggregated by [ Module.apply ][elegy.module.Module.apply] and automatically handled by [ Model ][elegy.model.Model]. Parameters: Name Type Description Default name str The name of the loss. If a name is repeated on different calls values will be added together. required value ndarray The value for the loss. required Source code in elegy/hooks.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def add_loss ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a loss within a [`module`][elegy.module.Module]. ```python w = elegy.get_parameter(\"w\", [3, 5], initializer=jnp.ones) # L2 regularization penalty elegy.add_loss(\"l2_regularization\", 0.01 * jnp.mean(w ** 2)) ``` The loss will be aggregated by [`Module.apply`][elegy.module.Module.apply] and automatically handled by [`Model`][elegy.model.Model]. Arguments: name: The name of the loss. If a `name` is repeated on different calls values will be added together. value: The value for the loss. \"\"\" if not name . endswith ( \"loss\" ): name += \"_loss\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] if name in context . losses : context . losses [ name ] += value else : context . losses [ name ] = value else : raise ValueError ( \"Cannot execute `add_loss` outside of an `elegy.context`\" )","title":"add_loss"},{"location":"api/add_loss/#elegyadd_loss","text":"","title":"elegy.add_loss"},{"location":"api/add_loss/#elegy.hooks.add_loss","text":"A hook that lets you define a loss within a [ module ][elegy.module.Module]. w = elegy . get_parameter ( \"w\" , [ 3 , 5 ], initializer = jnp . ones ) # L2 regularization penalty elegy . add_loss ( \"l2_regularization\" , 0.01 * jnp . mean ( w ** 2 )) The loss will be aggregated by [ Module.apply ][elegy.module.Module.apply] and automatically handled by [ Model ][elegy.model.Model]. Parameters: Name Type Description Default name str The name of the loss. If a name is repeated on different calls values will be added together. required value ndarray The value for the loss. required Source code in elegy/hooks.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def add_loss ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a loss within a [`module`][elegy.module.Module]. ```python w = elegy.get_parameter(\"w\", [3, 5], initializer=jnp.ones) # L2 regularization penalty elegy.add_loss(\"l2_regularization\", 0.01 * jnp.mean(w ** 2)) ``` The loss will be aggregated by [`Module.apply`][elegy.module.Module.apply] and automatically handled by [`Model`][elegy.model.Model]. Arguments: name: The name of the loss. If a `name` is repeated on different calls values will be added together. value: The value for the loss. \"\"\" if not name . endswith ( \"loss\" ): name += \"_loss\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] if name in context . losses : context . losses [ name ] += value else : context . losses [ name ] = value else : raise ValueError ( \"Cannot execute `add_loss` outside of an `elegy.context`\" )","title":"elegy.hooks.add_loss"},{"location":"api/add_metric/","text":"elegy.add_metric A hook that lets you define a metric within a [ module ][elegy.module.Module]. y = jax . nn . relu ( x ) elegy . add_metric ( \"activation_mean\" , jnp . mean ( y )) The metrics will be aggregated by [ Module.apply ][elegy.module.Module.apply] and automatically handled by [ Model ][elegy.model.Model]. Parameters: Name Type Description Default name str The name of the loss. If a metric with the same name already exists a unique identifier will be generated. required value ndarray The value for the metric. required Source code in elegy/hooks.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def add_metric ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a metric within a [`module`][elegy.module.Module]. ```python y = jax.nn.relu(x) elegy.add_metric(\"activation_mean\", jnp.mean(y)) ``` The metrics will be aggregated by [`Module.apply`][elegy.module.Module.apply] and automatically handled by [`Model`][elegy.model.Model]. Arguments: name: The name of the loss. If a metric with the same `name` already exists a unique identifier will be generated. value: The value for the metric. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] base_name = \"/\" . join ( context . path_names_c ) name = f \" { base_name } / { name } \" name = get_unique_name ( context . metrics , name ) context . metrics [ name ] = value else : raise ValueError ( \"Cannot execute `add_metric` outside of an `elegy.context`\" )","title":"add_metric"},{"location":"api/add_metric/#elegyadd_metric","text":"","title":"elegy.add_metric"},{"location":"api/add_metric/#elegy.hooks.add_metric","text":"A hook that lets you define a metric within a [ module ][elegy.module.Module]. y = jax . nn . relu ( x ) elegy . add_metric ( \"activation_mean\" , jnp . mean ( y )) The metrics will be aggregated by [ Module.apply ][elegy.module.Module.apply] and automatically handled by [ Model ][elegy.model.Model]. Parameters: Name Type Description Default name str The name of the loss. If a metric with the same name already exists a unique identifier will be generated. required value ndarray The value for the metric. required Source code in elegy/hooks.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def add_metric ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a metric within a [`module`][elegy.module.Module]. ```python y = jax.nn.relu(x) elegy.add_metric(\"activation_mean\", jnp.mean(y)) ``` The metrics will be aggregated by [`Module.apply`][elegy.module.Module.apply] and automatically handled by [`Model`][elegy.model.Model]. Arguments: name: The name of the loss. If a metric with the same `name` already exists a unique identifier will be generated. value: The value for the metric. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] base_name = \"/\" . join ( context . path_names_c ) name = f \" { base_name } / { name } \" name = get_unique_name ( context . metrics , name ) context . metrics [ name ] = value else : raise ValueError ( \"Cannot execute `add_metric` outside of an `elegy.context`\" )","title":"elegy.hooks.add_metric"},{"location":"api/add_summary/","text":"elegy.add_summary A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so Model.summary() can show a representation of architecture. def call ( self , x ): ... y = jax . nn . relu ( x ) elegy . add_summary ( \"relu\" , y ) ... The summaries will be aggregated by [ apply ][elegy.module.Module.apply] if get_summaries is set to True , else this hook does nothing. transformed_state = transform . apply ( ... , get_summaries = True , ... ) Parameters: Name Type Description Default name Optional[str] The name of the loss. If a summary with the same name already exists a unique identifier will be generated. required value ndarray The value for the summary. required Source code in elegy/hooks.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def add_summary ( name : tp . Optional [ str ], value : np . ndarray ) -> None : \"\"\" A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so `Model.summary()` can show a representation of architecture. ```python def call(self, x): ... y = jax.nn.relu(x) elegy.add_summary(\"relu\", y) ... ``` The summaries will be aggregated by [`apply`][elegy.module.Module.apply] if `get_summaries` is set to `True`, else this hook does nothing. ```python transformed_state = transform.apply(..., get_summaries=True, ...) ``` Arguments: name: The name of the loss. If a summary with the same `name` already exists a unique identifier will be generated. value: The value for the summary. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if not context . get_summaries : return # name = level_names[module] base_name = \"/\" . join ( context . path_names_c ) base_name = f \" { base_name } / { name } \" if name is not None else base_name base_name = get_unique_name ( context . summaries , base_name ) module = module if name is None else None # pass module only if name is None context . summaries . append (( module , base_name , value )) else : raise ValueError ( \"Cannot execute `add_summary` outside of an `elegy.context`\" )","title":"add_summary"},{"location":"api/add_summary/#elegyadd_summary","text":"","title":"elegy.add_summary"},{"location":"api/add_summary/#elegy.hooks.add_summary","text":"A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so Model.summary() can show a representation of architecture. def call ( self , x ): ... y = jax . nn . relu ( x ) elegy . add_summary ( \"relu\" , y ) ... The summaries will be aggregated by [ apply ][elegy.module.Module.apply] if get_summaries is set to True , else this hook does nothing. transformed_state = transform . apply ( ... , get_summaries = True , ... ) Parameters: Name Type Description Default name Optional[str] The name of the loss. If a summary with the same name already exists a unique identifier will be generated. required value ndarray The value for the summary. required Source code in elegy/hooks.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def add_summary ( name : tp . Optional [ str ], value : np . ndarray ) -> None : \"\"\" A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so `Model.summary()` can show a representation of architecture. ```python def call(self, x): ... y = jax.nn.relu(x) elegy.add_summary(\"relu\", y) ... ``` The summaries will be aggregated by [`apply`][elegy.module.Module.apply] if `get_summaries` is set to `True`, else this hook does nothing. ```python transformed_state = transform.apply(..., get_summaries=True, ...) ``` Arguments: name: The name of the loss. If a summary with the same `name` already exists a unique identifier will be generated. value: The value for the summary. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if not context . get_summaries : return # name = level_names[module] base_name = \"/\" . join ( context . path_names_c ) base_name = f \" { base_name } / { name } \" if name is not None else base_name base_name = get_unique_name ( context . summaries , base_name ) module = module if name is None else None # pass module only if name is None context . summaries . append (( module , base_name , value )) else : raise ValueError ( \"Cannot execute `add_summary` outside of an `elegy.context`\" )","title":"elegy.hooks.add_summary"},{"location":"api/get_parameter/","text":"elegy.get_parameter A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fbb5accf1f0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/hooks.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_parameter ( name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if not hasattr ( module , name ): if not context . building : raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) module . _params . add ( name ) if dtype is None : dtype = module . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( module , name , initial_value ) elif name not in module . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( module , name ) return value else : raise ValueError ( \"Cannot execute `get_parameter` outside of a `elegy.context`\" )","title":"get_parameter"},{"location":"api/get_parameter/#elegyget_parameter","text":"","title":"elegy.get_parameter"},{"location":"api/get_parameter/#elegy.hooks.get_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fbb5accf1f0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/hooks.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_parameter ( name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if not hasattr ( module , name ): if not context . building : raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) module . _params . add ( name ) if dtype is None : dtype = module . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( module , name , initial_value ) elif name not in module . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( module , name ) return value else : raise ValueError ( \"Cannot execute `get_parameter` outside of a `elegy.context`\" )","title":"elegy.hooks.get_parameter"},{"location":"api/get_state/","text":"elegy.get_state A hook that lets you add a state to the current module. The state will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the state. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the state. () dtype Optional[numpy.dtype] The type of the state. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fbb5accf1f0> Returns: Type Description Any The value of the state. Source code in elegy/hooks.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def get_state ( name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , ) -> tp . Any : \"\"\" A hook that lets you add a state to the current module. The state will only be created once during `init` and will reused afterwards. Arguments: name: The name of the state. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the state. dtype: The type of the state. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the state. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if not hasattr ( module , name ): if not context . building : raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) module . _states . add ( name ) initial_name = as_initial ( name ) if dtype is None : dtype = module . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( module , name , initial_value ) setattr ( module , initial_name , initial_value ) elif name not in module . _states : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the state.\" ) value = getattr ( module , name ) return value else : raise ValueError ( \"Cannot execute `get_state` outside of a `elegy.context`\" )","title":"get_state"},{"location":"api/get_state/#elegyget_state","text":"","title":"elegy.get_state"},{"location":"api/get_state/#elegy.hooks.get_state","text":"A hook that lets you add a state to the current module. The state will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the state. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the state. () dtype Optional[numpy.dtype] The type of the state. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fbb5accf1f0> Returns: Type Description Any The value of the state. Source code in elegy/hooks.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def get_state ( name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , ) -> tp . Any : \"\"\" A hook that lets you add a state to the current module. The state will only be created once during `init` and will reused afterwards. Arguments: name: The name of the state. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the state. dtype: The type of the state. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the state. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if not hasattr ( module , name ): if not context . building : raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) module . _states . add ( name ) initial_name = as_initial ( name ) if dtype is None : dtype = module . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( module , name , initial_value ) setattr ( module , initial_name , initial_value ) elif name not in module . _states : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the state.\" ) value = getattr ( module , name ) return value else : raise ValueError ( \"Cannot execute `get_state` outside of a `elegy.context`\" )","title":"elegy.hooks.get_state"},{"location":"api/next_rng_key/","text":"elegy.next_rng_key A hook that returns a unique JAX RNG key split from the current global key. key = elegy . next_rng_key () x = jax . random . uniform ( key , []) Returns: Type Description ndarray A unique (within a transformed function) JAX rng key that can be used with APIs such as jax.random.uniform . Source code in elegy/hooks.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def next_rng_key () -> PRNGKey : \"\"\" A hook that returns a unique JAX RNG key split from the current global key. ```python key = elegy.next_rng_key() x = jax.random.uniform(key, []) ``` Returns: A unique (within a transformed function) JAX rng key that can be used with APIs such as ``jax.random.uniform``. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] if context . rng_sequence is not None : context : Context = LOCAL . contexts [ - 1 ] return next ( context . rng_sequence ) else : raise ValueError ( \"Cannot execute `rng` not set in context, check init or apply.\" ) else : raise ValueError ( \"Cannot execute `next_rng_key` outside of an `elegy.context`\" )","title":"next_rng_key"},{"location":"api/next_rng_key/#elegynext_rng_key","text":"","title":"elegy.next_rng_key"},{"location":"api/next_rng_key/#elegy.hooks.next_rng_key","text":"A hook that returns a unique JAX RNG key split from the current global key. key = elegy . next_rng_key () x = jax . random . uniform ( key , []) Returns: Type Description ndarray A unique (within a transformed function) JAX rng key that can be used with APIs such as jax.random.uniform . Source code in elegy/hooks.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def next_rng_key () -> PRNGKey : \"\"\" A hook that returns a unique JAX RNG key split from the current global key. ```python key = elegy.next_rng_key() x = jax.random.uniform(key, []) ``` Returns: A unique (within a transformed function) JAX rng key that can be used with APIs such as ``jax.random.uniform``. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] if context . rng_sequence is not None : context : Context = LOCAL . contexts [ - 1 ] return next ( context . rng_sequence ) else : raise ValueError ( \"Cannot execute `rng` not set in context, check init or apply.\" ) else : raise ValueError ( \"Cannot execute `next_rng_key` outside of an `elegy.context`\" )","title":"elegy.hooks.next_rng_key"},{"location":"api/set_state/","text":"elegy.set_state A hook that lets you update a state of the current module, if the state does not exist it will be created. Parameters: Name Type Description Default name str The name of the state. It must be unique and no other field/property/method of the instance can have that name. required value Any The updated value of the state. required Source code in elegy/hooks.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def set_state ( name : str , value : tp . Any ) -> None : \"\"\" A hook that lets you update a state of the current module, if the state does not exist it will be created. Arguments: name: The name of the state. It must be unique and no other field/property/method of the instance can have that name. value: The updated value of the state. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if name not in module . _states : if not context . building : raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) module . _states . add ( name ) initial_name = as_initial ( name ) setattr ( module , name , value ) setattr ( module , initial_name , value ) else : setattr ( module , name , value ) else : raise ValueError ( \"Cannot execute `set_state` outside of a `elegy.context`\" )","title":"set_state"},{"location":"api/set_state/#elegyset_state","text":"","title":"elegy.set_state"},{"location":"api/set_state/#elegy.hooks.set_state","text":"A hook that lets you update a state of the current module, if the state does not exist it will be created. Parameters: Name Type Description Default name str The name of the state. It must be unique and no other field/property/method of the instance can have that name. required value Any The updated value of the state. required Source code in elegy/hooks.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def set_state ( name : str , value : tp . Any ) -> None : \"\"\" A hook that lets you update a state of the current module, if the state does not exist it will be created. Arguments: name: The name of the state. It must be unique and no other field/property/method of the instance can have that name. value: The updated value of the state. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if name not in module . _states : if not context . building : raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) module . _states . add ( name ) initial_name = as_initial ( name ) setattr ( module , name , value ) setattr ( module , initial_name , value ) else : setattr ( module , name , value ) else : raise ValueError ( \"Cannot execute `set_state` outside of a `elegy.context`\" )","title":"elegy.hooks.set_state"},{"location":"api/to_module/","text":"elegy.to_module Source code in elegy/module.py 721 722 723 724 725 726 727 728 729 730 731 732 733 734 def to_module ( f ): class MyModule ( Module ): def __init__ ( self , name : tp . Optional [ str ] = None ): super () . __init__ ( name = utils . lower_snake_case ( f . __name__ ) if name is None else name ) self . call = f def call ( self , * args , ** kwargs ): ... MyModule . __name__ = f . __name__ return MyModule","title":"to_module"},{"location":"api/to_module/#elegyto_module","text":"","title":"elegy.to_module"},{"location":"api/to_module/#elegy.module.to_module","text":"Source code in elegy/module.py 721 722 723 724 725 726 727 728 729 730 731 732 733 734 def to_module ( f ): class MyModule ( Module ): def __init__ ( self , name : tp . Optional [ str ] = None ): super () . __init__ ( name = utils . lower_snake_case ( f . __name__ ) if name is None else name ) self . call = f def call ( self , * args , ** kwargs ): ... MyModule . __name__ = f . __name__ return MyModule","title":"elegy.module.to_module"},{"location":"api/callbacks/CSVLogger/","text":"elegy.callbacks.CSVLogger Callback that streams epoch results to a csv file. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . Examples: csv_logger = CSVLogger ( 'training.log' ) model . fit ( X_train , Y_train , callbacks = [ csv_logger ]) on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/csv_logger.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} def handle_value ( k ): is_zero_dim_ndarray = isinstance ( k , np . ndarray ) and k . ndim == 0 if isinstance ( k , six . string_types ): return k elif isinstance ( k , tp . Iterable ) and not is_zero_dim_ndarray : return '\"[ %s ]\"' % ( \", \" . join ( map ( str , k ))) else : return k if self . keys is None : self . keys = sorted ( logs . keys ()) if self . model . stop_training : # We set NA so that csv parsers do not fail for this last epoch. logs = dict ([( k , logs [ k ]) if k in logs else ( k , \"NA\" ) for k in self . keys ]) if not self . writer : class CustomDialect ( csv . excel ): delimiter = self . sep fieldnames = [ \"epoch\" ] + self . keys self . writer = csv . DictWriter ( self . csv_file , fieldnames = fieldnames , dialect = CustomDialect ) if self . append_header : self . writer . writeheader () row_dict = collections . OrderedDict ({ \"epoch\" : epoch }) row_dict . update (( key , handle_value ( logs [ key ])) for key in self . keys ) self . writer . writerow ( row_dict ) self . csv_file . flush () on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 48 49 50 51 52 53 54 55 56 57 58 def on_train_begin ( self , logs = None ): if self . append : if os . path . exists ( self . filename ): with open ( self . filename , \"r\" + self . file_flags ) as f : self . append_header = not bool ( len ( f . readline ())) mode = \"a\" else : mode = \"w\" self . csv_file = io . open ( self . filename , mode + self . file_flags , ** self . _open_args ) on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 97 98 99 def on_train_end ( self , logs = None ): self . csv_file . close () self . writer = None","title":"CSVLogger"},{"location":"api/callbacks/CSVLogger/#elegycallbackscsvlogger","text":"","title":"elegy.callbacks.CSVLogger"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger","text":"Callback that streams epoch results to a csv file. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . Examples: csv_logger = CSVLogger ( 'training.log' ) model . fit ( X_train , Y_train , callbacks = [ csv_logger ])","title":"elegy.callbacks.csv_logger.CSVLogger"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/csv_logger.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} def handle_value ( k ): is_zero_dim_ndarray = isinstance ( k , np . ndarray ) and k . ndim == 0 if isinstance ( k , six . string_types ): return k elif isinstance ( k , tp . Iterable ) and not is_zero_dim_ndarray : return '\"[ %s ]\"' % ( \", \" . join ( map ( str , k ))) else : return k if self . keys is None : self . keys = sorted ( logs . keys ()) if self . model . stop_training : # We set NA so that csv parsers do not fail for this last epoch. logs = dict ([( k , logs [ k ]) if k in logs else ( k , \"NA\" ) for k in self . keys ]) if not self . writer : class CustomDialect ( csv . excel ): delimiter = self . sep fieldnames = [ \"epoch\" ] + self . keys self . writer = csv . DictWriter ( self . csv_file , fieldnames = fieldnames , dialect = CustomDialect ) if self . append_header : self . writer . writeheader () row_dict = collections . OrderedDict ({ \"epoch\" : epoch }) row_dict . update (( key , handle_value ( logs [ key ])) for key in self . keys ) self . writer . writerow ( row_dict ) self . csv_file . flush ()","title":"on_epoch_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 48 49 50 51 52 53 54 55 56 57 58 def on_train_begin ( self , logs = None ): if self . append : if os . path . exists ( self . filename ): with open ( self . filename , \"r\" + self . file_flags ) as f : self . append_header = not bool ( len ( f . readline ())) mode = \"a\" else : mode = \"w\" self . csv_file = io . open ( self . filename , mode + self . file_flags , ** self . _open_args )","title":"on_train_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py 97 98 99 def on_train_end ( self , logs = None ): self . csv_file . close () self . writer = None","title":"on_train_end()"},{"location":"api/callbacks/Callback/","text":"elegy.callbacks.Callback Abstract base class used to build new callbacks. The logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch. Currently, the .fit() method of the Model class will include the following quantities in the logs that it passes to its callbacks: on_epoch_end : logs include ` acc ` and ` loss ` , and optionally include ` val_loss ` ( if validation is enabled in ` fit ` ), and ` val_acc ` ( if validation and accuracy monitoring are enabled ) . on_train_batch_begin : logs include ` size ` , the number of samples in the current batch . on_train_batch_end : logs include ` loss ` , and optionally ` acc ` ( if accuracy monitoring is enabled ) . Attributes: Name Type Description params dict Training parameters (eg. verbosity, batch size, number of epochs...). model elegy.model.Model Reference of the model being trained. on_epoch_begin ( self , epoch , logs = None ) Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/callback.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass on_predict_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"Callback"},{"location":"api/callbacks/Callback/#elegycallbackscallback","text":"","title":"elegy.callbacks.Callback"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback","text":"Abstract base class used to build new callbacks. The logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch. Currently, the .fit() method of the Model class will include the following quantities in the logs that it passes to its callbacks: on_epoch_end : logs include ` acc ` and ` loss ` , and optionally include ` val_loss ` ( if validation is enabled in ` fit ` ), and ` val_acc ` ( if validation and accuracy monitoring are enabled ) . on_train_batch_begin : logs include ` size ` , the number of samples in the current batch . on_train_batch_end : logs include ` loss ` , and optionally ` acc ` ( if accuracy monitoring is enabled ) . Attributes: Name Type Description params dict Training parameters (eg. verbosity, batch size, number of epochs...). model elegy.model.Model Reference of the model being trained.","title":"elegy.callbacks.callback.Callback"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/callback.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass","title":"on_epoch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/CallbackList/","text":"elegy.callbacks.CallbackList Container abstracting a list of callbacks.","title":"CallbackList"},{"location":"api/callbacks/CallbackList/#elegycallbackscallbacklist","text":"","title":"elegy.callbacks.CallbackList"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList","text":"Container abstracting a list of callbacks.","title":"elegy.callbacks.callback_list.CallbackList"},{"location":"api/callbacks/EarlyStopping/","text":"elegy.callbacks.EarlyStopping Stop training when a monitored metric has stopped improving. Assuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates. The quantity to be monitored needs to be available in logs dict. To make it so, pass the loss or metrics at model.__init__() . Examples: np . random . seed ( 42 ) class MLP ( elegy . Module ): def call ( self , input ): mlp = hk . Sequential ([ hk . Linear ( 10 ),]) return mlp ( input ) callback = elegy . callbacks . EarlyStopping ( monitor = \"loss\" , patience = 3 ) # This callback will stop the training when there is no improvement in # the for three consecutive epochs. model = elegy . Model ( module = MLP (), loss = elegy . losses . MeanSquaredError (), optimizer = optax . rmsprop ( 0.01 ), ) history = model . fit ( np . arange ( 100 ) . reshape ( 5 , 20 ) . astype ( np . float32 ), np . zeros ( 5 ), epochs = 10 , batch_size = 1 , callbacks = [ callback ], verbose = 0 , ) assert len ( history . history [ \"loss\" ]) == 7 # Only 7 epochs are run. on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/early_stopping.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def on_epoch_end ( self , epoch , logs = None ): current = self . get_monitor_value ( logs ) if current is None : return if self . monitor_op ( current - self . min_delta , self . best ): self . best = current self . wait = 0 if self . restore_best_weights : # This will also save optimizer state self . best_state = self . model . full_state else : self . wait += 1 if self . wait >= self . patience : self . stopped_epoch = epoch self . model . stop_training = True if self . restore_best_weights : if self . verbose > 0 : print ( \"Restoring model weights from the end of the best epoch.\" ) self . model . full_state = self . best_state on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 122 123 124 125 126 127 128 129 def on_train_begin ( self , logs = None ): # Allow instances to be re-used self . wait = 0 self . stopped_epoch = 0 if self . baseline is not None : self . best = self . baseline else : self . best = np . Inf if self . monitor_op == np . less else - np . Inf on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 151 152 153 def on_train_end ( self , logs = None ): if self . stopped_epoch > 0 and self . verbose > 0 : print ( \"Epoch %05d : early stopping\" % ( self . stopped_epoch + 1 ))","title":"EarlyStopping"},{"location":"api/callbacks/EarlyStopping/#elegycallbacksearlystopping","text":"","title":"elegy.callbacks.EarlyStopping"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping","text":"Stop training when a monitored metric has stopped improving. Assuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates. The quantity to be monitored needs to be available in logs dict. To make it so, pass the loss or metrics at model.__init__() . Examples: np . random . seed ( 42 ) class MLP ( elegy . Module ): def call ( self , input ): mlp = hk . Sequential ([ hk . Linear ( 10 ),]) return mlp ( input ) callback = elegy . callbacks . EarlyStopping ( monitor = \"loss\" , patience = 3 ) # This callback will stop the training when there is no improvement in # the for three consecutive epochs. model = elegy . Model ( module = MLP (), loss = elegy . losses . MeanSquaredError (), optimizer = optax . rmsprop ( 0.01 ), ) history = model . fit ( np . arange ( 100 ) . reshape ( 5 , 20 ) . astype ( np . float32 ), np . zeros ( 5 ), epochs = 10 , batch_size = 1 , callbacks = [ callback ], verbose = 0 , ) assert len ( history . history [ \"loss\" ]) == 7 # Only 7 epochs are run.","title":"elegy.callbacks.early_stopping.EarlyStopping"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/early_stopping.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def on_epoch_end ( self , epoch , logs = None ): current = self . get_monitor_value ( logs ) if current is None : return if self . monitor_op ( current - self . min_delta , self . best ): self . best = current self . wait = 0 if self . restore_best_weights : # This will also save optimizer state self . best_state = self . model . full_state else : self . wait += 1 if self . wait >= self . patience : self . stopped_epoch = epoch self . model . stop_training = True if self . restore_best_weights : if self . verbose > 0 : print ( \"Restoring model weights from the end of the best epoch.\" ) self . model . full_state = self . best_state","title":"on_epoch_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 122 123 124 125 126 127 128 129 def on_train_begin ( self , logs = None ): # Allow instances to be re-used self . wait = 0 self . stopped_epoch = 0 if self . baseline is not None : self . best = self . baseline else : self . best = np . Inf if self . monitor_op == np . less else - np . Inf","title":"on_train_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py 151 152 153 def on_train_end ( self , logs = None ): if self . stopped_epoch > 0 and self . verbose > 0 : print ( \"Epoch %05d : early stopping\" % ( self . stopped_epoch + 1 ))","title":"on_train_end()"},{"location":"api/callbacks/History/","text":"elegy.callbacks.History Callback that records events into a History object. This callback is automatically applied to every Keras model. The History object gets returned by the fit method of models. on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/history.py 21 22 23 24 25 26 27 28 29 def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} self . epoch . append ( epoch ) for k , v in logs . items (): self . history . setdefault ( k , []) . append ( v ) # Set the history attribute on the model after the epoch ends. This will # make sure that the state which is set is the latest one. self . model . history = self on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 18 19 def on_train_begin ( self , logs = None ): self . epoch = [] on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"History"},{"location":"api/callbacks/History/#elegycallbackshistory","text":"","title":"elegy.callbacks.History"},{"location":"api/callbacks/History/#elegy.callbacks.history.History","text":"Callback that records events into a History object. This callback is automatically applied to every Keras model. The History object gets returned by the fit method of models.","title":"elegy.callbacks.history.History"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/history.py 21 22 23 24 25 26 27 28 29 def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} self . epoch . append ( epoch ) for k , v in logs . items (): self . history . setdefault ( k , []) . append ( v ) # Set the history attribute on the model after the epoch ends. This will # make sure that the state which is set is the latest one. self . model . history = self","title":"on_epoch_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 18 19 def on_train_begin ( self , logs = None ): self . epoch = []","title":"on_train_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/LambdaCallback/","text":"elegy.callbacks.LambdaCallback Callback for creating simple, custom callbacks on-the-fly. This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as: on_epoch_begin and on_epoch_end expect two positional arguments: epoch , logs on_train_batch_begin and on_train_batch_end expect two positional arguments: batch , logs on_train_begin and on_train_end expect one positional argument: logs Examples: # Print the batch number at the beginning of every batch. batch_print_callback = LambdaCallback ( on_train_batch_begin = lambda batch , logs : print ( batch )) # Stream the epoch loss to a file in JSON format. The file content # is not well-formed JSON but rather has a JSON object per line. import json json_log = open ( 'loss_log.json' , mode = 'wt' , buffering = 1 ) json_logging_callback = LambdaCallback ( on_epoch_end = lambda epoch , logs : json_log . write ( json . dumps ({ 'epoch' : epoch , 'loss' : logs [ 'loss' ]}) + ' \\n ' ), on_train_end = lambda logs : json_log . close () ) # Terminate some processes after having finished model training. processes = ... cleanup_callback = LambdaCallback ( on_train_end = lambda logs : [ p . terminate () for p in processes if p . is_alive ()]) model . fit ( ... , callbacks = [ batch_print_callback , json_logging_callback , cleanup_callback ]) on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) inherited Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/lambda_callback.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) inherited Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"LambdaCallback"},{"location":"api/callbacks/LambdaCallback/#elegycallbackslambdacallback","text":"","title":"elegy.callbacks.LambdaCallback"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback","text":"Callback for creating simple, custom callbacks on-the-fly. This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as: on_epoch_begin and on_epoch_end expect two positional arguments: epoch , logs on_train_batch_begin and on_train_batch_end expect two positional arguments: batch , logs on_train_begin and on_train_end expect one positional argument: logs Examples: # Print the batch number at the beginning of every batch. batch_print_callback = LambdaCallback ( on_train_batch_begin = lambda batch , logs : print ( batch )) # Stream the epoch loss to a file in JSON format. The file content # is not well-formed JSON but rather has a JSON object per line. import json json_log = open ( 'loss_log.json' , mode = 'wt' , buffering = 1 ) json_logging_callback = LambdaCallback ( on_epoch_end = lambda epoch , logs : json_log . write ( json . dumps ({ 'epoch' : epoch , 'loss' : logs [ 'loss' ]}) + ' \\n ' ), on_train_end = lambda logs : json_log . close () ) # Terminate some processes after having finished model training. processes = ... cleanup_callback = LambdaCallback ( on_train_end = lambda logs : [ p . terminate () for p in processes if p . is_alive ()]) model . fit ( ... , callbacks = [ batch_print_callback , json_logging_callback , cleanup_callback ])","title":"elegy.callbacks.lambda_callback.LambdaCallback"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/lambda_callback.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass","title":"on_epoch_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/ModelCheckpoint/","text":"elegy.callbacks.ModelCheckpoint Callback to save the Elegy model or model weights at some frequency. ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights at some interval, so the model or weights can be loaded later to continue the training from the state saved. A few options this callback provides include: Whether to only keep the model that has achieved the \"best performance\" so far, or whether to save the model at the end of every epoch regardless of performance. Definition of 'best'; which quantity to monitor and whether it should be maximized or minimized. The frequency it should save at. Currently, the callback supports saving at the end of every epoch, or after a fixed number of training batches. Examples: EPOCHS = 10 checkpoint_path = '/tmp/checkpoint' model_checkpoint_callback = elegy . callbacks . ModelCheckpoint ( path = checkpoint_path , monitor = 'val_acc' , mode = 'max' , save_best_only = True ) # Model is saved at the end of every epoch, if it's the best seen # so far. model . fit ( epochs = EPOCHS , callbacks = [ model_checkpoint_callback ]) # The model status (that are considered the best) are loaded into the model. model . load ( checkpoint_path ) on_epoch_begin ( self , epoch , logs = None ) Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 134 135 def on_epoch_begin ( self , epoch , logs = None ): self . _current_epoch = epoch on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/model_checkpoint.py 137 138 139 140 141 def on_epoch_end ( self , epoch , logs = None ): self . epochs_since_last_save += 1 # pylint: disable=protected-access if self . save_freq == \"epoch\" : self . _save_model ( epoch = epoch , logs = logs ) on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) inherited Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"ModelCheckpoint"},{"location":"api/callbacks/ModelCheckpoint/#elegycallbacksmodelcheckpoint","text":"","title":"elegy.callbacks.ModelCheckpoint"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint","text":"Callback to save the Elegy model or model weights at some frequency. ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights at some interval, so the model or weights can be loaded later to continue the training from the state saved. A few options this callback provides include: Whether to only keep the model that has achieved the \"best performance\" so far, or whether to save the model at the end of every epoch regardless of performance. Definition of 'best'; which quantity to monitor and whether it should be maximized or minimized. The frequency it should save at. Currently, the callback supports saving at the end of every epoch, or after a fixed number of training batches. Examples: EPOCHS = 10 checkpoint_path = '/tmp/checkpoint' model_checkpoint_callback = elegy . callbacks . ModelCheckpoint ( path = checkpoint_path , monitor = 'val_acc' , mode = 'max' , save_best_only = True ) # Model is saved at the end of every epoch, if it's the best seen # so far. model . fit ( epochs = EPOCHS , callbacks = [ model_checkpoint_callback ]) # The model status (that are considered the best) are loaded into the model. model . load ( checkpoint_path )","title":"elegy.callbacks.model_checkpoint.ModelCheckpoint"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 134 135 def on_epoch_begin ( self , epoch , logs = None ): self . _current_epoch = epoch","title":"on_epoch_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/model_checkpoint.py 137 138 139 140 141 def on_epoch_end ( self , epoch , logs = None ): self . epochs_since_last_save += 1 # pylint: disable=protected-access if self . save_freq == \"epoch\" : self . _save_model ( epoch = epoch , logs = logs )","title":"on_epoch_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/RemoteMonitor/","text":"elegy.callbacks.RemoteMonitor Callback used to stream events to a server. Requires the requests library. Events are sent to root + '/publish/epoch/end/' by default. Calls are HTTP POST, with a data argument which is a JSON-encoded dictionary of event data. If send_as_json is set to True, the content type of the request will be application/json. Otherwise the serialized JSON will be sent within a form. on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/remote_monitor.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def on_epoch_end ( self , epoch , logs = None ): if requests is None : raise ImportError ( \"RemoteMonitor requires the `requests` library.\" ) logs = logs or {} send = {} send [ \"epoch\" ] = epoch for k , v in logs . items (): # np.ndarray and np.generic are not scalar types # therefore we must unwrap their scalar values and # pass to the json-serializable dict 'send' if isinstance ( v , ( np . ndarray , np . generic )): send [ k ] = v . item () else : send [ k ] = v try : if self . send_as_json : requests . post ( self . root + self . path , json = send , headers = self . headers ) else : requests . post ( self . root + self . path , { self . field : json . dumps ( send )}, headers = self . headers , ) except requests . exceptions . RequestException : logging . warning ( \"Warning: could not reach RemoteMonitor \" \"root server at \" + str ( self . root ) ) on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) inherited Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"RemoteMonitor"},{"location":"api/callbacks/RemoteMonitor/#elegycallbacksremotemonitor","text":"","title":"elegy.callbacks.RemoteMonitor"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor","text":"Callback used to stream events to a server. Requires the requests library. Events are sent to root + '/publish/epoch/end/' by default. Calls are HTTP POST, with a data argument which is a JSON-encoded dictionary of event data. If send_as_json is set to True, the content type of the request will be application/json. Otherwise the serialized JSON will be sent within a form.","title":"elegy.callbacks.remote_monitor.RemoteMonitor"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/remote_monitor.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def on_epoch_end ( self , epoch , logs = None ): if requests is None : raise ImportError ( \"RemoteMonitor requires the `requests` library.\" ) logs = logs or {} send = {} send [ \"epoch\" ] = epoch for k , v in logs . items (): # np.ndarray and np.generic are not scalar types # therefore we must unwrap their scalar values and # pass to the json-serializable dict 'send' if isinstance ( v , ( np . ndarray , np . generic )): send [ k ] = v . item () else : send [ k ] = v try : if self . send_as_json : requests . post ( self . root + self . path , json = send , headers = self . headers ) else : requests . post ( self . root + self . path , { self . field : json . dumps ( send )}, headers = self . headers , ) except requests . exceptions . RequestException : logging . warning ( \"Warning: could not reach RemoteMonitor \" \"root server at \" + str ( self . root ) )","title":"on_epoch_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/TensorBoard/","text":"elegy.callbacks.TensorBoard Callback that streams epoch results to tensorboard events folder. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . tensorboard_logger = TensorBoard ( 'runs' ) model . fit ( X_train , Y_train , callbacks = [ tensorboard_logger ]) on_epoch_begin ( self , epoch , logs = None ) Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 105 106 def on_epoch_begin ( self , epoch : int , logs = None ): self . current_epoch = epoch on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/tensorboard.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} if self . keys is None : self . keys = logs . keys () # logs on on_{train, test}_batch_end do not have val metrics if self . write_per_batch : for key in logs : if \"val\" in key : self . val_writer . add_scalar ( key . replace ( \"val_\" , \"\" ), logs [ key ], self . global_step ) return elif epoch % self . update_freq == 0 : for key in self . keys : if \"val\" in key : self . val_writer . add_scalar ( key . replace ( \"val_\" , \"\" ), logs [ key ], epoch ) else : self . train_writer . add_scalar ( key , logs [ key ], epoch ) on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py 94 95 96 97 98 99 100 101 102 103 def on_train_batch_end ( self , batch : int , logs = None ): if not self . write_per_batch : return logs = logs or {} self . global_step = batch + self . current_epoch * ( self . steps ) if self . global_step % self . update_freq == 0 : if self . keys is None : self . keys = logs . keys () for key in self . keys : self . train_writer . add_scalar ( key , logs [ key ], self . global_step ) on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 84 85 86 87 88 89 90 91 92 def on_train_begin ( self , logs = None ): self . train_writer = SummaryWriter ( os . path . join ( self . logdir , \"train\" ), purge_step = self . purge_step ) self . val_writer = SummaryWriter ( os . path . join ( self . logdir , \"val\" ), purge_step = self . purge_step ) self . steps = self . params [ \"steps\" ] self . global_step = 0 on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 133 134 135 def on_train_end ( self , logs = None ): self . train_writer . close () self . val_writer . close ()","title":"TensorBoard"},{"location":"api/callbacks/TensorBoard/#elegycallbackstensorboard","text":"","title":"elegy.callbacks.TensorBoard"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard","text":"Callback that streams epoch results to tensorboard events folder. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . tensorboard_logger = TensorBoard ( 'runs' ) model . fit ( X_train , Y_train , callbacks = [ tensorboard_logger ])","title":"elegy.callbacks.tensorboard.TensorBoard"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 105 106 def on_epoch_begin ( self , epoch : int , logs = None ): self . current_epoch = epoch","title":"on_epoch_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/tensorboard.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} if self . keys is None : self . keys = logs . keys () # logs on on_{train, test}_batch_end do not have val metrics if self . write_per_batch : for key in logs : if \"val\" in key : self . val_writer . add_scalar ( key . replace ( \"val_\" , \"\" ), logs [ key ], self . global_step ) return elif epoch % self . update_freq == 0 : for key in self . keys : if \"val\" in key : self . val_writer . add_scalar ( key . replace ( \"val_\" , \"\" ), logs [ key ], epoch ) else : self . train_writer . add_scalar ( key , logs [ key ], epoch )","title":"on_epoch_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py 94 95 96 97 98 99 100 101 102 103 def on_train_batch_end ( self , batch : int , logs = None ): if not self . write_per_batch : return logs = logs or {} self . global_step = batch + self . current_epoch * ( self . steps ) if self . global_step % self . update_freq == 0 : if self . keys is None : self . keys = logs . keys () for key in self . keys : self . train_writer . add_scalar ( key , logs [ key ], self . global_step )","title":"on_train_batch_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 84 85 86 87 88 89 90 91 92 def on_train_begin ( self , logs = None ): self . train_writer = SummaryWriter ( os . path . join ( self . logdir , \"train\" ), purge_step = self . purge_step ) self . val_writer = SummaryWriter ( os . path . join ( self . logdir , \"val\" ), purge_step = self . purge_step ) self . steps = self . params [ \"steps\" ] self . global_step = 0","title":"on_train_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py 133 134 135 def on_train_end ( self , logs = None ): self . train_writer . close () self . val_writer . close ()","title":"on_train_end()"},{"location":"api/callbacks/TerminateOnNaN/","text":"elegy.callbacks.TerminateOnNaN Callback that terminates training when a NaN loss is encountered. on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) inherited Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/terminate_nan.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) inherited Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"TerminateOnNaN"},{"location":"api/callbacks/TerminateOnNaN/#elegycallbacksterminateonnan","text":"","title":"elegy.callbacks.TerminateOnNaN"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN","text":"Callback that terminates training when a NaN loss is encountered.","title":"elegy.callbacks.terminate_nan.TerminateOnNaN"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/terminate_nan.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass","title":"on_epoch_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py 197 198 199 200 201 202 203 204 205 206 207 208 209 @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 260 261 262 263 264 265 266 267 268 269 def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 272 273 274 275 276 277 278 279 280 281 def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 236 237 238 239 240 241 242 243 244 245 def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 248 249 250 251 252 253 254 255 256 257 def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py 129 130 131 132 133 134 135 136 137 138 139 140 141 @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 212 213 214 215 216 217 218 219 220 221 def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py 224 225 226 227 228 229 230 231 232 233 def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/hooks/add_loss/","text":"elegy.hooks.add_loss A hook that lets you define a loss within a [ module ][elegy.module.Module]. w = elegy . get_parameter ( \"w\" , [ 3 , 5 ], initializer = jnp . ones ) # L2 regularization penalty elegy . add_loss ( \"l2_regularization\" , 0.01 * jnp . mean ( w ** 2 )) The loss will be aggregated by [ Module.apply ][elegy.module.Module.apply] and automatically handled by [ Model ][elegy.model.Model]. Parameters: Name Type Description Default name str The name of the loss. If a name is repeated on different calls values will be added together. required value ndarray The value for the loss. required Source code in elegy/hooks.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def add_loss ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a loss within a [`module`][elegy.module.Module]. ```python w = elegy.get_parameter(\"w\", [3, 5], initializer=jnp.ones) # L2 regularization penalty elegy.add_loss(\"l2_regularization\", 0.01 * jnp.mean(w ** 2)) ``` The loss will be aggregated by [`Module.apply`][elegy.module.Module.apply] and automatically handled by [`Model`][elegy.model.Model]. Arguments: name: The name of the loss. If a `name` is repeated on different calls values will be added together. value: The value for the loss. \"\"\" if not name . endswith ( \"loss\" ): name += \"_loss\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] if name in context . losses : context . losses [ name ] += value else : context . losses [ name ] = value else : raise ValueError ( \"Cannot execute `add_loss` outside of an `elegy.context`\" )","title":"add_loss"},{"location":"api/hooks/add_loss/#elegyhooksadd_loss","text":"","title":"elegy.hooks.add_loss"},{"location":"api/hooks/add_loss/#elegy.hooks.add_loss","text":"A hook that lets you define a loss within a [ module ][elegy.module.Module]. w = elegy . get_parameter ( \"w\" , [ 3 , 5 ], initializer = jnp . ones ) # L2 regularization penalty elegy . add_loss ( \"l2_regularization\" , 0.01 * jnp . mean ( w ** 2 )) The loss will be aggregated by [ Module.apply ][elegy.module.Module.apply] and automatically handled by [ Model ][elegy.model.Model]. Parameters: Name Type Description Default name str The name of the loss. If a name is repeated on different calls values will be added together. required value ndarray The value for the loss. required Source code in elegy/hooks.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def add_loss ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a loss within a [`module`][elegy.module.Module]. ```python w = elegy.get_parameter(\"w\", [3, 5], initializer=jnp.ones) # L2 regularization penalty elegy.add_loss(\"l2_regularization\", 0.01 * jnp.mean(w ** 2)) ``` The loss will be aggregated by [`Module.apply`][elegy.module.Module.apply] and automatically handled by [`Model`][elegy.model.Model]. Arguments: name: The name of the loss. If a `name` is repeated on different calls values will be added together. value: The value for the loss. \"\"\" if not name . endswith ( \"loss\" ): name += \"_loss\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] if name in context . losses : context . losses [ name ] += value else : context . losses [ name ] = value else : raise ValueError ( \"Cannot execute `add_loss` outside of an `elegy.context`\" )","title":"elegy.hooks.add_loss"},{"location":"api/hooks/add_metric/","text":"elegy.hooks.add_metric A hook that lets you define a metric within a [ module ][elegy.module.Module]. y = jax . nn . relu ( x ) elegy . add_metric ( \"activation_mean\" , jnp . mean ( y )) The metrics will be aggregated by [ Module.apply ][elegy.module.Module.apply] and automatically handled by [ Model ][elegy.model.Model]. Parameters: Name Type Description Default name str The name of the loss. If a metric with the same name already exists a unique identifier will be generated. required value ndarray The value for the metric. required Source code in elegy/hooks.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def add_metric ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a metric within a [`module`][elegy.module.Module]. ```python y = jax.nn.relu(x) elegy.add_metric(\"activation_mean\", jnp.mean(y)) ``` The metrics will be aggregated by [`Module.apply`][elegy.module.Module.apply] and automatically handled by [`Model`][elegy.model.Model]. Arguments: name: The name of the loss. If a metric with the same `name` already exists a unique identifier will be generated. value: The value for the metric. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] base_name = \"/\" . join ( context . path_names_c ) name = f \" { base_name } / { name } \" name = get_unique_name ( context . metrics , name ) context . metrics [ name ] = value else : raise ValueError ( \"Cannot execute `add_metric` outside of an `elegy.context`\" )","title":"add_metric"},{"location":"api/hooks/add_metric/#elegyhooksadd_metric","text":"","title":"elegy.hooks.add_metric"},{"location":"api/hooks/add_metric/#elegy.hooks.add_metric","text":"A hook that lets you define a metric within a [ module ][elegy.module.Module]. y = jax . nn . relu ( x ) elegy . add_metric ( \"activation_mean\" , jnp . mean ( y )) The metrics will be aggregated by [ Module.apply ][elegy.module.Module.apply] and automatically handled by [ Model ][elegy.model.Model]. Parameters: Name Type Description Default name str The name of the loss. If a metric with the same name already exists a unique identifier will be generated. required value ndarray The value for the metric. required Source code in elegy/hooks.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def add_metric ( name : str , value : np . ndarray ) -> None : \"\"\" A hook that lets you define a metric within a [`module`][elegy.module.Module]. ```python y = jax.nn.relu(x) elegy.add_metric(\"activation_mean\", jnp.mean(y)) ``` The metrics will be aggregated by [`Module.apply`][elegy.module.Module.apply] and automatically handled by [`Model`][elegy.model.Model]. Arguments: name: The name of the loss. If a metric with the same `name` already exists a unique identifier will be generated. value: The value for the metric. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] base_name = \"/\" . join ( context . path_names_c ) name = f \" { base_name } / { name } \" name = get_unique_name ( context . metrics , name ) context . metrics [ name ] = value else : raise ValueError ( \"Cannot execute `add_metric` outside of an `elegy.context`\" )","title":"elegy.hooks.add_metric"},{"location":"api/hooks/add_summary/","text":"elegy.hooks.add_summary A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so Model.summary() can show a representation of architecture. def call ( self , x ): ... y = jax . nn . relu ( x ) elegy . add_summary ( \"relu\" , y ) ... The summaries will be aggregated by [ apply ][elegy.module.Module.apply] if get_summaries is set to True , else this hook does nothing. transformed_state = transform . apply ( ... , get_summaries = True , ... ) Parameters: Name Type Description Default name Optional[str] The name of the loss. If a summary with the same name already exists a unique identifier will be generated. required value ndarray The value for the summary. required Source code in elegy/hooks.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def add_summary ( name : tp . Optional [ str ], value : np . ndarray ) -> None : \"\"\" A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so `Model.summary()` can show a representation of architecture. ```python def call(self, x): ... y = jax.nn.relu(x) elegy.add_summary(\"relu\", y) ... ``` The summaries will be aggregated by [`apply`][elegy.module.Module.apply] if `get_summaries` is set to `True`, else this hook does nothing. ```python transformed_state = transform.apply(..., get_summaries=True, ...) ``` Arguments: name: The name of the loss. If a summary with the same `name` already exists a unique identifier will be generated. value: The value for the summary. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if not context . get_summaries : return # name = level_names[module] base_name = \"/\" . join ( context . path_names_c ) base_name = f \" { base_name } / { name } \" if name is not None else base_name base_name = get_unique_name ( context . summaries , base_name ) module = module if name is None else None # pass module only if name is None context . summaries . append (( module , base_name , value )) else : raise ValueError ( \"Cannot execute `add_summary` outside of an `elegy.context`\" )","title":"add_summary"},{"location":"api/hooks/add_summary/#elegyhooksadd_summary","text":"","title":"elegy.hooks.add_summary"},{"location":"api/hooks/add_summary/#elegy.hooks.add_summary","text":"A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so Model.summary() can show a representation of architecture. def call ( self , x ): ... y = jax . nn . relu ( x ) elegy . add_summary ( \"relu\" , y ) ... The summaries will be aggregated by [ apply ][elegy.module.Module.apply] if get_summaries is set to True , else this hook does nothing. transformed_state = transform . apply ( ... , get_summaries = True , ... ) Parameters: Name Type Description Default name Optional[str] The name of the loss. If a summary with the same name already exists a unique identifier will be generated. required value ndarray The value for the summary. required Source code in elegy/hooks.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def add_summary ( name : tp . Optional [ str ], value : np . ndarray ) -> None : \"\"\" A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so `Model.summary()` can show a representation of architecture. ```python def call(self, x): ... y = jax.nn.relu(x) elegy.add_summary(\"relu\", y) ... ``` The summaries will be aggregated by [`apply`][elegy.module.Module.apply] if `get_summaries` is set to `True`, else this hook does nothing. ```python transformed_state = transform.apply(..., get_summaries=True, ...) ``` Arguments: name: The name of the loss. If a summary with the same `name` already exists a unique identifier will be generated. value: The value for the summary. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if not context . get_summaries : return # name = level_names[module] base_name = \"/\" . join ( context . path_names_c ) base_name = f \" { base_name } / { name } \" if name is not None else base_name base_name = get_unique_name ( context . summaries , base_name ) module = module if name is None else None # pass module only if name is None context . summaries . append (( module , base_name , value )) else : raise ValueError ( \"Cannot execute `add_summary` outside of an `elegy.context`\" )","title":"elegy.hooks.add_summary"},{"location":"api/hooks/get_parameter/","text":"elegy.hooks.get_parameter A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fbb5accf1f0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/hooks.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_parameter ( name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if not hasattr ( module , name ): if not context . building : raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) module . _params . add ( name ) if dtype is None : dtype = module . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( module , name , initial_value ) elif name not in module . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( module , name ) return value else : raise ValueError ( \"Cannot execute `get_parameter` outside of a `elegy.context`\" )","title":"get_parameter"},{"location":"api/hooks/get_parameter/#elegyhooksget_parameter","text":"","title":"elegy.hooks.get_parameter"},{"location":"api/hooks/get_parameter/#elegy.hooks.get_parameter","text":"A hook that lets you add a parameter to the current module. The parameter will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the parameter. () dtype Optional[numpy.dtype] The type of the parameter. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fbb5accf1f0> Returns: Type Description ndarray The value of the parameter. Source code in elegy/hooks.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_parameter ( name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , ) -> np . ndarray : \"\"\" A hook that lets you add a parameter to the current module. The parameter will only be created once during `init` and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the parameter. dtype: The type of the parameter. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the parameter. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if not hasattr ( module , name ): if not context . building : raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) module . _params . add ( name ) if dtype is None : dtype = module . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( module , name , initial_value ) elif name not in module . _params : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the parameter.\" ) value = getattr ( module , name ) return value else : raise ValueError ( \"Cannot execute `get_parameter` outside of a `elegy.context`\" )","title":"elegy.hooks.get_parameter"},{"location":"api/hooks/get_state/","text":"elegy.hooks.get_state A hook that lets you add a state to the current module. The state will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the state. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the state. () dtype Optional[numpy.dtype] The type of the state. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fbb5accf1f0> Returns: Type Description Any The value of the state. Source code in elegy/hooks.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def get_state ( name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , ) -> tp . Any : \"\"\" A hook that lets you add a state to the current module. The state will only be created once during `init` and will reused afterwards. Arguments: name: The name of the state. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the state. dtype: The type of the state. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the state. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if not hasattr ( module , name ): if not context . building : raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) module . _states . add ( name ) initial_name = as_initial ( name ) if dtype is None : dtype = module . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( module , name , initial_value ) setattr ( module , initial_name , initial_value ) elif name not in module . _states : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the state.\" ) value = getattr ( module , name ) return value else : raise ValueError ( \"Cannot execute `get_state` outside of a `elegy.context`\" )","title":"get_state"},{"location":"api/hooks/get_state/#elegyhooksget_state","text":"","title":"elegy.hooks.get_state"},{"location":"api/hooks/get_state/#elegy.hooks.get_state","text":"A hook that lets you add a state to the current module. The state will only be created once during init and will reused afterwards. Parameters: Name Type Description Default name str The name of the state. It must be unique and no other field/property/method of the instance can have that name. required shape Sequence[int] The shape of the state. () dtype Optional[numpy.dtype] The type of the state. None initializer Union[Callable[[Sequence[int], Any], Any], Any] A callable that takes in a shape and dtype and returns the initial value. <function zeros at 0x7fbb5accf1f0> Returns: Type Description Any The value of the state. Source code in elegy/hooks.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def get_state ( name : str , shape : tp . Sequence [ int ] = (), dtype : tp . Optional [ np . dtype ] = None , initializer : tp . Union [ tp . Callable [[ tp . Sequence [ int ], tp . Any ], tp . Any ], tp . Any ] = jnp . zeros , ) -> tp . Any : \"\"\" A hook that lets you add a state to the current module. The state will only be created once during `init` and will reused afterwards. Arguments: name: The name of the state. It must be unique and no other field/property/method of the instance can have that name. shape: The shape of the state. dtype: The type of the state. initializer: A callable that takes in a shape and dtype and returns the initial value. Returns: The value of the state. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if not hasattr ( module , name ): if not context . building : raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) module . _states . add ( name ) initial_name = as_initial ( name ) if dtype is None : dtype = module . dtype initial_value = ( initializer ( shape , dtype ) if isinstance ( initializer , tp . Callable ) else initializer ) setattr ( module , name , initial_value ) setattr ( module , initial_name , initial_value ) elif name not in module . _states : raise ValueError ( f \"Class already contained a property named ' { name } ', \" \"please use a unique name for the state.\" ) value = getattr ( module , name ) return value else : raise ValueError ( \"Cannot execute `get_state` outside of a `elegy.context`\" )","title":"elegy.hooks.get_state"},{"location":"api/hooks/next_rng_key/","text":"elegy.hooks.next_rng_key A hook that returns a unique JAX RNG key split from the current global key. key = elegy . next_rng_key () x = jax . random . uniform ( key , []) Returns: Type Description ndarray A unique (within a transformed function) JAX rng key that can be used with APIs such as jax.random.uniform . Source code in elegy/hooks.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def next_rng_key () -> PRNGKey : \"\"\" A hook that returns a unique JAX RNG key split from the current global key. ```python key = elegy.next_rng_key() x = jax.random.uniform(key, []) ``` Returns: A unique (within a transformed function) JAX rng key that can be used with APIs such as ``jax.random.uniform``. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] if context . rng_sequence is not None : context : Context = LOCAL . contexts [ - 1 ] return next ( context . rng_sequence ) else : raise ValueError ( \"Cannot execute `rng` not set in context, check init or apply.\" ) else : raise ValueError ( \"Cannot execute `next_rng_key` outside of an `elegy.context`\" )","title":"next_rng_key"},{"location":"api/hooks/next_rng_key/#elegyhooksnext_rng_key","text":"","title":"elegy.hooks.next_rng_key"},{"location":"api/hooks/next_rng_key/#elegy.hooks.next_rng_key","text":"A hook that returns a unique JAX RNG key split from the current global key. key = elegy . next_rng_key () x = jax . random . uniform ( key , []) Returns: Type Description ndarray A unique (within a transformed function) JAX rng key that can be used with APIs such as jax.random.uniform . Source code in elegy/hooks.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def next_rng_key () -> PRNGKey : \"\"\" A hook that returns a unique JAX RNG key split from the current global key. ```python key = elegy.next_rng_key() x = jax.random.uniform(key, []) ``` Returns: A unique (within a transformed function) JAX rng key that can be used with APIs such as ``jax.random.uniform``. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] if context . rng_sequence is not None : context : Context = LOCAL . contexts [ - 1 ] return next ( context . rng_sequence ) else : raise ValueError ( \"Cannot execute `rng` not set in context, check init or apply.\" ) else : raise ValueError ( \"Cannot execute `next_rng_key` outside of an `elegy.context`\" )","title":"elegy.hooks.next_rng_key"},{"location":"api/hooks/set_state/","text":"elegy.hooks.set_state A hook that lets you update a state of the current module, if the state does not exist it will be created. Parameters: Name Type Description Default name str The name of the state. It must be unique and no other field/property/method of the instance can have that name. required value Any The updated value of the state. required Source code in elegy/hooks.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def set_state ( name : str , value : tp . Any ) -> None : \"\"\" A hook that lets you update a state of the current module, if the state does not exist it will be created. Arguments: name: The name of the state. It must be unique and no other field/property/method of the instance can have that name. value: The updated value of the state. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if name not in module . _states : if not context . building : raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) module . _states . add ( name ) initial_name = as_initial ( name ) setattr ( module , name , value ) setattr ( module , initial_name , value ) else : setattr ( module , name , value ) else : raise ValueError ( \"Cannot execute `set_state` outside of a `elegy.context`\" )","title":"set_state"},{"location":"api/hooks/set_state/#elegyhooksset_state","text":"","title":"elegy.hooks.set_state"},{"location":"api/hooks/set_state/#elegy.hooks.set_state","text":"A hook that lets you update a state of the current module, if the state does not exist it will be created. Parameters: Name Type Description Default name str The name of the state. It must be unique and no other field/property/method of the instance can have that name. required value Any The updated value of the state. required Source code in elegy/hooks.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def set_state ( name : str , value : tp . Any ) -> None : \"\"\" A hook that lets you update a state of the current module, if the state does not exist it will be created. Arguments: name: The name of the state. It must be unique and no other field/property/method of the instance can have that name. value: The updated value of the state. \"\"\" if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] module = context . module_c [ - 1 ] if name not in module . _states : if not context . building : raise ValueError ( f \"Trying to initialize ' { name } ' outside of `init`.\" ) module . _states . add ( name ) initial_name = as_initial ( name ) setattr ( module , name , value ) setattr ( module , initial_name , value ) else : setattr ( module , name , value ) else : raise ValueError ( \"Cannot execute `set_state` outside of a `elegy.context`\" )","title":"elegy.hooks.set_state"},{"location":"api/initializers/Constant/","text":"elegy.initializers.Constant Initializes with a constant.","title":"Constant"},{"location":"api/initializers/Constant/#elegyinitializersconstant","text":"","title":"elegy.initializers.Constant"},{"location":"api/initializers/Constant/#elegy.initializers.Constant","text":"Initializes with a constant.","title":"elegy.initializers.Constant"},{"location":"api/initializers/Orthogonal/","text":"elegy.initializers.Orthogonal Uniform scaling initializer.","title":"Orthogonal"},{"location":"api/initializers/Orthogonal/#elegyinitializersorthogonal","text":"","title":"elegy.initializers.Orthogonal"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal","text":"Uniform scaling initializer.","title":"elegy.initializers.Orthogonal"},{"location":"api/initializers/RandomNormal/","text":"elegy.initializers.RandomNormal Initializes by sampling from a normal distribution.","title":"RandomNormal"},{"location":"api/initializers/RandomNormal/#elegyinitializersrandomnormal","text":"","title":"elegy.initializers.RandomNormal"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal","text":"Initializes by sampling from a normal distribution.","title":"elegy.initializers.RandomNormal"},{"location":"api/initializers/RandomUniform/","text":"elegy.initializers.RandomUniform Initializes by sampling from a uniform distribution.","title":"RandomUniform"},{"location":"api/initializers/RandomUniform/#elegyinitializersrandomuniform","text":"","title":"elegy.initializers.RandomUniform"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform","text":"Initializes by sampling from a uniform distribution.","title":"elegy.initializers.RandomUniform"},{"location":"api/initializers/TruncatedNormal/","text":"elegy.initializers.TruncatedNormal Initializes by sampling from a truncated normal distribution.","title":"TruncatedNormal"},{"location":"api/initializers/TruncatedNormal/#elegyinitializerstruncatednormal","text":"","title":"elegy.initializers.TruncatedNormal"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal","text":"Initializes by sampling from a truncated normal distribution.","title":"elegy.initializers.TruncatedNormal"},{"location":"api/initializers/UniformScaling/","text":"elegy.initializers.UniformScaling Uniform scaling initializer. Initializes by sampling from a uniform distribution, but with the variance scaled by the inverse square root of the number of input units, multiplied by the scale.","title":"UniformScaling"},{"location":"api/initializers/UniformScaling/#elegyinitializersuniformscaling","text":"","title":"elegy.initializers.UniformScaling"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling","text":"Uniform scaling initializer. Initializes by sampling from a uniform distribution, but with the variance scaled by the inverse square root of the number of input units, multiplied by the scale.","title":"elegy.initializers.UniformScaling"},{"location":"api/initializers/VarianceScaling/","text":"elegy.initializers.VarianceScaling Initializer which adapts its scale to the shape of the initialized array. The initializer first computes the scaling factor s = scale / n , where n is: Number of input units in the weight tensor, if mode = fan_in . Number of output units, if mode = fan_out . Average of the numbers of input and output units, if mode = fan_avg . Then, with distribution=\"truncated_normal\" or \"normal\" , samples are drawn from a distribution with a mean of zero and a standard deviation (after truncation, if used) stddev = sqrt(s) . With distribution=uniform , samples are drawn from a uniform distribution within [-limit, limit] , with limit = sqrt(3 * s) . The variance scaling initializer can be configured to generate other standard initializers using the scale, mode and distribution arguments. Here are some example configurations: ============== ============================================================== Name Parameters ============== ============================================================== glorot_uniform scale=1.0, mode= fan_avg , distribution= uniform glorot_normal scale=1.0, mode= fan_avg , distribution= truncated_normal lecun_uniform scale=1.0, mode= fan_in , distribution= uniform lecun_normal scale=1.0, mode= fan_in , distribution= truncated_normal he_uniform scale=2.0, mode= fan_in , distribution= uniform he_normal scale=2.0, mode= fan_in , distribution= truncated_normal ============== ==============================================================","title":"VarianceScaling"},{"location":"api/initializers/VarianceScaling/#elegyinitializersvariancescaling","text":"","title":"elegy.initializers.VarianceScaling"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling","text":"Initializer which adapts its scale to the shape of the initialized array. The initializer first computes the scaling factor s = scale / n , where n is: Number of input units in the weight tensor, if mode = fan_in . Number of output units, if mode = fan_out . Average of the numbers of input and output units, if mode = fan_avg . Then, with distribution=\"truncated_normal\" or \"normal\" , samples are drawn from a distribution with a mean of zero and a standard deviation (after truncation, if used) stddev = sqrt(s) . With distribution=uniform , samples are drawn from a uniform distribution within [-limit, limit] , with limit = sqrt(3 * s) . The variance scaling initializer can be configured to generate other standard initializers using the scale, mode and distribution arguments. Here are some example configurations: ============== ============================================================== Name Parameters ============== ============================================================== glorot_uniform scale=1.0, mode= fan_avg , distribution= uniform glorot_normal scale=1.0, mode= fan_avg , distribution= truncated_normal lecun_uniform scale=1.0, mode= fan_in , distribution= uniform lecun_normal scale=1.0, mode= fan_in , distribution= truncated_normal he_uniform scale=2.0, mode= fan_in , distribution= uniform he_normal scale=2.0, mode= fan_in , distribution= truncated_normal ============== ==============================================================","title":"elegy.initializers.VarianceScaling"},{"location":"api/losses/BinaryCrossentropy/","text":"elegy.losses.BinaryCrossentropy Computes the cross-entropy loss between true labels and predicted labels. Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction. In the snippet below, each of the four examples has only a single floating-pointing value, and both y_pred and y_true have the shape [batch_size] . Usage: y_true = jnp . array ([[ 0. , 1. ], [ 0. , 0. ]]) y_pred = jnp . array [[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. bce = elegy . losses . BinaryCrossentropy () result = bce ( y_true , y_pred ) assert jnp . isclose ( result , 0.815 , rtol = 0.01 ) # Calling with 'sample_weight'. bce = elegy . losses . BinaryCrossentropy () result = bce ( y_true , y_pred , sample_weight = jnp . array ([ 1 , 0 ])) assert jnp . isclose ( result , 0.458 , rtol = 0.01 ) # Using 'sum' reduction type. bce = elegy . losses . BinaryCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = bce ( y_true , y_pred ) assert jnp . isclose ( result , 1.630 , rtol = 0.01 ) # Using 'none' reduction type. bce = elegy . losses . BinaryCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = bce ( y_true , y_pred ) assert jnp . all ( jnp . isclose ( result , [ 0.916 , 0.713 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . BinaryCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , from_logits = False , label_smoothing = 0 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE` will raise an error. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/binary_crossentropy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, ` or `SUM_OVER_BATCH_SIZE` will raise an error. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/binary_crossentropy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Invokes the BinaryCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/binary_crossentropy.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `BinaryCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return binary_crossentropy ( y_true , y_pred , from_logits = self . _from_logits ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/binary_crossentropy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/binary_crossentropy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/binary_crossentropy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/binary_crossentropy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/binary_crossentropy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/binary_crossentropy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"BinaryCrossentropy"},{"location":"api/losses/BinaryCrossentropy/#elegylossesbinarycrossentropy","text":"","title":"elegy.losses.BinaryCrossentropy"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy","text":"Computes the cross-entropy loss between true labels and predicted labels. Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction. In the snippet below, each of the four examples has only a single floating-pointing value, and both y_pred and y_true have the shape [batch_size] . Usage: y_true = jnp . array ([[ 0. , 1. ], [ 0. , 0. ]]) y_pred = jnp . array [[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. bce = elegy . losses . BinaryCrossentropy () result = bce ( y_true , y_pred ) assert jnp . isclose ( result , 0.815 , rtol = 0.01 ) # Calling with 'sample_weight'. bce = elegy . losses . BinaryCrossentropy () result = bce ( y_true , y_pred , sample_weight = jnp . array ([ 1 , 0 ])) assert jnp . isclose ( result , 0.458 , rtol = 0.01 ) # Using 'sum' reduction type. bce = elegy . losses . BinaryCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = bce ( y_true , y_pred ) assert jnp . isclose ( result , 1.630 , rtol = 0.01 ) # Using 'none' reduction type. bce = elegy . losses . BinaryCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = bce ( y_true , y_pred ) assert jnp . all ( jnp . isclose ( result , [ 0.916 , 0.713 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . BinaryCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"elegy.losses.binary_crossentropy.BinaryCrossentropy"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.__init__","text":"Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE` will raise an error. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/binary_crossentropy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, ` or `SUM_OVER_BATCH_SIZE` will raise an error. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/binary_crossentropy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.call","text":"Invokes the BinaryCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/binary_crossentropy.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `BinaryCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return binary_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"call()"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/binary_crossentropy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/binary_crossentropy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/binary_crossentropy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/binary_crossentropy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/binary_crossentropy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/binary_crossentropy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/losses/CategoricalCrossentropy/","text":"elegy.losses.CategoricalCrossentropy Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Usage: y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 0 , 1 ]]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cce = elegy . losses . CategoricalCrossentropy () assert cce ( y_true , y_pred ) == 1.177 # Calling with 'sample_weight'. assert cce ( y_true , y_pred , sample_weight = tf . constant ([ 0.3 , 0.7 ])) == 0.814 # Using 'sum' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) assert cce ( y_true , y_pred ) == 2.354 # Using 'none' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) assert list ( cce ( y_true , y_pred )) == [ 0.0513 , 2.303 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , from_logits = False , label_smoothing = 0 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/categorical_crossentropy.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/categorical_crossentropy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Invokes the CategoricalCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/categorical_crossentropy.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `CategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/categorical_crossentropy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/categorical_crossentropy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/categorical_crossentropy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/categorical_crossentropy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/categorical_crossentropy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/categorical_crossentropy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegylossescategoricalcrossentropy","text":"","title":"elegy.losses.CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy","text":"Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Usage: y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 0 , 1 ]]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cce = elegy . losses . CategoricalCrossentropy () assert cce ( y_true , y_pred ) == 1.177 # Calling with 'sample_weight'. assert cce ( y_true , y_pred , sample_weight = tf . constant ([ 0.3 , 0.7 ])) == 0.814 # Using 'sum' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) assert cce ( y_true , y_pred ) == 2.354 # Using 'none' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) assert list ( cce ( y_true , y_pred )) == [ 0.0513 , 2.303 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"elegy.losses.categorical_crossentropy.CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.__init__","text":"Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/categorical_crossentropy.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/categorical_crossentropy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.call","text":"Invokes the CategoricalCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/categorical_crossentropy.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `CategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"call()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/categorical_crossentropy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/categorical_crossentropy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/categorical_crossentropy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/categorical_crossentropy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/categorical_crossentropy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/categorical_crossentropy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/losses/Loss/","text":"elegy.losses.Loss Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/loss.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/loss.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/loss.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/loss.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/loss.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/loss.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/loss.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/loss.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Loss"},{"location":"api/losses/Loss/#elegylossesloss","text":"","title":"elegy.losses.Loss"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss","text":"Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this.","title":"elegy.losses.loss.Loss"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/loss.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"__init__()"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/loss.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/loss.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/loss.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/loss.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/loss.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/loss.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/loss.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/losses/MeanAbsoluteError/","text":"elegy.losses.MeanAbsoluteError Computes the mean absolute errors between labels and predictions. loss = mean(abs(y_true - y_pred)) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = elegy . losses . MeanAbsoluteError () assert mae ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mae ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = elegy . losses . MeanAbsoluteError ( reduction = elegy . losses . Reduction . SUM ) assert mae ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mae = elegy . losses . MeanAbsoluteError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mae ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_absolute_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/mean_absolute_error.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_error.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( y_true , y_pred ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/mean_absolute_error.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/mean_absolute_error.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/mean_absolute_error.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/mean_absolute_error.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/mean_absolute_error.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/mean_absolute_error.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#elegylossesmeanabsoluteerror","text":"","title":"elegy.losses.MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError","text":"Computes the mean absolute errors between labels and predictions. loss = mean(abs(y_true - y_pred)) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = elegy . losses . MeanAbsoluteError () assert mae ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mae ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = elegy . losses . MeanAbsoluteError ( reduction = elegy . losses . Reduction . SUM ) assert mae ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mae = elegy . losses . MeanAbsoluteError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mae ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), )","title":"elegy.losses.mean_absolute_error.MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_absolute_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/mean_absolute_error.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.call","text":"Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_error.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/mean_absolute_error.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/mean_absolute_error.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/mean_absolute_error.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/mean_absolute_error.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/mean_absolute_error.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/mean_absolute_error.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/losses/MeanAbsolutePercentageError/","text":"elegy.losses.MeanAbsolutePercentageError Computes the mean absolute errors between labels and predictions. loss = mean(abs((y_true - y_pred) / y_true)) Usage: y_true = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = elegy . losses . MeanAbsolutePercentageError () result = mape ( y_true , y_pred ) assert jnp . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert jnp . isclose ( mape ( y_true , y_pred , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = elegy . losses . MeanAbsolutePercentageError ( reduction = elegy . losses . Reduction . SUM ) assert jnp . isclose ( mape ( y_true , y_pred ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = elegy . losses . MeanAbsolutePercentageError ( reduction = elegy . losses . Reduction . NONE ) assert jnp . all ( jnp . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_absolute_percentage_error.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/mean_absolute_percentage_error.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_percentage_error.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_percentage_absolute_error ( y_true , y_pred ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/mean_absolute_percentage_error.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/mean_absolute_percentage_error.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/mean_absolute_percentage_error.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/mean_absolute_percentage_error.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/mean_absolute_percentage_error.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/mean_absolute_percentage_error.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#elegylossesmeanabsolutepercentageerror","text":"","title":"elegy.losses.MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError","text":"Computes the mean absolute errors between labels and predictions. loss = mean(abs((y_true - y_pred) / y_true)) Usage: y_true = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = elegy . losses . MeanAbsolutePercentageError () result = mape ( y_true , y_pred ) assert jnp . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert jnp . isclose ( mape ( y_true , y_pred , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = elegy . losses . MeanAbsolutePercentageError ( reduction = elegy . losses . Reduction . SUM ) assert jnp . isclose ( mape ( y_true , y_pred ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = elegy . losses . MeanAbsolutePercentageError ( reduction = elegy . losses . Reduction . NONE ) assert jnp . all ( jnp . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), )","title":"elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_absolute_percentage_error.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/mean_absolute_percentage_error.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.call","text":"Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_percentage_error.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_percentage_absolute_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/mean_absolute_percentage_error.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/mean_absolute_percentage_error.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/mean_absolute_percentage_error.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/mean_absolute_percentage_error.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/mean_absolute_percentage_error.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/mean_absolute_percentage_error.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/losses/MeanSquaredError/","text":"elegy.losses.MeanSquaredError Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = elegy . losses . MeanSquaredError () assert mse ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mse ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . SUM ) assert mse ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mse ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_squared_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/mean_squared_error.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanSquaredError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_error.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( y_true , y_pred ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/mean_squared_error.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/mean_squared_error.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/mean_squared_error.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/mean_squared_error.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/mean_squared_error.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/mean_squared_error.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegylossesmeansquarederror","text":"","title":"elegy.losses.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError","text":"Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = elegy . losses . MeanSquaredError () assert mse ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mse ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . SUM ) assert mse ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mse ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), )","title":"elegy.losses.mean_squared_error.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_squared_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/mean_squared_error.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.call","text":"Invokes the MeanSquaredError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_error.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/mean_squared_error.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/mean_squared_error.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/mean_squared_error.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/mean_squared_error.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/mean_squared_error.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/mean_squared_error.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/losses/Reduction/","text":"elegy.losses.Reduction Types of loss reduction. Contains the following values: * NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * SUM : Scalar sum of weighted losses. * SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses.","title":"Reduction"},{"location":"api/losses/Reduction/#elegylossesreduction","text":"","title":"elegy.losses.Reduction"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction","text":"Types of loss reduction. Contains the following values: * NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * SUM : Scalar sum of weighted losses. * SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses.","title":"elegy.losses.loss.Reduction"},{"location":"api/losses/SparseCategoricalCrossentropy/","text":"elegy.losses.SparseCategoricalCrossentropy Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Usage: y_true = jnp . array ([ 1 , 2 ]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy () result = scce ( y_true , y_pred ) # 1.177 assert jnp . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( y_true , y_pred , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert jnp . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = scce ( y_true , y_pred ) # 2.354 assert jnp . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = scce ( y_true , y_pred ) # [0.0513, 2.303] assert jnp . all ( jnp . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = lelegy . losses . SparseCategoricalCrossentropy (), metrics = lelegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , from_logits = False , reduction = None , weight = None , on = None , ** kwargs ) special Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/sparse_categorical_crossentropy.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , from_logits : bool = False , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/sparse_categorical_crossentropy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default y_true Ground truth values. required y_pred The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/sparse_categorical_crossentropy.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def call ( self , y_true , y_pred , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return sparse_categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/sparse_categorical_crossentropy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/sparse_categorical_crossentropy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/sparse_categorical_crossentropy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/sparse_categorical_crossentropy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/sparse_categorical_crossentropy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/sparse_categorical_crossentropy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegylossessparsecategoricalcrossentropy","text":"","title":"elegy.losses.SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy","text":"Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Usage: y_true = jnp . array ([ 1 , 2 ]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy () result = scce ( y_true , y_pred ) # 1.177 assert jnp . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( y_true , y_pred , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert jnp . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = scce ( y_true , y_pred ) # 2.354 assert jnp . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = scce ( y_true , y_pred ) # [0.0513, 2.303] assert jnp . all ( jnp . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = lelegy . losses . SparseCategoricalCrossentropy (), metrics = lelegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.__init__","text":"Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/sparse_categorical_crossentropy.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , from_logits : bool = False , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits","title":"__init__()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/losses/sparse_categorical_crossentropy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.call","text":"Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default y_true Ground truth values. required y_pred The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/sparse_categorical_crossentropy.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def call ( self , y_true , y_pred , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return sparse_categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"call()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/sparse_categorical_crossentropy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/losses/sparse_categorical_crossentropy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/losses/sparse_categorical_crossentropy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/losses/sparse_categorical_crossentropy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/sparse_categorical_crossentropy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/losses/sparse_categorical_crossentropy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/losses/binary_crossentropy/","text":"elegy.losses.binary_crossentropy","title":"binary_crossentropy"},{"location":"api/losses/binary_crossentropy/#elegylossesbinary_crossentropy","text":"","title":"elegy.losses.binary_crossentropy"},{"location":"api/losses/binary_crossentropy/#elegy.losses.binary_crossentropy","text":"","title":"elegy.losses.binary_crossentropy"},{"location":"api/losses/mean_absolute_error/","text":"elegy.losses.mean_absolute_error","title":"mean_absolute_error"},{"location":"api/losses/mean_absolute_error/#elegylossesmean_absolute_error","text":"","title":"elegy.losses.mean_absolute_error"},{"location":"api/losses/mean_absolute_error/#elegy.losses.mean_absolute_error","text":"","title":"elegy.losses.mean_absolute_error"},{"location":"api/losses/mean_percentage_absolute_error/","text":"elegy.losses.mean_percentage_absolute_error Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_percentage_absolute_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , 100. * jnp . mean ( jnp . abs (( y_pred - y_true ) / jnp . clip ( y_true , utils . EPSILON , None )))) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_percentage_error.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def mean_percentage_absolute_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_percentage_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((y_pred - y_true) / jnp.clip(y_true, utils.EPSILON, None)))) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) diff = jnp . abs (( y_pred - y_true ) / jnp . maximum ( jnp . abs ( y_true ), utils . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"mean_percentage_absolute_error"},{"location":"api/losses/mean_percentage_absolute_error/#elegylossesmean_percentage_absolute_error","text":"","title":"elegy.losses.mean_percentage_absolute_error"},{"location":"api/losses/mean_percentage_absolute_error/#elegy.losses.mean_absolute_percentage_error.mean_percentage_absolute_error","text":"Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_percentage_absolute_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , 100. * jnp . mean ( jnp . abs (( y_pred - y_true ) / jnp . clip ( y_true , utils . EPSILON , None )))) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_percentage_error.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def mean_percentage_absolute_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_percentage_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((y_pred - y_true) / jnp.clip(y_true, utils.EPSILON, None)))) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) diff = jnp . abs (( y_pred - y_true ) / jnp . maximum ( jnp . abs ( y_true ), utils . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"elegy.losses.mean_absolute_percentage_error.mean_percentage_absolute_error"},{"location":"api/losses/mean_squared_error/","text":"elegy.losses.mean_squared_error","title":"mean_squared_error"},{"location":"api/losses/mean_squared_error/#elegylossesmean_squared_error","text":"","title":"elegy.losses.mean_squared_error"},{"location":"api/losses/mean_squared_error/#elegy.losses.mean_squared_error","text":"","title":"elegy.losses.mean_squared_error"},{"location":"api/losses/sparse_categorical_crossentropy/","text":"elegy.losses.sparse_categorical_crossentropy","title":"sparse_categorical_crossentropy"},{"location":"api/losses/sparse_categorical_crossentropy/#elegylossessparse_categorical_crossentropy","text":"","title":"elegy.losses.sparse_categorical_crossentropy"},{"location":"api/losses/sparse_categorical_crossentropy/#elegy.losses.sparse_categorical_crossentropy","text":"","title":"elegy.losses.sparse_categorical_crossentropy"},{"location":"api/metrics/Accuracy/","text":"elegy.metrics.Accuracy Calculates how often predictions equals labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. accuracy = elegy . metrics . Accuracy () result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ]) ) assert result == 0.75 # 3 / 4 result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ]) ) assert result == 0.5 # 4 / 8 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , on = None , ** kwargs ) special Creates a Accuracy instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/accuracy.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `Accuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/accuracy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/accuracy.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/accuracy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/accuracy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/accuracy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/accuracy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/accuracy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/accuracy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Accuracy"},{"location":"api/metrics/Accuracy/#elegymetricsaccuracy","text":"","title":"elegy.metrics.Accuracy"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy","text":"Calculates how often predictions equals labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. accuracy = elegy . metrics . Accuracy () result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ]) ) assert result == 0.75 # 3 / 4 result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ]) ) assert result == 0.5 # 4 / 8 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"elegy.metrics.accuracy.Accuracy"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.__init__","text":"Creates a Accuracy instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/accuracy.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `Accuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/accuracy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/accuracy.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/accuracy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/accuracy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/accuracy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/accuracy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/accuracy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/accuracy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/metrics/BinaryAccuracy/","text":"elegy.metrics.BinaryAccuracy Calculates how often predictions matches binary labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Parameters: Name Type Description Default name (Optional) string name of the metric instance. required dtype (Optional) data type of the metric result. required threshold (Optional) Float representing the threshold for deciding whether prediction values are 1 or 0. required Standalone usage: m = elegy . metrics . BinaryAccuracy () result = m ( y_true = np . array ([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]), y_pred = np . array ([[ 0.98 ], [ 1 ], [ 0 ], [ 0.6 ]]), ) assert result == 0.75 m = elegy . metrics . BinaryAccuracy () result = m ( y_true = np . array ([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]), y_pred = np . array ([[ 0.98 ], [ 1 ], [ 0 ], [ 0.6 ]]), sample_weight = np . array ([ 1 , 0 , 0 , 1 ]), ) assert result == 0.5 Usage with Model API: model = elegy . Model ( ... metrics = [ tf . keras . metrics . BinaryAccuracy ()], ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , threshold = 0.5 , on = None , ** kwargs ) special Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default threshold float Float representing the threshold for deciding whether prediction values are 1 or 0. 0.5 on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/binary_accuracy.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , threshold : float = 0.5 , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: threshold: Float representing the threshold for deciding whether prediction values are 1 or 0. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = threshold apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/binary_accuracy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/binary_accuracy.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = binary_accuracy ( y_true = y_true , y_pred = y_pred , threshold = self . threshold ), sample_weight = sample_weight , ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/binary_accuracy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/binary_accuracy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/binary_accuracy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/binary_accuracy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/binary_accuracy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/binary_accuracy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"BinaryAccuracy"},{"location":"api/metrics/BinaryAccuracy/#elegymetricsbinaryaccuracy","text":"","title":"elegy.metrics.BinaryAccuracy"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy","text":"Calculates how often predictions matches binary labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Parameters: Name Type Description Default name (Optional) string name of the metric instance. required dtype (Optional) data type of the metric result. required threshold (Optional) Float representing the threshold for deciding whether prediction values are 1 or 0. required Standalone usage: m = elegy . metrics . BinaryAccuracy () result = m ( y_true = np . array ([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]), y_pred = np . array ([[ 0.98 ], [ 1 ], [ 0 ], [ 0.6 ]]), ) assert result == 0.75 m = elegy . metrics . BinaryAccuracy () result = m ( y_true = np . array ([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]), y_pred = np . array ([[ 0.98 ], [ 1 ], [ 0 ], [ 0.6 ]]), sample_weight = np . array ([ 1 , 0 , 0 , 1 ]), ) assert result == 0.5 Usage with Model API: model = elegy . Model ( ... metrics = [ tf . keras . metrics . BinaryAccuracy ()], )","title":"elegy.metrics.binary_accuracy.BinaryAccuracy"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.__init__","text":"Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default threshold float Float representing the threshold for deciding whether prediction values are 1 or 0. 0.5 on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/binary_accuracy.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , threshold : float = 0.5 , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: threshold: Float representing the threshold for deciding whether prediction values are 1 or 0. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = threshold","title":"__init__()"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/binary_accuracy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/binary_accuracy.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = binary_accuracy ( y_true = y_true , y_pred = y_pred , threshold = self . threshold ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/binary_accuracy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/binary_accuracy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/binary_accuracy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/binary_accuracy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/binary_accuracy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/binary_accuracy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/metrics/BinaryCrossentropy/","text":"elegy.metrics.BinaryCrossentropy Computes the crossentropy metric between the labels and predictions. This is the crossentropy metric class to be used when there are only two label classes (0 and 1). Usage: y_true = jnp . array ([[ 0. , 1. ], [ 0. , 0. ]]), y_pred = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) bce = elegy . metrics . BinaryCrossentropy () result = bce ( y_true = y_true , y_pred = y_pred , ) assert jnp . isclose ( result , 0.815 , rtol = 0.01 ) # BCE using sample_weight bce = elegy . metrics . BinaryCrossentropy () result = bce ( y_true , y_pred , sample_weight = jnp . array ([ 1. , 0. ])) assert jnp . isclose ( result , 0.916 , rtol = 0.01 ) Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . BinaryCrossentropy (), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , from_logits = False , on = None , ** kwargs ) special Creates a BinaryCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/binary_crossentropy.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , from_logits : bool = False , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `BinaryCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . _from_logits = from_logits apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/binary_crossentropy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/binary_crossentropy.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = binary_crossentropy ( y_true = y_true , y_pred = y_pred , from_logits = self . _from_logits ), sample_weight = sample_weight , ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/binary_crossentropy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/binary_crossentropy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/binary_crossentropy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/binary_crossentropy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/binary_crossentropy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/binary_crossentropy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"BinaryCrossentropy"},{"location":"api/metrics/BinaryCrossentropy/#elegymetricsbinarycrossentropy","text":"","title":"elegy.metrics.BinaryCrossentropy"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy","text":"Computes the crossentropy metric between the labels and predictions. This is the crossentropy metric class to be used when there are only two label classes (0 and 1). Usage: y_true = jnp . array ([[ 0. , 1. ], [ 0. , 0. ]]), y_pred = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) bce = elegy . metrics . BinaryCrossentropy () result = bce ( y_true = y_true , y_pred = y_pred , ) assert jnp . isclose ( result , 0.815 , rtol = 0.01 ) # BCE using sample_weight bce = elegy . metrics . BinaryCrossentropy () result = bce ( y_true , y_pred , sample_weight = jnp . array ([ 1. , 0. ])) assert jnp . isclose ( result , 0.916 , rtol = 0.01 ) Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . BinaryCrossentropy (), )","title":"elegy.metrics.binary_crossentropy.BinaryCrossentropy"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.__init__","text":"Creates a BinaryCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/binary_crossentropy.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , from_logits : bool = False , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `BinaryCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . _from_logits = from_logits","title":"__init__()"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/binary_crossentropy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/binary_crossentropy.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = binary_crossentropy ( y_true = y_true , y_pred = y_pred , from_logits = self . _from_logits ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/binary_crossentropy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/binary_crossentropy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/binary_crossentropy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/binary_crossentropy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/binary_crossentropy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/binary_crossentropy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/metrics/CategoricalAccuracy/","text":"elegy.metrics.CategoricalAccuracy Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . CategoricalAccuracy () result = accuracy ( y_true = jnp . array ([[ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . CategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , on = None , ** kwargs ) special Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/categorical_accuracy.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/categorical_accuracy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/categorical_accuracy.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/categorical_accuracy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/categorical_accuracy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/categorical_accuracy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/categorical_accuracy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/categorical_accuracy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/categorical_accuracy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegymetricscategoricalaccuracy","text":"","title":"elegy.metrics.CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy","text":"Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . CategoricalAccuracy () result = accuracy ( y_true = jnp . array ([[ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . CategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"elegy.metrics.categorical_accuracy.CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.__init__","text":"Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/categorical_accuracy.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/categorical_accuracy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/categorical_accuracy.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/categorical_accuracy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/categorical_accuracy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/categorical_accuracy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/categorical_accuracy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/categorical_accuracy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/categorical_accuracy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/metrics/Mean/","text":"elegy.metrics.Mean Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , on = None , ** kwargs ) special Creates a Mean instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean.py 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\"Creates a `Mean` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . WEIGHTED_MEAN , on = on , ** kwargs ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/mean.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , values , sample_weight = None ) Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ndarray Array with the cumulative mean. Source code in elegy/metrics/mean.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" return super () . call ( values = values , sample_weight = sample_weight ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/mean.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/mean.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/mean.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/mean.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/mean.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/mean.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Mean"},{"location":"api/metrics/Mean/#elegymetricsmean","text":"","title":"elegy.metrics.Mean"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean","text":"Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), )","title":"elegy.metrics.mean.Mean"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.__init__","text":"Creates a Mean instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean.py 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\"Creates a `Mean` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . WEIGHTED_MEAN , on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/mean.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.call","text":"Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ndarray Array with the cumulative mean. Source code in elegy/metrics/mean.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" return super () . call ( values = values , sample_weight = sample_weight )","title":"call()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/mean.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/mean.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/mean.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/mean.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/mean.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/mean.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/metrics/MeanAbsoluteError/","text":"elegy.metrics.MeanAbsoluteError Computes the cumulative mean absolute error between y_true and y_pred . Usage: mae = elegy . metrics . MeanAbsoluteError () result = mae ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mae ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . MeanAbsoluteError (), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , on = None , ** kwargs ) special Creates a MeanAbsoluteError instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_absolute_error.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanAbsoluteError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/mean_absolute_error.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_absolute_error.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_absolute_error ( y_true = y_true , y_pred = y_pred )) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/mean_absolute_error.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/mean_absolute_error.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/mean_absolute_error.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/mean_absolute_error.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/mean_absolute_error.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/mean_absolute_error.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#elegymetricsmeanabsoluteerror","text":"","title":"elegy.metrics.MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError","text":"Computes the cumulative mean absolute error between y_true and y_pred . Usage: mae = elegy . metrics . MeanAbsoluteError () result = mae ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mae ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . MeanAbsoluteError (), )","title":"elegy.metrics.mean_absolute_error.MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.__init__","text":"Creates a MeanAbsoluteError instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_absolute_error.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanAbsoluteError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/mean_absolute_error.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_absolute_error.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_absolute_error ( y_true = y_true , y_pred = y_pred ))","title":"call()"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/mean_absolute_error.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/mean_absolute_error.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/mean_absolute_error.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/mean_absolute_error.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/mean_absolute_error.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/mean_absolute_error.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/metrics/MeanSquaredError/","text":"elegy.metrics.MeanSquaredError Computes the cumulative mean squared error between y_true and y_pred . Usage: mse = elegy . metrics . MeanSquaredError () result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . MeanSquaredError (), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , on = None , ** kwargs ) special Creates a MeanSquaredError instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_squared_error.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanSquaredError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/mean_squared_error.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_squared_error.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_squared_error ( y_true = y_true , y_pred = y_pred )) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/mean_squared_error.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/mean_squared_error.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/mean_squared_error.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/mean_squared_error.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/mean_squared_error.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/mean_squared_error.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegymetricsmeansquarederror","text":"","title":"elegy.metrics.MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError","text":"Computes the cumulative mean squared error between y_true and y_pred . Usage: mse = elegy . metrics . MeanSquaredError () result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . MeanSquaredError (), )","title":"elegy.metrics.mean_squared_error.MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.__init__","text":"Creates a MeanSquaredError instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_squared_error.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanSquaredError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/mean_squared_error.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_squared_error.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_squared_error ( y_true = y_true , y_pred = y_pred ))","title":"call()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/mean_squared_error.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/mean_squared_error.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/mean_squared_error.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/mean_squared_error.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/mean_squared_error.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/mean_squared_error.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/metrics/Metric/","text":"elegy.metrics.Metric Encapsulates metric logic and state. Usage: m = SomeMetric ( ... ) for input in ... : result = m ( input ) print ( 'Final result: ' , result ) Usage with the Model API: class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), ], metrics = [ elegy . metrics . SparseCategoricalAccuracy () ], optimizer = optax . rmsprop ( 1e-3 ), ) To be implemented by subclasses: call() : All state variables should be created in this method by calling haiku.get_state() , update this state by calling haiku.set_state(...) , and return a result based on these states. Example subclass implementation: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = hk . get_state ( \"total\" , [], jnp . zeros ) count = hk . get_state ( \"count\" , [], jnp . zeros ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) hk . set_state ( \"total\" , total ) hk . set_state ( \"count\" , count ) return total / count submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , on = None , ** kwargs ) special Base Metric constructor. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/metrics/metric.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Base Metric constructor. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/metric.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/metric.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/metric.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/metric.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/metric.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/metric.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/metric.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Metric"},{"location":"api/metrics/Metric/#elegymetricsmetric","text":"","title":"elegy.metrics.Metric"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric","text":"Encapsulates metric logic and state. Usage: m = SomeMetric ( ... ) for input in ... : result = m ( input ) print ( 'Final result: ' , result ) Usage with the Model API: class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), ], metrics = [ elegy . metrics . SparseCategoricalAccuracy () ], optimizer = optax . rmsprop ( 1e-3 ), ) To be implemented by subclasses: call() : All state variables should be created in this method by calling haiku.get_state() , update this state by calling haiku.set_state(...) , and return a result based on these states. Example subclass implementation: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = hk . get_state ( \"total\" , [], jnp . zeros ) count = hk . get_state ( \"count\" , [], jnp . zeros ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) hk . set_state ( \"total\" , total ) hk . set_state ( \"count\" , count ) return total / count","title":"elegy.metrics.metric.Metric"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric.__init__","text":"Base Metric constructor. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/metrics/metric.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Base Metric constructor. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"__init__()"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/metric.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/metric.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/metric.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/metric.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/metric.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/metric.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/metric.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/metrics/Reduce/","text":"elegy.metrics.Reduce Encapsulates metrics that perform a reduce operation on the values. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/reduce.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , values , sample_weight = None ) Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ndarray Array with the cummulative reduce. Source code in elegy/metrics/reduce.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cummulative reduce. \"\"\" total = hooks . get_state ( \"total\" , shape = [], dtype = self . dtype , initializer = initializers . Constant ( 0 ) ) if self . _reduction in ( Reduction . SUM_OVER_BATCH_SIZE , Reduction . WEIGHTED_MEAN , ): count = hooks . get_state ( \"count\" , shape = [], dtype = jnp . int32 , initializer = initializers . Constant ( 0 ), ) else : count = None value , total , count = reduce ( total = total , count = count , values = values , reduction = self . _reduction , sample_weight = sample_weight , dtype = self . dtype , ) hooks . set_state ( \"total\" , total ) if count is not None : hooks . set_state ( \"count\" , count ) return value get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/reduce.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/reduce.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/reduce.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/reduce.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/reduce.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/reduce.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Reduce"},{"location":"api/metrics/Reduce/#elegymetricsreduce","text":"","title":"elegy.metrics.Reduce"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce","text":"Encapsulates metrics that perform a reduce operation on the values.","title":"elegy.metrics.reduce.Reduce"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/reduce.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce.call","text":"Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ndarray Array with the cummulative reduce. Source code in elegy/metrics/reduce.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cummulative reduce. \"\"\" total = hooks . get_state ( \"total\" , shape = [], dtype = self . dtype , initializer = initializers . Constant ( 0 ) ) if self . _reduction in ( Reduction . SUM_OVER_BATCH_SIZE , Reduction . WEIGHTED_MEAN , ): count = hooks . get_state ( \"count\" , shape = [], dtype = jnp . int32 , initializer = initializers . Constant ( 0 ), ) else : count = None value , total , count = reduce ( total = total , count = count , values = values , reduction = self . _reduction , sample_weight = sample_weight , dtype = self . dtype , ) hooks . set_state ( \"total\" , total ) if count is not None : hooks . set_state ( \"count\" , count ) return value","title":"call()"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/reduce.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/reduce.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/reduce.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/reduce.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/reduce.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/reduce.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/metrics/SparseCategoricalAccuracy/","text":"elegy.metrics.SparseCategoricalAccuracy Calculates how often predictions matches integer labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . SparseCategoricalAccuracy () result = accuracy ( y_true = jnp . array ([ 2 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]) ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([ 1 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , on = None , ** kwargs ) special Creates a SparseCategoricalAccuracy instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/sparse_categorical_accuracy.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `SparseCategoricalAccuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/sparse_categorical_accuracy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape except y_true should not have the last dimension of y_pred . Parameters: Name Type Description Default y_true ndarray Sparse ground truth values. shape = [batch_size, d0, .. dN-1] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN-1, dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/sparse_categorical_accuracy.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape except `y_true` should not have the last dimension of `y_pred`. Arguments: y_true: Sparse ground truth values. shape = `[batch_size, d0, .. dN-1]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN-1, dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = sparse_categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/sparse_categorical_accuracy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/sparse_categorical_accuracy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/sparse_categorical_accuracy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/sparse_categorical_accuracy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/sparse_categorical_accuracy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/sparse_categorical_accuracy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegymetricssparsecategoricalaccuracy","text":"","title":"elegy.metrics.SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy","text":"Calculates how often predictions matches integer labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . SparseCategoricalAccuracy () result = accuracy ( y_true = jnp . array ([ 2 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]) ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([ 1 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.__init__","text":"Creates a SparseCategoricalAccuracy instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/sparse_categorical_accuracy.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `SparseCategoricalAccuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/sparse_categorical_accuracy.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape except y_true should not have the last dimension of y_pred . Parameters: Name Type Description Default y_true ndarray Sparse ground truth values. shape = [batch_size, d0, .. dN-1] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN-1, dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/sparse_categorical_accuracy.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape except `y_true` should not have the last dimension of `y_pred`. Arguments: y_true: Sparse ground truth values. shape = `[batch_size, d0, .. dN-1]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN-1, dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = sparse_categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/sparse_categorical_accuracy.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/sparse_categorical_accuracy.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/sparse_categorical_accuracy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/sparse_categorical_accuracy.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/sparse_categorical_accuracy.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/sparse_categorical_accuracy.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/metrics/Sum/","text":"elegy.metrics.Sum Computes the (weighted) sum of the given values. For example, if values is [1, 3, 5, 7] then the sum is 16. If the weights were specified as [1, 1, 0, 0] then the sum would be 4. This metric creates one variable, total , that is used to compute the sum of values . This is ultimately returned as sum . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: m = elegy . metrics . Sum () assert 16.0 == m ([ 1 , 3 , 5 , 7 ]) Usage with Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Sum ( name = 'sum_1' ), ) model = elegy . Model ( inputs , outputs ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , on = None , ** kwargs ) special Creates a Sum instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/sum.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\"Creates a `Sum` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . SUM , on = on , ** kwargs ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/sum.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , values , sample_weight = None ) inherited Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ndarray Array with the cummulative reduce. Source code in elegy/metrics/sum.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cummulative reduce. \"\"\" total = hooks . get_state ( \"total\" , shape = [], dtype = self . dtype , initializer = initializers . Constant ( 0 ) ) if self . _reduction in ( Reduction . SUM_OVER_BATCH_SIZE , Reduction . WEIGHTED_MEAN , ): count = hooks . get_state ( \"count\" , shape = [], dtype = jnp . int32 , initializer = initializers . Constant ( 0 ), ) else : count = None value , total , count = reduce ( total = total , count = count , values = values , reduction = self . _reduction , sample_weight = sample_weight , dtype = self . dtype , ) hooks . set_state ( \"total\" , total ) if count is not None : hooks . set_state ( \"count\" , count ) return value get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/sum.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/sum.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/sum.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/sum.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/sum.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/sum.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Sum"},{"location":"api/metrics/Sum/#elegymetricssum","text":"","title":"elegy.metrics.Sum"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum","text":"Computes the (weighted) sum of the given values. For example, if values is [1, 3, 5, 7] then the sum is 16. If the weights were specified as [1, 1, 0, 0] then the sum would be 4. This metric creates one variable, total , that is used to compute the sum of values . This is ultimately returned as sum . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: m = elegy . metrics . Sum () assert 16.0 == m ([ 1 , 3 , 5 , 7 ]) Usage with Elegy API: model = elegy . Model ( module_fn , loss = elegy . losses . CategoricalCrossentropy (), metrics = elegy . metrics . Sum ( name = 'sum_1' ), ) model = elegy . Model ( inputs , outputs )","title":"elegy.metrics.sum.Sum"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.__init__","text":"Creates a Sum instance. Parameters: Name Type Description Default on Optional[Union[str, int, Iterable[Union[str, int]]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/sum.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\"Creates a `Sum` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . SUM , on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/metrics/sum.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.call","text":"Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ndarray Array with the cummulative reduce. Source code in elegy/metrics/sum.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cummulative reduce. \"\"\" total = hooks . get_state ( \"total\" , shape = [], dtype = self . dtype , initializer = initializers . Constant ( 0 ) ) if self . _reduction in ( Reduction . SUM_OVER_BATCH_SIZE , Reduction . WEIGHTED_MEAN , ): count = hooks . get_state ( \"count\" , shape = [], dtype = jnp . int32 , initializer = initializers . Constant ( 0 ), ) else : count = None value , total , count = reduce ( total = total , count = count , values = values , reduction = self . _reduction , sample_weight = sample_weight , dtype = self . dtype , ) hooks . set_state ( \"total\" , total ) if count is not None : hooks . set_state ( \"count\" , count ) return value","title":"call()"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/sum.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/metrics/sum.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/metrics/sum.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/metrics/sum.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/sum.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/metrics/sum.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/metrics/accuracy/","text":"elegy.metrics.accuracy","title":"accuracy"},{"location":"api/metrics/accuracy/#elegymetricsaccuracy","text":"","title":"elegy.metrics.accuracy"},{"location":"api/metrics/accuracy/#elegy.metrics.accuracy","text":"","title":"elegy.metrics.accuracy"},{"location":"api/metrics/binary_accuracy/","text":"elegy.metrics.binary_accuracy","title":"binary_accuracy"},{"location":"api/metrics/binary_accuracy/#elegymetricsbinary_accuracy","text":"","title":"elegy.metrics.binary_accuracy"},{"location":"api/metrics/binary_accuracy/#elegy.metrics.binary_accuracy","text":"","title":"elegy.metrics.binary_accuracy"},{"location":"api/metrics/binary_crossentropy/","text":"elegy.metrics.binary_crossentropy","title":"binary_crossentropy"},{"location":"api/metrics/binary_crossentropy/#elegymetricsbinary_crossentropy","text":"","title":"elegy.metrics.binary_crossentropy"},{"location":"api/metrics/binary_crossentropy/#elegy.metrics.binary_crossentropy","text":"","title":"elegy.metrics.binary_crossentropy"},{"location":"api/metrics/categorical_accuracy/","text":"elegy.metrics.categorical_accuracy","title":"categorical_accuracy"},{"location":"api/metrics/categorical_accuracy/#elegymetricscategorical_accuracy","text":"","title":"elegy.metrics.categorical_accuracy"},{"location":"api/metrics/categorical_accuracy/#elegy.metrics.categorical_accuracy","text":"","title":"elegy.metrics.categorical_accuracy"},{"location":"api/metrics/mean_absolute_error/","text":"elegy.metrics.mean_absolute_error","title":"mean_absolute_error"},{"location":"api/metrics/mean_absolute_error/#elegymetricsmean_absolute_error","text":"","title":"elegy.metrics.mean_absolute_error"},{"location":"api/metrics/mean_absolute_error/#elegy.metrics.mean_absolute_error","text":"","title":"elegy.metrics.mean_absolute_error"},{"location":"api/metrics/mean_squared_error/","text":"elegy.metrics.mean_squared_error","title":"mean_squared_error"},{"location":"api/metrics/mean_squared_error/#elegymetricsmean_squared_error","text":"","title":"elegy.metrics.mean_squared_error"},{"location":"api/metrics/mean_squared_error/#elegy.metrics.mean_squared_error","text":"","title":"elegy.metrics.mean_squared_error"},{"location":"api/metrics/reduce/","text":"elegy.metrics.reduce","title":"reduce"},{"location":"api/metrics/reduce/#elegymetricsreduce","text":"","title":"elegy.metrics.reduce"},{"location":"api/metrics/reduce/#elegy.metrics.reduce","text":"","title":"elegy.metrics.reduce"},{"location":"api/metrics/sparse_categorical_accuracy/","text":"elegy.metrics.sparse_categorical_accuracy","title":"sparse_categorical_accuracy"},{"location":"api/metrics/sparse_categorical_accuracy/#elegymetricssparse_categorical_accuracy","text":"","title":"elegy.metrics.sparse_categorical_accuracy"},{"location":"api/metrics/sparse_categorical_accuracy/#elegy.metrics.sparse_categorical_accuracy","text":"","title":"elegy.metrics.sparse_categorical_accuracy"},{"location":"api/model/Model/","text":"elegy.model.Model Model is tasked with performing training, evaluation, and inference for a given elegy.Module or haiku.Module . To create a Model you first have to define its architecture in a Module : class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) Then you can pass this Module to the Model 's constructor and specify additional things like losses, metrics, optimizer, and callbacks: model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . rmsprop ( 1e-3 ), ) Once the model is created, you can train the model with model.fit() , or use the model to do prediction with model.predict() . Checkout Getting Started for additional details. Attributes: Name Type Description parameters A haiku.Params structure with the weights of the model. states A haiku.State structure with non-trainable parameters of the model. optimizer_state Optional[NamedTuple] A optax.OptState structure with states of the optimizer. metrics_states A haiku.State structure with the states of the metrics. initial_metrics_state Optional[Dict] A haiku.State structure with the initial states of the metrics. run_eagerly bool Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code, instead of using Jax's jit to optimize the computation. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. seed: ndarray property writable Current random states of the model. evaluate ( self , x , y = None , verbose = 1 , batch_size = None , sample_weight = None , steps = None , callbacks = None ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required y Optional[Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 1 batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None steps Optional[int] Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . This argument is not supported with array inputs. None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.Model.fit]. Returns: Type Description Dict[str, numpy.ndarray] A dictionary for mapping the losses and metrics names to the values obtained. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model.py 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 def evaluate ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], y : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. This argument is not supported with array inputs. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.Model.fit]. Returns: A dictionary for mapping the losses and metrics names to the values obtained. Raises: ValueError: in case of invalid arguments. \"\"\" data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , training = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . test_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 1 , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 ) Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable]] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for generator type is given below. None y Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator. 0.0 validation_data Optional[Union[Tuple, Iterable]] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy/Jax arrays, list of arrays or mappings tuple (x_val, y_val, val_sample_weights) of Numpy/Jax arrays, list of arrays or mappings generator For the first two cases, batch_size must be provided. For the last case, validation_steps should be provided, and should follow the same convention for yielding data as x . Note that validation_data does not support all the data types that are supported in x , eg, dict. None shuffle bool Boolean (whether to shuffle the training data before each epoch). This argument is ignored when x is a generator. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of generators (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description History A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model.py 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 def fit ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , None , ] = None , y : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ np . ndarray ] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , ) -> History : \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for generator type is given below. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a generator. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy/Jax arrays, list of arrays or mappings - tuple `(x_val, y_val, val_sample_weights)` of Numpy/Jax arrays, list of arrays or mappings - generator For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` should be provided, and should follow the same convention for yielding data as `x`. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict. shuffle: Boolean (whether to shuffle the training data before each epoch). This argument is ignored when `x` is a generator. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of generators (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if x is None : x = {} if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) # sample_weight = batch[2] if len(batch) == 3 else None x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . train_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) epoch_logs = copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) # print( # f\"epoch: {epoch} - \" # + \" - \".join(f\"{key}: {value:.3f}\" for key, value in epoch_logs.items()) # ) if self . stop_training : break callbacks . on_train_end () return self . history load ( self , path ) Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the haiku.Params + haiku.State structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Source code in elegy/model.py 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 def load ( self , path : tp . Union [ str , Path ]) -> None : \"\"\" Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the `haiku.Params` + `haiku.State` structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Arguments: path: path to a saved model's directory. \"\"\" if isinstance ( path , str ): path = Path ( path ) states : tp . Dict = deepdish . io . load ( path / \"parameters.h5\" ) optimizer_state_path = path / \"optimizer_state.pkl\" if optimizer_state_path . exists (): with open ( optimizer_state_path , \"rb\" ) as f : states [ \"optimizer_state\" ] = pickle . load ( f ) self . full_state = states predict ( self , x , verbose = 0 , batch_size = None , steps = None , callbacks = None ) Generates output predictions for the input samples. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generators (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.Model.fit]. Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description ndarray Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model.py 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 def predict ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> np . ndarray : \"\"\"Generates output predictions for the input samples. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generators (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.Model.fit]. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs , ) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. required Returns: Type Description Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Jax array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between given number of inputs and expectations of the model. Source code in elegy/model.py 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 def predict_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ] ) -> tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ]: \"\"\" Returns predictions for a single batch of samples. Arguments: x: Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. Returns: Jax array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. \"\"\" self . _maybe_initialize ( mode = Mode . predict , x = x , y = None , sample_weight = None , class_weight = None ) assert self . parameters is not None assert self . states is not None y_pred , context = self . _predict ( False , # training False , # get_summaries x = x , parameters = self . parameters , states = self . states , rng = next ( self . _rngs ), ) return y_pred save ( self , path , include_optimizer = True ) Saves the model to disk. It creates a directory that includes: The Model object instance serialized with pickle as as {path}/model.pkl , this allows you to re-instantiate the model later. The model parameters + states serialized into HDF5 as {path}/parameters.h5 . The states of the optimizer serialized with pickle as as {path}/optimizer_state.pkl , allowing to resume training exactly where you left off. We hope to use HDF5 in the future but optax states is incompatible with deepdish . This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via Model.load if the model is already instiated or elegy.model.load to load the model instance from its pickled version. import elegy model . save ( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load ( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path where model structure will be saved. required include_optimizer bool If True, save optimizer's states together. True Source code in elegy/model.py 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 def save ( self , path : tp . Union [ str , Path ], include_optimizer : bool = True ) -> None : \"\"\" Saves the model to disk. It creates a directory that includes: - The `Model` object instance serialized with `pickle` as as `{path}/model.pkl`, this allows you to re-instantiate the model later. - The model parameters + states serialized into HDF5 as `{path}/parameters.h5`. - The states of the optimizer serialized with `pickle` as as `{path}/optimizer_state.pkl`, allowing to resume training exactly where you left off. We hope to use HDF5 in the future but `optax` states is incompatible with `deepdish`. This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via `Model.load` if the model is already instiated or `elegy.model.load` to load the model instance from its pickled version. ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path where model structure will be saved. include_optimizer: If True, save optimizer's states together. \"\"\" if isinstance ( path , str ): path = Path ( path ) path . mkdir ( parents = True , exist_ok = True ) states = self . full_state original_state = copy ( states ) states . pop ( \"metrics_states\" , None ) states . pop ( \"initial_metrics_state\" , None ) optimizer_state = states . pop ( \"optimizer_state\" , None ) deepdish . io . save ( path / \"parameters.h5\" , states ) if include_optimizer and optimizer_state is not None : with open ( path / \"optimizer_state.pkl\" , \"wb\" ) as f : pickle . dump ( optimizer_state , f ) # getting pickle errors self . reset () try : path = path / \"model.pkl\" with open ( path , \"wb\" ) as f : cloudpickle . dump ( self , f ) except BaseException as e : print ( f \"Error occurred saving the model object at { path } \\n Continuing....\" ) self . full_state = original_state summary ( self , x , depth = 2 , tablefmt = 'fancy_grid' , ** tablulate_kwargs ) Prints a summary of the network. Parameters: Name Type Description Default x A sample of inputs to the network. required depth int The level number of nested level which will be showed. Information about summaries from modules deeper than depth will be aggregated together. 2 tablefmt str A string represeting the style of the table generated by tabulate . See python-tabulate for more options. 'fancy_grid' tablulate_kwargs Additional keyword arguments passed to tabulate . See python-tabulate for more options. {} Source code in elegy/model.py 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 def summary ( self , x , depth : int = 2 , tablefmt : str = \"fancy_grid\" , ** tablulate_kwargs ): \"\"\" Prints a summary of the network. Arguments: x: A sample of inputs to the network. depth: The level number of nested level which will be showed. Information about summaries from modules deeper than `depth` will be aggregated together. tablefmt: A string represeting the style of the table generated by `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. tablulate_kwargs: Additional keyword arguments passed to `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. \"\"\" self . _maybe_initialize ( mode = Mode . predict , x = x , y = None , sample_weight = None , class_weight = None , ) assert self . parameters is not None assert self . states is not None y_pred , context = self . _predict ( training = False , get_summaries = True , x = x , parameters = self . parameters , states = self . states , rng = next ( self . _rngs ), ) def format_output ( outputs ) -> str : file = StringIO () outputs = jax . tree_map ( lambda x : f \" { x . shape } {{ pad }} { x . dtype } \" , outputs ) yaml . safe_dump ( outputs , file , default_flow_style = False , indent = 2 , explicit_end = False ) return file . getvalue () . replace ( \" \\n ...\" , \"\" ) def format_size ( size ): return ( f \" { size / 1e9 : ,.1f } GB\" if size > 1e9 else f \" { size / 1e6 : ,.1f } MB\" if size > 1e6 else f \" { size / 1e3 : ,.1f } KB\" if size > 1e3 else f \" { size : , } B\" ) table : tp . List = [[ \"Inputs\" , format_output ( x ), \"0\" , \"0\" ]] for module , base_name , value in context . summaries : base_name_parts = base_name . split ( \"/\" )[ 1 :] module_depth = len ( base_name_parts ) if module_depth > depth : continue include_submodules = module_depth == depth params_count = ( module . parameters_size ( include_submodules ) if module is not None else 0 ) params_size = ( module . parameters_bytes ( include_submodules ) if module is not None else 0 ) states_count = ( module . states_size ( include_submodules ) if module is not None else 0 ) states_size = ( module . states_bytes ( include_submodules ) if module is not None else 0 ) class_name = f \"( { module . __class__ . __name__ } )\" if module is not None else \"\" base_name = \"/\" . join ( base_name_parts ) if not base_name : base_name = \"Outputs\" table . append ( [ f \" { base_name } {{ pad }} { class_name } \" , format_output ( value ), f \" { params_count : , } {{ pad }} { format_size ( params_size ) } \" if params_count > 0 else \"0\" , f \" { states_count : , } {{ pad }} { format_size ( states_size ) } \" if states_count > 0 else \"0\" , ] ) # add papdding for col in range ( 4 ): max_length = max ( len ( line . split ( \" {pad} \" )[ 0 ]) for row in table for line in row [ col ] . split ( \" \\n \" ) ) for row in table : row [ col ] = \" \\n \" . join ( line . format ( pad = \" \" * ( max_length - len ( line . rstrip () . split ( \" {pad} \" )[ 0 ])) ) for line in row [ col ] . rstrip () . split ( \" \\n \" ) ) print ( \" \\n \" + tabulate ( table , headers = [ \"Layer\" , \"Outputs Shape\" , \"Trainable \\n Parameters\" , \"Non-trainable \\n Parameters\" , ], tablefmt = tablefmt , ** tablulate_kwargs , ) ) params_count = self . module . parameters_size () params_size = self . module . parameters_bytes () states_count = self . module . states_size () states_size = self . module . states_bytes () total_count = params_count + states_count total_size = params_size + states_size print ( tabulate ( [ [ f \"Total Parameters:\" , f \" { total_count : , } \" , f \" { format_size ( total_size ) } \" if total_count > 0 else \"\" , ], [ f \"Trainable Parameters:\" , f \" { params_count : , } \" , f \" { format_size ( params_size ) } \" if params_count > 0 else \"\" , ], [ f \"Non-trainable Parameters:\" , f \" { states_count : , } \" , f \" { format_size ( states_size ) } \" if states_count > 0 else \"\" , ], ], tablefmt = \"plain\" , ) + \" \\n \" ) test_on_batch ( self , x , y = None , sample_weight = None , class_weight = None ) Test the model on a single batch of samples. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). None sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None Returns: Type Description Dict[str, jax.numpy.lax_numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model.py 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 def test_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ jnp . ndarray ] = None , class_weight : tp . Optional [ jnp . ndarray ] = None , ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . _maybe_initialize ( mode = Mode . test , x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , ) assert self . parameters is not None assert self . states is not None ( logs , metrics_states ) = self . _test ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , parameters = self . parameters , states = self . states , metrics_states = self . metrics_states , rng = next ( self . _rngs ), ) if metrics_states is not None : self . metrics_states = metrics_states # logs = jax.tree_map(np.asarray, logs) return logs train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None ) Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). It should be consistent with x (you cannot have Numpy inputs and array targets, or inversely). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight Optional[numpy.ndarray] Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None Returns: Type Description Dict[str, numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model.py 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def train_on_batch ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ np . ndarray ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\" Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). It should be consistent with `x` (you cannot have Numpy inputs and array targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . _maybe_initialize ( mode = Mode . train , x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , ) assert self . parameters is not None assert self . states is not None assert self . optimizer_state is not None ( logs , self . parameters , self . states , self . optimizer_state , metrics_states , ) = self . _update ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , parameters = self . parameters , states = self . states , optimizer_state = self . optimizer_state , metrics_states = self . metrics_states , rng = next ( self . _rngs ), ) if metrics_states is not None : self . metrics_states = metrics_states # logs = jax.tree_map(np.asarray, logs) return logs","title":"Model"},{"location":"api/model/Model/#elegymodelmodel","text":"","title":"elegy.model.Model"},{"location":"api/model/Model/#elegy.model.Model","text":"Model is tasked with performing training, evaluation, and inference for a given elegy.Module or haiku.Module . To create a Model you first have to define its architecture in a Module : class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) Then you can pass this Module to the Model 's constructor and specify additional things like losses, metrics, optimizer, and callbacks: model = elegy . Model ( module = MLP (), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . rmsprop ( 1e-3 ), ) Once the model is created, you can train the model with model.fit() , or use the model to do prediction with model.predict() . Checkout Getting Started for additional details. Attributes: Name Type Description parameters A haiku.Params structure with the weights of the model. states A haiku.State structure with non-trainable parameters of the model. optimizer_state Optional[NamedTuple] A optax.OptState structure with states of the optimizer. metrics_states A haiku.State structure with the states of the metrics. initial_metrics_state Optional[Dict] A haiku.State structure with the initial states of the metrics. run_eagerly bool Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code, instead of using Jax's jit to optimize the computation. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls.","title":"elegy.model.Model"},{"location":"api/model/Model/#elegy.model.Model.seed","text":"Current random states of the model.","title":"seed"},{"location":"api/model/Model/#elegy.model.Model.evaluate","text":"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required y Optional[Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 1 batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None steps Optional[int] Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . This argument is not supported with array inputs. None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.Model.fit]. Returns: Type Description Dict[str, numpy.ndarray] A dictionary for mapping the losses and metrics names to the values obtained. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model.py 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 def evaluate ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], y : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. This argument is not supported with array inputs. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.Model.fit]. Returns: A dictionary for mapping the losses and metrics names to the values obtained. Raises: ValueError: in case of invalid arguments. \"\"\" data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , training = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . test_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs","title":"evaluate()"},{"location":"api/model/Model/#elegy.model.Model.fit","text":"Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable]] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for generator type is given below. None y Optional[Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray]]] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator. 0.0 validation_data Optional[Union[Tuple, Iterable]] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy/Jax arrays, list of arrays or mappings tuple (x_val, y_val, val_sample_weights) of Numpy/Jax arrays, list of arrays or mappings generator For the first two cases, batch_size must be provided. For the last case, validation_steps should be provided, and should follow the same convention for yielding data as x . Note that validation_data does not support all the data types that are supported in x , eg, dict. None shuffle bool Boolean (whether to shuffle the training data before each epoch). This argument is ignored when x is a generator. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of generators (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description History A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model.py 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 def fit ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , None , ] = None , y : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], None , ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ np . ndarray ] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , ) -> History : \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for generator type is given below. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a generator. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy/Jax arrays, list of arrays or mappings - tuple `(x_val, y_val, val_sample_weights)` of Numpy/Jax arrays, list of arrays or mappings - generator For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` should be provided, and should follow the same convention for yielding data as `x`. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict. shuffle: Boolean (whether to shuffle the training data before each epoch). This argument is ignored when `x` is a generator. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of generators (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if x is None : x = {} if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) # sample_weight = batch[2] if len(batch) == 3 else None x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) tmp_logs = self . train_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) epoch_logs = copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) # print( # f\"epoch: {epoch} - \" # + \" - \".join(f\"{key}: {value:.3f}\" for key, value in epoch_logs.items()) # ) if self . stop_training : break callbacks . on_train_end () return self . history","title":"fit()"},{"location":"api/model/Model/#elegy.model.Model.load","text":"Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the haiku.Params + haiku.State structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Source code in elegy/model.py 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 def load ( self , path : tp . Union [ str , Path ]) -> None : \"\"\" Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the `haiku.Params` + `haiku.State` structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Arguments: path: path to a saved model's directory. \"\"\" if isinstance ( path , str ): path = Path ( path ) states : tp . Dict = deepdish . io . load ( path / \"parameters.h5\" ) optimizer_state_path = path / \"optimizer_state.pkl\" if optimizer_state_path . exists (): with open ( optimizer_state_path , \"rb\" ) as f : states [ \"optimizer_state\" ] = pickle . load ( f ) self . full_state = states","title":"load()"},{"location":"api/model/Model/#elegy.model.Model.predict","text":"Generates output predictions for the input samples. Computation is done in batches. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, numpy.ndarray], Tuple[numpy.ndarray], Iterable] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . required batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generators (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for [ Model.fit ][elegy.model.Model.fit]. Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description ndarray Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model.py 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 def predict ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , np . ndarray ], tp . Tuple [ np . ndarray ], tp . Iterable , ], verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ) -> np . ndarray : \"\"\"Generates output predictions for the input samples. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generators (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.Model.fit]. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs , ) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs","title":"predict()"},{"location":"api/model/Model/#elegy.model.Model.predict_on_batch","text":"Returns predictions for a single batch of samples. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. required Returns: Type Description Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Jax array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between given number of inputs and expectations of the model. Source code in elegy/model.py 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 def predict_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ] ) -> tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ]: \"\"\" Returns predictions for a single batch of samples. Arguments: x: Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. Returns: Jax array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. \"\"\" self . _maybe_initialize ( mode = Mode . predict , x = x , y = None , sample_weight = None , class_weight = None ) assert self . parameters is not None assert self . states is not None y_pred , context = self . _predict ( False , # training False , # get_summaries x = x , parameters = self . parameters , states = self . states , rng = next ( self . _rngs ), ) return y_pred","title":"predict_on_batch()"},{"location":"api/model/Model/#elegy.model.Model.save","text":"Saves the model to disk. It creates a directory that includes: The Model object instance serialized with pickle as as {path}/model.pkl , this allows you to re-instantiate the model later. The model parameters + states serialized into HDF5 as {path}/parameters.h5 . The states of the optimizer serialized with pickle as as {path}/optimizer_state.pkl , allowing to resume training exactly where you left off. We hope to use HDF5 in the future but optax states is incompatible with deepdish . This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via Model.load if the model is already instiated or elegy.model.load to load the model instance from its pickled version. import elegy model . save ( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load ( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path where model structure will be saved. required include_optimizer bool If True, save optimizer's states together. True Source code in elegy/model.py 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 def save ( self , path : tp . Union [ str , Path ], include_optimizer : bool = True ) -> None : \"\"\" Saves the model to disk. It creates a directory that includes: - The `Model` object instance serialized with `pickle` as as `{path}/model.pkl`, this allows you to re-instantiate the model later. - The model parameters + states serialized into HDF5 as `{path}/parameters.h5`. - The states of the optimizer serialized with `pickle` as as `{path}/optimizer_state.pkl`, allowing to resume training exactly where you left off. We hope to use HDF5 in the future but `optax` states is incompatible with `deepdish`. This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via `Model.load` if the model is already instiated or `elegy.model.load` to load the model instance from its pickled version. ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path where model structure will be saved. include_optimizer: If True, save optimizer's states together. \"\"\" if isinstance ( path , str ): path = Path ( path ) path . mkdir ( parents = True , exist_ok = True ) states = self . full_state original_state = copy ( states ) states . pop ( \"metrics_states\" , None ) states . pop ( \"initial_metrics_state\" , None ) optimizer_state = states . pop ( \"optimizer_state\" , None ) deepdish . io . save ( path / \"parameters.h5\" , states ) if include_optimizer and optimizer_state is not None : with open ( path / \"optimizer_state.pkl\" , \"wb\" ) as f : pickle . dump ( optimizer_state , f ) # getting pickle errors self . reset () try : path = path / \"model.pkl\" with open ( path , \"wb\" ) as f : cloudpickle . dump ( self , f ) except BaseException as e : print ( f \"Error occurred saving the model object at { path } \\n Continuing....\" ) self . full_state = original_state","title":"save()"},{"location":"api/model/Model/#elegy.model.Model.summary","text":"Prints a summary of the network. Parameters: Name Type Description Default x A sample of inputs to the network. required depth int The level number of nested level which will be showed. Information about summaries from modules deeper than depth will be aggregated together. 2 tablefmt str A string represeting the style of the table generated by tabulate . See python-tabulate for more options. 'fancy_grid' tablulate_kwargs Additional keyword arguments passed to tabulate . See python-tabulate for more options. {} Source code in elegy/model.py 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 def summary ( self , x , depth : int = 2 , tablefmt : str = \"fancy_grid\" , ** tablulate_kwargs ): \"\"\" Prints a summary of the network. Arguments: x: A sample of inputs to the network. depth: The level number of nested level which will be showed. Information about summaries from modules deeper than `depth` will be aggregated together. tablefmt: A string represeting the style of the table generated by `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. tablulate_kwargs: Additional keyword arguments passed to `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. \"\"\" self . _maybe_initialize ( mode = Mode . predict , x = x , y = None , sample_weight = None , class_weight = None , ) assert self . parameters is not None assert self . states is not None y_pred , context = self . _predict ( training = False , get_summaries = True , x = x , parameters = self . parameters , states = self . states , rng = next ( self . _rngs ), ) def format_output ( outputs ) -> str : file = StringIO () outputs = jax . tree_map ( lambda x : f \" { x . shape } {{ pad }} { x . dtype } \" , outputs ) yaml . safe_dump ( outputs , file , default_flow_style = False , indent = 2 , explicit_end = False ) return file . getvalue () . replace ( \" \\n ...\" , \"\" ) def format_size ( size ): return ( f \" { size / 1e9 : ,.1f } GB\" if size > 1e9 else f \" { size / 1e6 : ,.1f } MB\" if size > 1e6 else f \" { size / 1e3 : ,.1f } KB\" if size > 1e3 else f \" { size : , } B\" ) table : tp . List = [[ \"Inputs\" , format_output ( x ), \"0\" , \"0\" ]] for module , base_name , value in context . summaries : base_name_parts = base_name . split ( \"/\" )[ 1 :] module_depth = len ( base_name_parts ) if module_depth > depth : continue include_submodules = module_depth == depth params_count = ( module . parameters_size ( include_submodules ) if module is not None else 0 ) params_size = ( module . parameters_bytes ( include_submodules ) if module is not None else 0 ) states_count = ( module . states_size ( include_submodules ) if module is not None else 0 ) states_size = ( module . states_bytes ( include_submodules ) if module is not None else 0 ) class_name = f \"( { module . __class__ . __name__ } )\" if module is not None else \"\" base_name = \"/\" . join ( base_name_parts ) if not base_name : base_name = \"Outputs\" table . append ( [ f \" { base_name } {{ pad }} { class_name } \" , format_output ( value ), f \" { params_count : , } {{ pad }} { format_size ( params_size ) } \" if params_count > 0 else \"0\" , f \" { states_count : , } {{ pad }} { format_size ( states_size ) } \" if states_count > 0 else \"0\" , ] ) # add papdding for col in range ( 4 ): max_length = max ( len ( line . split ( \" {pad} \" )[ 0 ]) for row in table for line in row [ col ] . split ( \" \\n \" ) ) for row in table : row [ col ] = \" \\n \" . join ( line . format ( pad = \" \" * ( max_length - len ( line . rstrip () . split ( \" {pad} \" )[ 0 ])) ) for line in row [ col ] . rstrip () . split ( \" \\n \" ) ) print ( \" \\n \" + tabulate ( table , headers = [ \"Layer\" , \"Outputs Shape\" , \"Trainable \\n Parameters\" , \"Non-trainable \\n Parameters\" , ], tablefmt = tablefmt , ** tablulate_kwargs , ) ) params_count = self . module . parameters_size () params_size = self . module . parameters_bytes () states_count = self . module . states_size () states_size = self . module . states_bytes () total_count = params_count + states_count total_size = params_size + states_size print ( tabulate ( [ [ f \"Total Parameters:\" , f \" { total_count : , } \" , f \" { format_size ( total_size ) } \" if total_count > 0 else \"\" , ], [ f \"Trainable Parameters:\" , f \" { params_count : , } \" , f \" { format_size ( params_size ) } \" if params_count > 0 else \"\" , ], [ f \"Non-trainable Parameters:\" , f \" { states_count : , } \" , f \" { format_size ( states_size ) } \" if states_count > 0 else \"\" , ], ], tablefmt = \"plain\" , ) + \" \\n \" )","title":"summary()"},{"location":"api/model/Model/#elegy.model.Model.test_on_batch","text":"Test the model on a single batch of samples. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[jax.numpy.lax_numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). None sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None Returns: Type Description Dict[str, jax.numpy.lax_numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model.py 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 def test_on_batch ( self , x : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ jnp . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ jnp . ndarray ] = None , class_weight : tp . Optional [ jnp . ndarray ] = None , ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . _maybe_initialize ( mode = Mode . test , x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , ) assert self . parameters is not None assert self . states is not None ( logs , metrics_states ) = self . _test ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , parameters = self . parameters , states = self . states , metrics_states = self . metrics_states , rng = next ( self . _rngs ), ) if metrics_states is not None : self . metrics_states = metrics_states # logs = jax.tree_map(np.asarray, logs) return logs","title":"test_on_batch()"},{"location":"api/model/Model/#elegy.model.Model.train_on_batch","text":"Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x Union[numpy.ndarray, Mapping[str, Any], Tuple] Input data. It could be: A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Optional[Union[numpy.ndarray, Mapping[str, Any], Tuple]] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). It should be consistent with x (you cannot have Numpy inputs and array targets, or inversely). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight Optional[numpy.ndarray] Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None Returns: Type Description Dict[str, numpy.ndarray] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model.py 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def train_on_batch ( self , x : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple ], y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ np . ndarray ] = None , ) -> tp . Dict [ str , np . ndarray ]: \"\"\" Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). It should be consistent with `x` (you cannot have Numpy inputs and array targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . _maybe_initialize ( mode = Mode . train , x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , ) assert self . parameters is not None assert self . states is not None assert self . optimizer_state is not None ( logs , self . parameters , self . states , self . optimizer_state , metrics_states , ) = self . _update ( x = x , y = y , sample_weight = sample_weight , class_weight = class_weight , parameters = self . parameters , states = self . states , optimizer_state = self . optimizer_state , metrics_states = self . metrics_states , rng = next ( self . _rngs ), ) if metrics_states is not None : self . metrics_states = metrics_states # logs = jax.tree_map(np.asarray, logs) return logs","title":"train_on_batch()"},{"location":"api/model/load/","text":"elegy.model.load Loads a model from disk. This function will restore both the model architecture, that is, its Model class instance, along with all of its parameters, states, and optimizer states. Examples: import elegy model . save ( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load ( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Exceptions: Type Description OSError in case the model was not found or could not be loaded from disk successfully. Source code in elegy/model.py 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 def load ( path : tp . Union [ str , Path ]) -> Model : \"\"\" Loads a model from disk. This function will restore both the model architecture, that is, its `Model` class instance, along with all of its parameters, states, and optimizer states. Example: ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path to a saved model's directory. Raises: OSError: in case the model was not found or could not be loaded from disk successfully. \"\"\" if isinstance ( path , str ): path = Path ( path ) with open ( path / \"model.pkl\" , \"rb\" ) as f : try : model : Model = pickle . load ( f ) except BaseException as e : raise OSError ( f \"Could not load the model. Got exception: { e } \" ) model . load ( path ) return model","title":"load"},{"location":"api/model/load/#elegymodelload","text":"","title":"elegy.model.load"},{"location":"api/model/load/#elegy.model.load","text":"Loads a model from disk. This function will restore both the model architecture, that is, its Model class instance, along with all of its parameters, states, and optimizer states. Examples: import elegy model . save ( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load ( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Exceptions: Type Description OSError in case the model was not found or could not be loaded from disk successfully. Source code in elegy/model.py 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 def load ( path : tp . Union [ str , Path ]) -> Model : \"\"\" Loads a model from disk. This function will restore both the model architecture, that is, its `Model` class instance, along with all of its parameters, states, and optimizer states. Example: ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path to a saved model's directory. Raises: OSError: in case the model was not found or could not be loaded from disk successfully. \"\"\" if isinstance ( path , str ): path = Path ( path ) with open ( path / \"model.pkl\" , \"rb\" ) as f : try : model : Model = pickle . load ( f ) except BaseException as e : raise OSError ( f \"Could not load the model. Got exception: { e } \" ) model . load ( path ) return model","title":"elegy.model.load"},{"location":"api/module/Module/","text":"elegy.module.Module Basic Elegy Module. Its a thin wrapper around hk.Module that add custom functionalities related to Elegy. submodules: Dict [ str , Any ] property readonly A dictionary with all submodules contained in this Module. __init__ ( self , name = None , dtype =< class ' jax . numpy . lax_numpy . float32 '>) special Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Parameters: Name Type Description Default name Optional[str] An optional string name for the class. Must be a valid elsePython identifier. If name is not provided then the class name for the current instance is converted to lower_snake_case and used instead. None Source code in elegy/module.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : np . dtype = jnp . float32 ): \"\"\" Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Arguments: name: An optional string name for the class. Must be a valid elsePython identifier. If ``name`` is not provided then the class name for the current instance is converted to ``lower_snake_case`` and used instead. \"\"\" self . name = name if name else utils . lower_snake_case ( self . __class__ . __name__ ) self . dtype = dtype self . _params = set () self . _states = set () self . _submodules = set () self . _dynamic_submodules = [] self . _ignore = set () apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/module.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn get_parameters ( self ) Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/module.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/module.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/module.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/module.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/module.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/module.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Module"},{"location":"api/module/Module/#elegymodulemodule","text":"","title":"elegy.module.Module"},{"location":"api/module/Module/#elegy.module.Module","text":"Basic Elegy Module. Its a thin wrapper around hk.Module that add custom functionalities related to Elegy.","title":"elegy.module.Module"},{"location":"api/module/Module/#elegy.module.Module.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/module/Module/#elegy.module.Module.__init__","text":"Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Parameters: Name Type Description Default name Optional[str] An optional string name for the class. Must be a valid elsePython identifier. If name is not provided then the class name for the current instance is converted to lower_snake_case and used instead. None Source code in elegy/module.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : np . dtype = jnp . float32 ): \"\"\" Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Arguments: name: An optional string name for the class. Must be a valid elsePython identifier. If ``name`` is not provided then the class name for the current instance is converted to ``lower_snake_case`` and used instead. \"\"\" self . name = name if name else utils . lower_snake_case ( self . __class__ . __name__ ) self . dtype = dtype self . _params = set () self . _states = set () self . _submodules = set () self . _dynamic_submodules = [] self . _ignore = set ()","title":"__init__()"},{"location":"api/module/Module/#elegy.module.Module.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/module.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/module/Module/#elegy.module.Module.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/module.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/module/Module/#elegy.module.Module.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/module.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/module/Module/#elegy.module.Module.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/module.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/module/Module/#elegy.module.Module.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/module.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/module/Module/#elegy.module.Module.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/module.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/module/Module/#elegy.module.Module.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/module.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/module/to_module/","text":"elegy.module.to_module Source code in elegy/module.py 721 722 723 724 725 726 727 728 729 730 731 732 733 734 def to_module ( f ): class MyModule ( Module ): def __init__ ( self , name : tp . Optional [ str ] = None ): super () . __init__ ( name = utils . lower_snake_case ( f . __name__ ) if name is None else name ) self . call = f def call ( self , * args , ** kwargs ): ... MyModule . __name__ = f . __name__ return MyModule","title":"to_module"},{"location":"api/module/to_module/#elegymoduleto_module","text":"","title":"elegy.module.to_module"},{"location":"api/module/to_module/#elegy.module.to_module","text":"Source code in elegy/module.py 721 722 723 724 725 726 727 728 729 730 731 732 733 734 def to_module ( f ): class MyModule ( Module ): def __init__ ( self , name : tp . Optional [ str ] = None ): super () . __init__ ( name = utils . lower_snake_case ( f . __name__ ) if name is None else name ) self . call = f def call ( self , * args , ** kwargs ): ... MyModule . __name__ = f . __name__ return MyModule","title":"elegy.module.to_module"},{"location":"api/nn/BatchNormalization/","text":"elegy.nn.BatchNormalization Normalizes inputs to maintain a mean of ~0 and stddev of ~1. See: https://arxiv.org/abs/1502.03167. There are many different variations for how users want to manage scale and offset if they require them at all. These are: No scale/offset in which case create_* should be set to False and scale / offset aren't passed when the module is called. Trainable scale/offset in which case create_* should be set to True and again scale / offset aren't passed when the module is called. In this case this module creates and owns the scale / offset variables. Externally generated scale / offset , such as for conditional normalization, in which case create_* should be set to False and then the values fed in at call time. NOTE: jax.vmap(hk.transform(BatchNorm)) will update summary statistics and normalize values on a per-batch basis; we currently do not support normalizing across a batch axis introduced by vmap. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , create_scale = True , create_offset = True , decay_rate = 0.99 , eps = 1e-05 , scale_init = None , offset_init = None , axis = None , cross_replica_axis = None , data_format = 'channels_last' , ** kwargs ) special Constructs a BatchNorm module. Parameters: Name Type Description Default create_scale bool Whether to include a trainable scaling factor. True create_offset bool Whether to include a trainable offset. True decay_rate float Decay rate for EMA. 0.99 eps float Small epsilon to avoid division by zero variance. Defaults 1e-5 , as in the paper and Sonnet. 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for gain (aka scale). Can only be set if create_scale=True . By default, 1 . None offset_init Optional[elegy.types.Initializer] Optional initializer for bias (aka offset). Can only be set if create_offset=True . By default, 0 . None axis Optional[Sequence[int]] Which axes to reduce over. The default ( None ) signifies that all but the channel axis should be normalized. Otherwise this is a list of axis indices which will have normalization statistics calculated. None cross_replica_axis Optional[str] If not None , it should be a string representing the axis name over which this module is being run within a jax.pmap . Supplying this argument means that batch statistics are calculated across all replicas on that axis. None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default it is channels_last . 'channels_last' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/batch_normalization.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , create_scale : bool = True , create_offset : bool = True , decay_rate : float = 0.99 , eps : float = 1e-5 , scale_init : Optional [ initializers . Initializer ] = None , offset_init : Optional [ initializers . Initializer ] = None , axis : Optional [ Sequence [ int ]] = None , cross_replica_axis : Optional [ str ] = None , data_format : str = \"channels_last\" , ** kwargs ): \"\"\"Constructs a BatchNorm module. Args: create_scale: Whether to include a trainable scaling factor. create_offset: Whether to include a trainable offset. decay_rate: Decay rate for EMA. eps: Small epsilon to avoid division by zero variance. Defaults ``1e-5``, as in the paper and Sonnet. scale_init: Optional initializer for gain (aka scale). Can only be set if ``create_scale=True``. By default, ``1``. offset_init: Optional initializer for bias (aka offset). Can only be set if ``create_offset=True``. By default, ``0``. axis: Which axes to reduce over. The default (``None``) signifies that all but the channel axis should be normalized. Otherwise this is a list of axis indices which will have normalization statistics calculated. cross_replica_axis: If not ``None``, it should be a string representing the axis name over which this module is being run within a ``jax.pmap``. Supplying this argument means that batch statistics are calculated across all replicas on that axis. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default it is ``channels_last``. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if not create_scale and scale_init is not None : raise ValueError ( \"Cannot set `scale_init` if `create_scale=False`\" ) if not create_offset and offset_init is not None : raise ValueError ( \"Cannot set `offset_init` if `create_offset=False`\" ) self . create_scale = create_scale self . create_offset = create_offset self . eps = eps self . scale_init = scale_init or jnp . ones self . offset_init = offset_init or jnp . zeros self . axis = axis self . cross_replica_axis = cross_replica_axis self . channel_index = haiku_utils . get_channel_index ( data_format ) self . mean_ema = ExponentialMovingAverage ( decay_rate , name = \"mean_ema\" ) self . var_ema = ExponentialMovingAverage ( decay_rate , name = \"var_ema\" ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/batch_normalization.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , inputs , training = None , test_local_stats = False , scale = None , offset = None ) Computes the normalized version of the input. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [..., C] . required training Optional[bool] Whether training is currently happening. None test_local_stats bool Whether local stats are used when training=False. False scale Optional[jax.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized across all but the last dimension. Source code in elegy/nn/batch_normalization.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def call ( self , inputs : jnp . ndarray , training : tp . Optional [ bool ] = None , test_local_stats : bool = False , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Computes the normalized version of the input. Args: inputs: An array, where the data format is ``[..., C]``. training: Whether training is currently happening. test_local_stats: Whether local stats are used when training=False. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized across all but the last dimension. \"\"\" if training is None : training = hooks . is_training () if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) channel_index = self . channel_index if channel_index < 0 : channel_index += inputs . ndim if self . axis is not None : axis = self . axis else : axis = [ i for i in range ( inputs . ndim ) if i != channel_index ] if training or test_local_stats : cross_replica_axis = self . cross_replica_axis if self . cross_replica_axis : mean = jnp . mean ( inputs , axis , keepdims = True ) mean = jax . lax . pmean ( mean , cross_replica_axis ) mean_of_squares = jnp . mean ( inputs ** 2 , axis , keepdims = True ) mean_of_squares = jax . lax . pmean ( mean_of_squares , cross_replica_axis ) var = mean_of_squares - mean ** 2 else : mean = jnp . mean ( inputs , axis , keepdims = True ) # This uses E[(X - E[X])^2]. # TODO(tycai): Consider the faster, but possibly less stable # E[X^2] - E[X]^2 method. var = jnp . var ( inputs , axis , keepdims = True ) else : mean = self . mean_ema . average var = self . var_ema . average if training : self . mean_ema ( mean ) self . var_ema ( var ) w_shape = [ 1 if i in axis else inputs . shape [ i ] for i in range ( inputs . ndim )] w_dtype = inputs . dtype if self . create_scale : scale = hooks . get_parameter ( \"scale\" , w_shape , w_dtype , self . scale_init ) elif scale is None : scale = np . ones ([], dtype = w_dtype ) if self . create_offset : offset = hooks . get_parameter ( \"offset\" , w_shape , w_dtype , self . offset_init ) elif offset is None : offset = np . zeros ([], dtype = w_dtype ) inv = scale * jax . lax . rsqrt ( var + self . eps ) return ( inputs - mean ) * inv + offset get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/batch_normalization.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/batch_normalization.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/batch_normalization.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/batch_normalization.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/batch_normalization.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/batch_normalization.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"BatchNormalization"},{"location":"api/nn/BatchNormalization/#elegynnbatchnormalization","text":"","title":"elegy.nn.BatchNormalization"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization","text":"Normalizes inputs to maintain a mean of ~0 and stddev of ~1. See: https://arxiv.org/abs/1502.03167. There are many different variations for how users want to manage scale and offset if they require them at all. These are: No scale/offset in which case create_* should be set to False and scale / offset aren't passed when the module is called. Trainable scale/offset in which case create_* should be set to True and again scale / offset aren't passed when the module is called. In this case this module creates and owns the scale / offset variables. Externally generated scale / offset , such as for conditional normalization, in which case create_* should be set to False and then the values fed in at call time. NOTE: jax.vmap(hk.transform(BatchNorm)) will update summary statistics and normalize values on a per-batch basis; we currently do not support normalizing across a batch axis introduced by vmap.","title":"elegy.nn.batch_normalization.BatchNormalization"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.__init__","text":"Constructs a BatchNorm module. Parameters: Name Type Description Default create_scale bool Whether to include a trainable scaling factor. True create_offset bool Whether to include a trainable offset. True decay_rate float Decay rate for EMA. 0.99 eps float Small epsilon to avoid division by zero variance. Defaults 1e-5 , as in the paper and Sonnet. 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for gain (aka scale). Can only be set if create_scale=True . By default, 1 . None offset_init Optional[elegy.types.Initializer] Optional initializer for bias (aka offset). Can only be set if create_offset=True . By default, 0 . None axis Optional[Sequence[int]] Which axes to reduce over. The default ( None ) signifies that all but the channel axis should be normalized. Otherwise this is a list of axis indices which will have normalization statistics calculated. None cross_replica_axis Optional[str] If not None , it should be a string representing the axis name over which this module is being run within a jax.pmap . Supplying this argument means that batch statistics are calculated across all replicas on that axis. None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default it is channels_last . 'channels_last' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/batch_normalization.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , create_scale : bool = True , create_offset : bool = True , decay_rate : float = 0.99 , eps : float = 1e-5 , scale_init : Optional [ initializers . Initializer ] = None , offset_init : Optional [ initializers . Initializer ] = None , axis : Optional [ Sequence [ int ]] = None , cross_replica_axis : Optional [ str ] = None , data_format : str = \"channels_last\" , ** kwargs ): \"\"\"Constructs a BatchNorm module. Args: create_scale: Whether to include a trainable scaling factor. create_offset: Whether to include a trainable offset. decay_rate: Decay rate for EMA. eps: Small epsilon to avoid division by zero variance. Defaults ``1e-5``, as in the paper and Sonnet. scale_init: Optional initializer for gain (aka scale). Can only be set if ``create_scale=True``. By default, ``1``. offset_init: Optional initializer for bias (aka offset). Can only be set if ``create_offset=True``. By default, ``0``. axis: Which axes to reduce over. The default (``None``) signifies that all but the channel axis should be normalized. Otherwise this is a list of axis indices which will have normalization statistics calculated. cross_replica_axis: If not ``None``, it should be a string representing the axis name over which this module is being run within a ``jax.pmap``. Supplying this argument means that batch statistics are calculated across all replicas on that axis. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default it is ``channels_last``. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if not create_scale and scale_init is not None : raise ValueError ( \"Cannot set `scale_init` if `create_scale=False`\" ) if not create_offset and offset_init is not None : raise ValueError ( \"Cannot set `offset_init` if `create_offset=False`\" ) self . create_scale = create_scale self . create_offset = create_offset self . eps = eps self . scale_init = scale_init or jnp . ones self . offset_init = offset_init or jnp . zeros self . axis = axis self . cross_replica_axis = cross_replica_axis self . channel_index = haiku_utils . get_channel_index ( data_format ) self . mean_ema = ExponentialMovingAverage ( decay_rate , name = \"mean_ema\" ) self . var_ema = ExponentialMovingAverage ( decay_rate , name = \"var_ema\" )","title":"__init__()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/batch_normalization.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.call","text":"Computes the normalized version of the input. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [..., C] . required training Optional[bool] Whether training is currently happening. None test_local_stats bool Whether local stats are used when training=False. False scale Optional[jax.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized across all but the last dimension. Source code in elegy/nn/batch_normalization.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def call ( self , inputs : jnp . ndarray , training : tp . Optional [ bool ] = None , test_local_stats : bool = False , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Computes the normalized version of the input. Args: inputs: An array, where the data format is ``[..., C]``. training: Whether training is currently happening. test_local_stats: Whether local stats are used when training=False. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized across all but the last dimension. \"\"\" if training is None : training = hooks . is_training () if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) channel_index = self . channel_index if channel_index < 0 : channel_index += inputs . ndim if self . axis is not None : axis = self . axis else : axis = [ i for i in range ( inputs . ndim ) if i != channel_index ] if training or test_local_stats : cross_replica_axis = self . cross_replica_axis if self . cross_replica_axis : mean = jnp . mean ( inputs , axis , keepdims = True ) mean = jax . lax . pmean ( mean , cross_replica_axis ) mean_of_squares = jnp . mean ( inputs ** 2 , axis , keepdims = True ) mean_of_squares = jax . lax . pmean ( mean_of_squares , cross_replica_axis ) var = mean_of_squares - mean ** 2 else : mean = jnp . mean ( inputs , axis , keepdims = True ) # This uses E[(X - E[X])^2]. # TODO(tycai): Consider the faster, but possibly less stable # E[X^2] - E[X]^2 method. var = jnp . var ( inputs , axis , keepdims = True ) else : mean = self . mean_ema . average var = self . var_ema . average if training : self . mean_ema ( mean ) self . var_ema ( var ) w_shape = [ 1 if i in axis else inputs . shape [ i ] for i in range ( inputs . ndim )] w_dtype = inputs . dtype if self . create_scale : scale = hooks . get_parameter ( \"scale\" , w_shape , w_dtype , self . scale_init ) elif scale is None : scale = np . ones ([], dtype = w_dtype ) if self . create_offset : offset = hooks . get_parameter ( \"offset\" , w_shape , w_dtype , self . offset_init ) elif offset is None : offset = np . zeros ([], dtype = w_dtype ) inv = scale * jax . lax . rsqrt ( var + self . eps ) return ( inputs - mean ) * inv + offset","title":"call()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/batch_normalization.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/batch_normalization.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/batch_normalization.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/batch_normalization.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/batch_normalization.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/batch_normalization.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/Conv1D/","text":"elegy.nn.Conv1D One dimensional convolution. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , output_channels , kernel_shape , stride = 1 , rate = 1 , padding = 'SAME' , with_bias = True , w_init = None , b_init = None , data_format = 'NWC' , mask = None , ** kwargs ) special Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 1. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 1. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 1. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 1. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NWC or NCW . By default, NWC . 'NWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NWC\" , mask : tp . Optional [ np . ndarray ] = None , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 1. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 1. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 1. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 1. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NWC`` or ``NCW``. By default, ``NWC``. mask: tp.Optional mask of the weights. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 1 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , ** kwargs , ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/conv.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , inputs ) inherited Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ], self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = hooks . get_parameter ( \"w\" , w_shape , inputs . dtype , initializer = w_init ) if self . mask is not None : w *= self . mask out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = hooks . get_parameter ( \"b\" , bias_shape , inputs . dtype , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) out = out + b return out get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/conv.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Conv1D"},{"location":"api/nn/Conv1D/#elegynnconv1d","text":"","title":"elegy.nn.Conv1D"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D","text":"One dimensional convolution.","title":"elegy.nn.conv.Conv1D"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.__init__","text":"Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 1. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 1. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 1. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 1. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NWC or NCW . By default, NWC . 'NWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NWC\" , mask : tp . Optional [ np . ndarray ] = None , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 1. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 1. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 1. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 1. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NWC`` or ``NCW``. By default, ``NWC``. mask: tp.Optional mask of the weights. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 1 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , ** kwargs , )","title":"__init__()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/conv.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.call","text":"Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ], self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = hooks . get_parameter ( \"w\" , w_shape , inputs . dtype , initializer = w_init ) if self . mask is not None : w *= self . mask out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = hooks . get_parameter ( \"b\" , bias_shape , inputs . dtype , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) out = out + b return out","title":"call()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/conv.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/Conv2D/","text":"elegy.nn.Conv2D Two dimensional convolution. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , output_channels , kernel_shape , stride = 1 , rate = 1 , padding = 'SAME' , with_bias = True , w_init = None , b_init = None , data_format = 'NHWC' , mask = None , ** kwargs ) special Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 2. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 2. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 2. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 2. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NHWC or NCHW . By default, NHWC . 'NHWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NHWC\" , mask : tp . Optional [ np . ndarray ] = None , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 2. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 2. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 2. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 2. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NHWC`` or ``NCHW``. By default, ``NHWC``. mask: tp.Optional mask of the weights. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 2 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , ** kwargs , ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/conv.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , inputs ) inherited Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ], self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = hooks . get_parameter ( \"w\" , w_shape , inputs . dtype , initializer = w_init ) if self . mask is not None : w *= self . mask out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = hooks . get_parameter ( \"b\" , bias_shape , inputs . dtype , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) out = out + b return out get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/conv.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Conv2D"},{"location":"api/nn/Conv2D/#elegynnconv2d","text":"","title":"elegy.nn.Conv2D"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D","text":"Two dimensional convolution.","title":"elegy.nn.conv.Conv2D"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.__init__","text":"Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 2. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 2. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 2. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 2. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NHWC or NCHW . By default, NHWC . 'NHWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NHWC\" , mask : tp . Optional [ np . ndarray ] = None , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 2. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 2. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 2. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 2. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NHWC`` or ``NCHW``. By default, ``NHWC``. mask: tp.Optional mask of the weights. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 2 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , ** kwargs , )","title":"__init__()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/conv.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.call","text":"Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ], self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = hooks . get_parameter ( \"w\" , w_shape , inputs . dtype , initializer = w_init ) if self . mask is not None : w *= self . mask out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = hooks . get_parameter ( \"b\" , bias_shape , inputs . dtype , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) out = out + b return out","title":"call()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/conv.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/Conv3D/","text":"elegy.nn.Conv3D Three dimensional convolution. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , output_channels , kernel_shape , stride = 1 , rate = 1 , padding = 'SAME' , with_bias = True , w_init = None , b_init = None , data_format = 'NDHWC' , mask = None , ** kwargs ) special Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 3. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 3. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 3. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 3. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NDHWC or NCDHW . By default, NDHWC . 'NDHWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NDHWC\" , mask : tp . Optional [ np . ndarray ] = None , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 3. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 3. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 3. 1 corresponds to standard ND convolution, `rate > 1` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 3. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NDHWC`` or ``NCDHW``. By default, ``NDHWC``. mask: tp.Optional mask of the weights. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 3 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , ** kwargs , ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/conv.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , inputs ) inherited Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ], self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = hooks . get_parameter ( \"w\" , w_shape , inputs . dtype , initializer = w_init ) if self . mask is not None : w *= self . mask out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = hooks . get_parameter ( \"b\" , bias_shape , inputs . dtype , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) out = out + b return out get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/conv.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Conv3D"},{"location":"api/nn/Conv3D/#elegynnconv3d","text":"","title":"elegy.nn.Conv3D"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D","text":"Three dimensional convolution.","title":"elegy.nn.conv.Conv3D"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.__init__","text":"Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 3. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 3. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 3. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 3. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NDHWC or NCDHW . By default, NDHWC . 'NDHWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NDHWC\" , mask : tp . Optional [ np . ndarray ] = None , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 3. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 3. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 3. 1 corresponds to standard ND convolution, `rate > 1` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 3. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NDHWC`` or ``NCDHW``. By default, ``NDHWC``. mask: tp.Optional mask of the weights. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 3 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , ** kwargs , )","title":"__init__()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/conv.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.call","text":"Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ], self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = hooks . get_parameter ( \"w\" , w_shape , inputs . dtype , initializer = w_init ) if self . mask is not None : w *= self . mask out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = hooks . get_parameter ( \"b\" , bias_shape , inputs . dtype , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) out = out + b return out","title":"call()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/conv.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/ConvND/","text":"elegy.nn.ConvND General N-dimensional convolutional. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , num_spatial_dims , output_channels , kernel_shape , stride = 1 , rate = 1 , padding = 'SAME' , with_bias = True , w_init = None , b_init = None , data_format = 'channels_last' , mask = None , ** kwargs ) special Initializes the module. Parameters: Name Type Description Default num_spatial_dims int The number of spatial dimensions of the input. required output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length num_spatial_dims . required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length num_spatial_dims . Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length num_spatial_dims . 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a sequence of n (low, high) integer pairs that give the padding to apply before and after each spatial dimension. or a callable or sequence of callables of size num_spatial_dims . Any callables must take a single integer argument equal to the effective kernel size and return a sequence of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default, channels_last . 'channels_last' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __init__ ( self , num_spatial_dims : int , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"channels_last\" , mask : tp . Optional [ np . ndarray ] = None , ** kwargs , ): \"\"\" Initializes the module. Args: num_spatial_dims: The number of spatial dimensions of the input. output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length ``num_spatial_dims``. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length ``num_spatial_dims``. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length ``num_spatial_dims``. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a sequence of n ``(low, high)`` integer pairs that give the padding to apply before and after each spatial dimension. or a callable or sequence of callables of size ``num_spatial_dims``. Any callables must take a single integer argument equal to the effective kernel size and return a sequence of two integers representing the padding before and after. See ``haiku.pad.*`` for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default, ``channels_last``. mask: tp.Optional mask of the weights. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if num_spatial_dims <= 0 : raise ValueError ( \"We only support convolution operations for `num_spatial_dims` \" f \"greater than 0, received num_spatial_dims= { num_spatial_dims } .\" ) self . num_spatial_dims = num_spatial_dims self . output_channels = output_channels self . kernel_shape = hk_utils . replicate ( kernel_shape , num_spatial_dims , \"kernel_shape\" ) self . with_bias = with_bias self . stride = hk_utils . replicate ( stride , num_spatial_dims , \"strides\" ) self . w_init = w_init self . b_init = b_init or jnp . zeros self . mask = mask self . lhs_dilation = hk_utils . replicate ( 1 , num_spatial_dims , \"lhs_dilation\" ) self . kernel_dilation = hk_utils . replicate ( rate , num_spatial_dims , \"kernel_dilation\" ) self . data_format = data_format self . channel_index = hk_utils . get_channel_index ( data_format ) self . dimension_numbers = to_dimension_numbers ( num_spatial_dims , channels_last = ( self . channel_index == - 1 ), transpose = False ) if isinstance ( padding , str ): self . padding = padding . upper () else : self . padding = hk . pad . create ( padding = padding , kernel = self . kernel_shape , rate = self . kernel_dilation , n = self . num_spatial_dims , ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/conv.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , inputs ) Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ], self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = hooks . get_parameter ( \"w\" , w_shape , inputs . dtype , initializer = w_init ) if self . mask is not None : w *= self . mask out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = hooks . get_parameter ( \"b\" , bias_shape , inputs . dtype , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) out = out + b return out get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/conv.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"ConvND"},{"location":"api/nn/ConvND/#elegynnconvnd","text":"","title":"elegy.nn.ConvND"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND","text":"General N-dimensional convolutional.","title":"elegy.nn.conv.ConvND"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.__init__","text":"Initializes the module. Parameters: Name Type Description Default num_spatial_dims int The number of spatial dimensions of the input. required output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length num_spatial_dims . required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length num_spatial_dims . Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length num_spatial_dims . 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a sequence of n (low, high) integer pairs that give the padding to apply before and after each spatial dimension. or a callable or sequence of callables of size num_spatial_dims . Any callables must take a single integer argument equal to the effective kernel size and return a sequence of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default, channels_last . 'channels_last' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __init__ ( self , num_spatial_dims : int , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"channels_last\" , mask : tp . Optional [ np . ndarray ] = None , ** kwargs , ): \"\"\" Initializes the module. Args: num_spatial_dims: The number of spatial dimensions of the input. output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length ``num_spatial_dims``. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length ``num_spatial_dims``. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length ``num_spatial_dims``. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a sequence of n ``(low, high)`` integer pairs that give the padding to apply before and after each spatial dimension. or a callable or sequence of callables of size ``num_spatial_dims``. Any callables must take a single integer argument equal to the effective kernel size and return a sequence of two integers representing the padding before and after. See ``haiku.pad.*`` for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default, ``channels_last``. mask: tp.Optional mask of the weights. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if num_spatial_dims <= 0 : raise ValueError ( \"We only support convolution operations for `num_spatial_dims` \" f \"greater than 0, received num_spatial_dims= { num_spatial_dims } .\" ) self . num_spatial_dims = num_spatial_dims self . output_channels = output_channels self . kernel_shape = hk_utils . replicate ( kernel_shape , num_spatial_dims , \"kernel_shape\" ) self . with_bias = with_bias self . stride = hk_utils . replicate ( stride , num_spatial_dims , \"strides\" ) self . w_init = w_init self . b_init = b_init or jnp . zeros self . mask = mask self . lhs_dilation = hk_utils . replicate ( 1 , num_spatial_dims , \"lhs_dilation\" ) self . kernel_dilation = hk_utils . replicate ( rate , num_spatial_dims , \"kernel_dilation\" ) self . data_format = data_format self . channel_index = hk_utils . get_channel_index ( data_format ) self . dimension_numbers = to_dimension_numbers ( num_spatial_dims , channels_last = ( self . channel_index == - 1 ), transpose = False ) if isinstance ( padding , str ): self . padding = padding . upper () else : self . padding = hk . pad . create ( padding = padding , kernel = self . kernel_shape , rate = self . kernel_dilation , n = self . num_spatial_dims , )","title":"__init__()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/conv.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.call","text":"Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ], self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = hooks . get_parameter ( \"w\" , w_shape , inputs . dtype , initializer = w_init ) if self . mask is not None : w *= self . mask out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = hooks . get_parameter ( \"b\" , bias_shape , inputs . dtype , initializer = self . b_init ) b = jnp . broadcast_to ( b , out . shape ) out = out + b return out","title":"call()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/conv.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/conv.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/conv.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/conv.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/Dropout/","text":"elegy.nn.Dropout Applies Dropout to the input. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged. Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit , training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer. Example dropout = elegy . nn . Dropout ( 0.2 ) data = np . arange ( 10 ) . reshape ( 5 , 2 ) . astype ( np . float32 ) print ( data ) # [[0. 1.] # [2. 3.] # [4. 5.] # [6. 7.] # [8. 9.]] outputs = dropout ( data , training = True ) print ( outputs ) # [[ 0. 1.25] # [ 2.5 3.75] # [ 5. 6.25] # [ 7.5 0. ] # [10. 0. ]] submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/dropout.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , x , training = None , rng = None ) Parameters: Name Type Description Default x ndarray The value to be dropped out. required training Optional[bool] Whether training is currently happening. None rng Optional[numpy.ndarray] Optional RNGKey. None Returns: Type Description ndarray x but dropped out and scaled by 1 / (1 - rate) . Source code in elegy/nn/dropout.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def call ( self , x : np . ndarray , training : tp . Optional [ bool ] = None , rng : tp . Optional [ np . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Arguments: x: The value to be dropped out. training: Whether training is currently happening. rng: Optional RNGKey. Returns: x but dropped out and scaled by `1 / (1 - rate)`. \"\"\" if training is None : training = hooks . is_training () return hk . dropout ( rng = rng if rng is not None else hooks . next_rng_key (), rate = self . rate if training else 0.0 , x = x , ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/dropout.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/dropout.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/dropout.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/dropout.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/dropout.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/dropout.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Dropout"},{"location":"api/nn/Dropout/#elegynndropout","text":"","title":"elegy.nn.Dropout"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout","text":"Applies Dropout to the input. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged. Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit , training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer.","title":"elegy.nn.dropout.Dropout"},{"location":"api/nn/Dropout/#example","text":"dropout = elegy . nn . Dropout ( 0.2 ) data = np . arange ( 10 ) . reshape ( 5 , 2 ) . astype ( np . float32 ) print ( data ) # [[0. 1.] # [2. 3.] # [4. 5.] # [6. 7.] # [8. 9.]] outputs = dropout ( data , training = True ) print ( outputs ) # [[ 0. 1.25] # [ 2.5 3.75] # [ 5. 6.25] # [ 7.5 0. ] # [10. 0. ]]","title":"Example"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/dropout.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.call","text":"Parameters: Name Type Description Default x ndarray The value to be dropped out. required training Optional[bool] Whether training is currently happening. None rng Optional[numpy.ndarray] Optional RNGKey. None Returns: Type Description ndarray x but dropped out and scaled by 1 / (1 - rate) . Source code in elegy/nn/dropout.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def call ( self , x : np . ndarray , training : tp . Optional [ bool ] = None , rng : tp . Optional [ np . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Arguments: x: The value to be dropped out. training: Whether training is currently happening. rng: Optional RNGKey. Returns: x but dropped out and scaled by `1 / (1 - rate)`. \"\"\" if training is None : training = hooks . is_training () return hk . dropout ( rng = rng if rng is not None else hooks . next_rng_key (), rate = self . rate if training else 0.0 , x = x , )","title":"call()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/dropout.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/dropout.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/dropout.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/dropout.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/dropout.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/dropout.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/EmbedLookupStyle/","text":"elegy.nn.EmbedLookupStyle How to return the embedding matrices given IDs.","title":"EmbedLookupStyle"},{"location":"api/nn/EmbedLookupStyle/#elegynnembedlookupstyle","text":"","title":"elegy.nn.EmbedLookupStyle"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle","text":"How to return the embedding matrices given IDs.","title":"elegy.nn.embedding.EmbedLookupStyle"},{"location":"api/nn/Embedding/","text":"elegy.nn.Embedding Module for embedding tokens in a low-dimensional space. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , vocab_size = None , embed_dim = None , embedding_matrix = None , w_init = None , lookup_style = 'ARRAY_INDEX' , name = None ) special Constructs an Embed module. The number of unique tokens to embed. If not provided, an existing vocabulary matrix from which vocab_size can be inferred must be provided as existing_vocab . Number of dimensions to assign to each embedding. If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. A matrix-like object equivalent in size to [vocab_size, embed_dim] . If given, it is used as the initial value for the embedding matrix and neither vocab_size or embed_dim need be given. If they are given, their values are checked to be consistent with the dimensions of embedding_matrix . An initializer for the embeddings matrix. As a default, embeddings are initialized via a truncated normal distribution. One of the enum values of :class: EmbedLookupStyle determining how to access the value of the embbeddings given an ID. Regardless the input should be a dense array of integer values representing ids. This setting changes how internally this module maps those ides to embeddings. The result is the same, but the speed and memory tradeoffs are different. It default to using numpy-style array indexing. This value is only the default for the module, and at any given invocation can be overriden in :meth: __call__ . name: Optional name for this module. If none of embed_dim , embedding_matrix and vocab_size are supplied, or if embedding_matrix is supplied and embed_dim or vocab_size is not consistent with the supplied matrix. Source code in elegy/nn/embedding.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self , vocab_size : Optional [ int ] = None , embed_dim : Optional [ int ] = None , embedding_matrix : Optional [ jnp . ndarray ] = None , w_init : Optional [ initializers . Initializer ] = None , lookup_style : Union [ str , EmbedLookupStyle ] = \"ARRAY_INDEX\" , name : Optional [ str ] = None , ): \"\"\" Constructs an Embed module. Args: vocab_size: The number of unique tokens to embed. If not provided, an existing vocabulary matrix from which ``vocab_size`` can be inferred must be provided as ``existing_vocab``. embed_dim: Number of dimensions to assign to each embedding. If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. embedding_matrix: A matrix-like object equivalent in size to ``[vocab_size, embed_dim]``. If given, it is used as the initial value for the embedding matrix and neither ``vocab_size`` or ``embed_dim`` need be given. If they are given, their values are checked to be consistent with the dimensions of ``embedding_matrix``. w_init: An initializer for the embeddings matrix. As a default, embeddings are initialized via a truncated normal distribution. lookup_style: One of the enum values of :class:`EmbedLookupStyle` determining how to access the value of the embbeddings given an ID. Regardless the input should be a dense array of integer values representing ids. This setting changes how internally this module maps those ides to embeddings. The result is the same, but the speed and memory tradeoffs are different. It default to using numpy-style array indexing. This value is only the default for the module, and at any given invocation can be overriden in :meth:`__call__`. name: Optional name for this module. Raises: ValueError: If none of ``embed_dim``, ``embedding_matrix`` and ``vocab_size`` are supplied, or if ``embedding_matrix`` is supplied and ``embed_dim`` or ``vocab_size`` is not consistent with the supplied matrix. \"\"\" super () . __init__ ( name = name ) if embedding_matrix is None and not ( vocab_size and embed_dim ): raise ValueError ( \"Embedding must be supplied either with an initial `embedding_matrix` \" \"or with `embed_dim` and `vocab_size`.\" ) if embedding_matrix is not None : embedding_matrix = jnp . asarray ( embedding_matrix ) if vocab_size and embedding_matrix . shape [ 0 ] != vocab_size : raise ValueError ( \"An `embedding_matrix` was supplied but the `vocab_size` of \" f \" { vocab_size } was not consistent with its shape \" f \" { embedding_matrix . shape } .\" ) if embed_dim and embedding_matrix . shape [ 1 ] != embed_dim : raise ValueError ( \"An `embedding_matrix` was supplied but the `embed_dim` of \" f \" { embed_dim } was not consistent with its shape \" f \" { embedding_matrix . shape } .\" ) self . embeddings = hooks . get_parameter ( \"embeddings\" , embedding_matrix . shape , initializer = lambda _ , __ : embedding_matrix , ) else : assert embed_dim is not None assert vocab_size is not None w_init = w_init or initializers . TruncatedNormal () self . embeddings = hooks . get_parameter ( \"embeddings\" , [ vocab_size , embed_dim ], initializer = w_init ) self . vocab_size = vocab_size or embedding_matrix . shape [ 0 ] self . embed_dim = embed_dim or embedding_matrix . shape [ 1 ] self . lookup_style = lookup_style apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/embedding.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , ids , lookup_style = None ) Lookup embeddings. Looks up an embedding vector for each value in ids . All ids must be within [0, vocab_size) to prevent NaN \\ s from propagating. Parameters: Name Type Description Default ids ndarray integer array. required lookup_style Optional[Union[str, elegy.nn.embedding.EmbedLookupStyle]] Overrides the lookup_style given in the constructor. None Returns: Type Description ndarray Tensor of ids.shape + [embedding_dim] . Exceptions: Type Description AttributeError If lookup_style is not valid. ValueError If ids is not an integer array. Source code in elegy/nn/embedding.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def call ( self , ids : jnp . ndarray , lookup_style : Optional [ Union [ str , EmbedLookupStyle ]] = None , ) -> jnp . ndarray : r \"\"\" Lookup embeddings. Looks up an embedding vector for each value in ``ids``. All ids must be within ``[0, vocab_size)`` to prevent ``NaN``\\ s from propagating. Args: ids: integer array. lookup_style: Overrides the ``lookup_style`` given in the constructor. Returns: Tensor of ``ids.shape + [embedding_dim]``. Raises: AttributeError: If ``lookup_style`` is not valid. ValueError: If ``ids`` is not an integer array. \"\"\" # TODO(tomhennigan) Consider removing asarray here. ids = jnp . asarray ( ids ) if not jnp . issubdtype ( ids . dtype , jnp . integer ): raise ValueError ( \"Embedding's __call__ method must take an array of \" \"integer dtype but was called with an array of \" f \" { ids . dtype } \" ) lookup_style = lookup_style or self . lookup_style if isinstance ( lookup_style , str ): lookup_style = getattr ( EmbedLookupStyle , lookup_style . upper ()) if lookup_style == EmbedLookupStyle . ARRAY_INDEX : return self . embeddings [( ids ,)] elif lookup_style == EmbedLookupStyle . ONE_HOT : one_hot_ids = jax . nn . one_hot ( ids , self . vocab_size )[ ... , None ] return ( self . embeddings * one_hot_ids ) . sum ( axis =- 2 ) else : raise NotImplementedError ( f \" { lookup_style } is not supported by Embedding.\" ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/embedding.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/embedding.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/embedding.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/embedding.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/embedding.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/embedding.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Embedding"},{"location":"api/nn/Embedding/#elegynnembedding","text":"","title":"elegy.nn.Embedding"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding","text":"Module for embedding tokens in a low-dimensional space.","title":"elegy.nn.embedding.Embedding"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.__init__","text":"Constructs an Embed module. The number of unique tokens to embed. If not provided, an existing vocabulary matrix from which vocab_size can be inferred must be provided as existing_vocab . Number of dimensions to assign to each embedding. If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. A matrix-like object equivalent in size to [vocab_size, embed_dim] . If given, it is used as the initial value for the embedding matrix and neither vocab_size or embed_dim need be given. If they are given, their values are checked to be consistent with the dimensions of embedding_matrix . An initializer for the embeddings matrix. As a default, embeddings are initialized via a truncated normal distribution. One of the enum values of :class: EmbedLookupStyle determining how to access the value of the embbeddings given an ID. Regardless the input should be a dense array of integer values representing ids. This setting changes how internally this module maps those ides to embeddings. The result is the same, but the speed and memory tradeoffs are different. It default to using numpy-style array indexing. This value is only the default for the module, and at any given invocation can be overriden in :meth: __call__ . name: Optional name for this module. If none of embed_dim , embedding_matrix and vocab_size are supplied, or if embedding_matrix is supplied and embed_dim or vocab_size is not consistent with the supplied matrix. Source code in elegy/nn/embedding.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self , vocab_size : Optional [ int ] = None , embed_dim : Optional [ int ] = None , embedding_matrix : Optional [ jnp . ndarray ] = None , w_init : Optional [ initializers . Initializer ] = None , lookup_style : Union [ str , EmbedLookupStyle ] = \"ARRAY_INDEX\" , name : Optional [ str ] = None , ): \"\"\" Constructs an Embed module. Args: vocab_size: The number of unique tokens to embed. If not provided, an existing vocabulary matrix from which ``vocab_size`` can be inferred must be provided as ``existing_vocab``. embed_dim: Number of dimensions to assign to each embedding. If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. embedding_matrix: A matrix-like object equivalent in size to ``[vocab_size, embed_dim]``. If given, it is used as the initial value for the embedding matrix and neither ``vocab_size`` or ``embed_dim`` need be given. If they are given, their values are checked to be consistent with the dimensions of ``embedding_matrix``. w_init: An initializer for the embeddings matrix. As a default, embeddings are initialized via a truncated normal distribution. lookup_style: One of the enum values of :class:`EmbedLookupStyle` determining how to access the value of the embbeddings given an ID. Regardless the input should be a dense array of integer values representing ids. This setting changes how internally this module maps those ides to embeddings. The result is the same, but the speed and memory tradeoffs are different. It default to using numpy-style array indexing. This value is only the default for the module, and at any given invocation can be overriden in :meth:`__call__`. name: Optional name for this module. Raises: ValueError: If none of ``embed_dim``, ``embedding_matrix`` and ``vocab_size`` are supplied, or if ``embedding_matrix`` is supplied and ``embed_dim`` or ``vocab_size`` is not consistent with the supplied matrix. \"\"\" super () . __init__ ( name = name ) if embedding_matrix is None and not ( vocab_size and embed_dim ): raise ValueError ( \"Embedding must be supplied either with an initial `embedding_matrix` \" \"or with `embed_dim` and `vocab_size`.\" ) if embedding_matrix is not None : embedding_matrix = jnp . asarray ( embedding_matrix ) if vocab_size and embedding_matrix . shape [ 0 ] != vocab_size : raise ValueError ( \"An `embedding_matrix` was supplied but the `vocab_size` of \" f \" { vocab_size } was not consistent with its shape \" f \" { embedding_matrix . shape } .\" ) if embed_dim and embedding_matrix . shape [ 1 ] != embed_dim : raise ValueError ( \"An `embedding_matrix` was supplied but the `embed_dim` of \" f \" { embed_dim } was not consistent with its shape \" f \" { embedding_matrix . shape } .\" ) self . embeddings = hooks . get_parameter ( \"embeddings\" , embedding_matrix . shape , initializer = lambda _ , __ : embedding_matrix , ) else : assert embed_dim is not None assert vocab_size is not None w_init = w_init or initializers . TruncatedNormal () self . embeddings = hooks . get_parameter ( \"embeddings\" , [ vocab_size , embed_dim ], initializer = w_init ) self . vocab_size = vocab_size or embedding_matrix . shape [ 0 ] self . embed_dim = embed_dim or embedding_matrix . shape [ 1 ] self . lookup_style = lookup_style","title":"__init__()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/embedding.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.call","text":"Lookup embeddings. Looks up an embedding vector for each value in ids . All ids must be within [0, vocab_size) to prevent NaN \\ s from propagating. Parameters: Name Type Description Default ids ndarray integer array. required lookup_style Optional[Union[str, elegy.nn.embedding.EmbedLookupStyle]] Overrides the lookup_style given in the constructor. None Returns: Type Description ndarray Tensor of ids.shape + [embedding_dim] . Exceptions: Type Description AttributeError If lookup_style is not valid. ValueError If ids is not an integer array. Source code in elegy/nn/embedding.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def call ( self , ids : jnp . ndarray , lookup_style : Optional [ Union [ str , EmbedLookupStyle ]] = None , ) -> jnp . ndarray : r \"\"\" Lookup embeddings. Looks up an embedding vector for each value in ``ids``. All ids must be within ``[0, vocab_size)`` to prevent ``NaN``\\ s from propagating. Args: ids: integer array. lookup_style: Overrides the ``lookup_style`` given in the constructor. Returns: Tensor of ``ids.shape + [embedding_dim]``. Raises: AttributeError: If ``lookup_style`` is not valid. ValueError: If ``ids`` is not an integer array. \"\"\" # TODO(tomhennigan) Consider removing asarray here. ids = jnp . asarray ( ids ) if not jnp . issubdtype ( ids . dtype , jnp . integer ): raise ValueError ( \"Embedding's __call__ method must take an array of \" \"integer dtype but was called with an array of \" f \" { ids . dtype } \" ) lookup_style = lookup_style or self . lookup_style if isinstance ( lookup_style , str ): lookup_style = getattr ( EmbedLookupStyle , lookup_style . upper ()) if lookup_style == EmbedLookupStyle . ARRAY_INDEX : return self . embeddings [( ids ,)] elif lookup_style == EmbedLookupStyle . ONE_HOT : one_hot_ids = jax . nn . one_hot ( ids , self . vocab_size )[ ... , None ] return ( self . embeddings * one_hot_ids ) . sum ( axis =- 2 ) else : raise NotImplementedError ( f \" { lookup_style } is not supported by Embedding.\" )","title":"call()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/embedding.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/embedding.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/embedding.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/embedding.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/embedding.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/embedding.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/Flatten/","text":"elegy.nn.Flatten Flattens the input, preserving the batch dimension(s). By default, Flatten combines all dimensions except the first. Additional leading dimensions can be preserved by setting preserve_dims. x = jnp . ones ([ 3 , 2 , 4 ]) flat = elegy . nn . Flatten () assert flat ( x ) . shape == ( 3 , 8 ) When the input to flatten has fewer than preserve_dims dimensions it is returned unchanged: x = jnp . ones ([ 3 ]) assert flat ( x ) . shape == ( 3 ,) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/flatten.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , inputs ) inherited Parameters: Name Type Description Default inputs ndarray the array to be reshaped. required Returns: Type Description ndarray A reshaped array. Source code in elegy/nn/flatten.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Arguments: inputs: the array to be reshaped. Returns: A reshaped array. \"\"\" if inputs . ndim <= self . preserve_dims : return inputs if - 1 in self . output_shape : reshaped_shape = _infer_shape ( self . output_shape , inputs . shape [ self . preserve_dims :] ) else : reshaped_shape = self . output_shape shape = inputs . shape [: self . preserve_dims ] + reshaped_shape return jnp . reshape ( inputs , shape ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/flatten.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/flatten.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/flatten.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/flatten.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/flatten.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/flatten.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Flatten"},{"location":"api/nn/Flatten/#elegynnflatten","text":"","title":"elegy.nn.Flatten"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten","text":"Flattens the input, preserving the batch dimension(s). By default, Flatten combines all dimensions except the first. Additional leading dimensions can be preserved by setting preserve_dims. x = jnp . ones ([ 3 , 2 , 4 ]) flat = elegy . nn . Flatten () assert flat ( x ) . shape == ( 3 , 8 ) When the input to flatten has fewer than preserve_dims dimensions it is returned unchanged: x = jnp . ones ([ 3 ]) assert flat ( x ) . shape == ( 3 ,)","title":"elegy.nn.flatten.Flatten"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/flatten.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.call","text":"Parameters: Name Type Description Default inputs ndarray the array to be reshaped. required Returns: Type Description ndarray A reshaped array. Source code in elegy/nn/flatten.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Arguments: inputs: the array to be reshaped. Returns: A reshaped array. \"\"\" if inputs . ndim <= self . preserve_dims : return inputs if - 1 in self . output_shape : reshaped_shape = _infer_shape ( self . output_shape , inputs . shape [ self . preserve_dims :] ) else : reshaped_shape = self . output_shape shape = inputs . shape [: self . preserve_dims ] + reshaped_shape return jnp . reshape ( inputs , shape )","title":"call()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/flatten.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/flatten.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/flatten.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/flatten.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/flatten.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/flatten.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/InstanceNormalization/","text":"elegy.nn.InstanceNormalization Normalizes inputs along the spatial dimensions. See LayerNorm for more details. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , create_scale = True , create_offset = True , eps = 1e-05 , scale_init = None , offset_init = None , data_format = 'channels_last' , ** kwargs ) special Constructs an InstanceNormalization module. This method creates a module which normalizes over the spatial dimensions. Parameters: Name Type Description Default create_scale bool bool representing whether to create a trainable scale per channel applied after the normalization. True create_offset bool bool representing whether to create a trainable offset per channel applied after normalization and scaling. True eps float Small epsilon to avoid division by zero variance. Defaults to 1e-5 . 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for the scale variable. Can only be set if create_scale=True . By default scale is initialized to 1 . None offset_init Optional[elegy.types.Initializer] Optional initializer for the offset variable. Can only be set if create_offset=True . By default offset is initialized to 0 . None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default it is channels_last . 'channels_last' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/layer_normalization.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , create_scale : bool = True , create_offset : bool = True , eps : float = 1e-5 , scale_init : Optional [ initializers . Initializer ] = None , offset_init : Optional [ initializers . Initializer ] = None , data_format : str = \"channels_last\" , ** kwargs ): \"\"\"Constructs an `InstanceNormalization` module. This method creates a module which normalizes over the spatial dimensions. Args: create_scale: ``bool`` representing whether to create a trainable scale per channel applied after the normalization. create_offset: ``bool`` representing whether to create a trainable offset per channel applied after normalization and scaling. eps: Small epsilon to avoid division by zero variance. Defaults to ``1e-5``. scale_init: Optional initializer for the scale variable. Can only be set if ``create_scale=True``. By default scale is initialized to ``1``. offset_init: Optional initializer for the offset variable. Can only be set if ``create_offset=True``. By default offset is initialized to ``0``. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default it is ``channels_last``. kwargs: Additional keyword arguments passed to Module. \"\"\" if hk_utils . get_channel_index ( data_format ) == 1 : axis = slice ( 2 , None ) else : # channel_index = -1 axis = slice ( 1 , - 1 ) super () . __init__ ( axis = axis , create_scale = create_scale , create_offset = create_offset , eps = eps , scale_init = scale_init , offset_init = offset_init , ** kwargs ) apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/layer_normalization.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , inputs , scale = None , offset = None ) inherited Connects the layer norm. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [N, ..., C] . required scale Optional[jax.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized. Source code in elegy/nn/layer_normalization.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def call ( self , inputs : jnp . ndarray , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Connects the layer norm. Args: inputs: An array, where the data format is ``[N, ..., C]``. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized. \"\"\" if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) axis = self . axis if isinstance ( axis , slice ): axis = tuple ( range ( inputs . ndim )[ axis ]) mean = jnp . mean ( inputs , axis = axis , keepdims = True ) variance = jnp . var ( inputs , axis = axis , keepdims = True ) param_shape = inputs . shape [ - 1 :] if self . create_scale : scale = hooks . get_parameter ( \"scale\" , param_shape , jnp . float32 , initializer = self . scale_init ) elif scale is None : scale = np . array ( 1.0 , dtype = inputs . dtype ) if self . create_offset : offset = hooks . get_parameter ( \"offset\" , param_shape , jnp . float32 , initializer = self . offset_init ) elif offset is None : offset = np . array ( 0.0 , dtype = inputs . dtype ) scale = jnp . broadcast_to ( scale , inputs . shape ) offset = jnp . broadcast_to ( offset , inputs . shape ) mean = jnp . broadcast_to ( mean , inputs . shape ) inv = scale * jax . lax . rsqrt ( variance + self . eps ) return inv * ( inputs - mean ) + offset get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/layer_normalization.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/layer_normalization.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/layer_normalization.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/layer_normalization.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/layer_normalization.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/layer_normalization.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"InstanceNormalization"},{"location":"api/nn/InstanceNormalization/#elegynninstancenormalization","text":"","title":"elegy.nn.InstanceNormalization"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization","text":"Normalizes inputs along the spatial dimensions. See LayerNorm for more details.","title":"elegy.nn.layer_normalization.InstanceNormalization"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.__init__","text":"Constructs an InstanceNormalization module. This method creates a module which normalizes over the spatial dimensions. Parameters: Name Type Description Default create_scale bool bool representing whether to create a trainable scale per channel applied after the normalization. True create_offset bool bool representing whether to create a trainable offset per channel applied after normalization and scaling. True eps float Small epsilon to avoid division by zero variance. Defaults to 1e-5 . 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for the scale variable. Can only be set if create_scale=True . By default scale is initialized to 1 . None offset_init Optional[elegy.types.Initializer] Optional initializer for the offset variable. Can only be set if create_offset=True . By default offset is initialized to 0 . None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default it is channels_last . 'channels_last' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/layer_normalization.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , create_scale : bool = True , create_offset : bool = True , eps : float = 1e-5 , scale_init : Optional [ initializers . Initializer ] = None , offset_init : Optional [ initializers . Initializer ] = None , data_format : str = \"channels_last\" , ** kwargs ): \"\"\"Constructs an `InstanceNormalization` module. This method creates a module which normalizes over the spatial dimensions. Args: create_scale: ``bool`` representing whether to create a trainable scale per channel applied after the normalization. create_offset: ``bool`` representing whether to create a trainable offset per channel applied after normalization and scaling. eps: Small epsilon to avoid division by zero variance. Defaults to ``1e-5``. scale_init: Optional initializer for the scale variable. Can only be set if ``create_scale=True``. By default scale is initialized to ``1``. offset_init: Optional initializer for the offset variable. Can only be set if ``create_offset=True``. By default offset is initialized to ``0``. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default it is ``channels_last``. kwargs: Additional keyword arguments passed to Module. \"\"\" if hk_utils . get_channel_index ( data_format ) == 1 : axis = slice ( 2 , None ) else : # channel_index = -1 axis = slice ( 1 , - 1 ) super () . __init__ ( axis = axis , create_scale = create_scale , create_offset = create_offset , eps = eps , scale_init = scale_init , offset_init = offset_init , ** kwargs )","title":"__init__()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/layer_normalization.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.call","text":"Connects the layer norm. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [N, ..., C] . required scale Optional[jax.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized. Source code in elegy/nn/layer_normalization.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def call ( self , inputs : jnp . ndarray , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Connects the layer norm. Args: inputs: An array, where the data format is ``[N, ..., C]``. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized. \"\"\" if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) axis = self . axis if isinstance ( axis , slice ): axis = tuple ( range ( inputs . ndim )[ axis ]) mean = jnp . mean ( inputs , axis = axis , keepdims = True ) variance = jnp . var ( inputs , axis = axis , keepdims = True ) param_shape = inputs . shape [ - 1 :] if self . create_scale : scale = hooks . get_parameter ( \"scale\" , param_shape , jnp . float32 , initializer = self . scale_init ) elif scale is None : scale = np . array ( 1.0 , dtype = inputs . dtype ) if self . create_offset : offset = hooks . get_parameter ( \"offset\" , param_shape , jnp . float32 , initializer = self . offset_init ) elif offset is None : offset = np . array ( 0.0 , dtype = inputs . dtype ) scale = jnp . broadcast_to ( scale , inputs . shape ) offset = jnp . broadcast_to ( offset , inputs . shape ) mean = jnp . broadcast_to ( mean , inputs . shape ) inv = scale * jax . lax . rsqrt ( variance + self . eps ) return inv * ( inputs - mean ) + offset","title":"call()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/layer_normalization.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/layer_normalization.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/layer_normalization.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/layer_normalization.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/layer_normalization.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/layer_normalization.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/LayerNormalization/","text":"elegy.nn.LayerNormalization LayerNorm module. See: https://arxiv.org/abs/1607.06450. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , axis =- 1 , create_scale = True , create_offset = True , eps = 1e-05 , scale_init = None , offset_init = None , ** kwargs ) special Constructs a LayerNorm module. Parameters: Name Type Description Default axis Union[int, Sequence[int], slice] Integer, list of integers, or slice indicating which axes to normalize over. -1 create_scale bool Bool, defines whether to create a trainable scale per channel applied after the normalization. True create_offset bool Bool, defines whether to create a trainable offset per channel applied after normalization and scaling. True eps float Small epsilon to avoid division by zero variance. Defaults 1e-5 , as in the paper and Sonnet. 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for gain (aka scale). By default, one. None offset_init Optional[elegy.types.Initializer] Optional initializer for bias (aka offset). By default, zero. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/layer_normalization.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , axis : Union [ int , Sequence [ int ], slice ] = - 1 , create_scale : bool = True , create_offset : bool = True , eps : float = 1e-5 , scale_init : Optional [ initializers . Initializer ] = None , offset_init : Optional [ initializers . Initializer ] = None , ** kwargs ): \"\"\"Constructs a LayerNorm module. Args: axis: Integer, list of integers, or slice indicating which axes to normalize over. create_scale: Bool, defines whether to create a trainable scale per channel applied after the normalization. create_offset: Bool, defines whether to create a trainable offset per channel applied after normalization and scaling. eps: Small epsilon to avoid division by zero variance. Defaults ``1e-5``, as in the paper and Sonnet. scale_init: Optional initializer for gain (aka scale). By default, one. offset_init: Optional initializer for bias (aka offset). By default, zero. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if not create_scale and scale_init is not None : raise ValueError ( \"Cannot set `scale_init` if `create_scale=False`.\" ) if not create_offset and offset_init is not None : raise ValueError ( \"Cannot set `offset_init` if `create_offset=False`.\" ) if isinstance ( axis , slice ): self . axis = axis elif isinstance ( axis , int ): self . axis = ( axis ,) elif isinstance ( axis , collections . Iterable ) and all ( isinstance ( ax , int ) for ax in axis ): self . axis = tuple ( axis ) else : raise ValueError ( \"`axis` should be an int, slice or iterable of ints.\" ) self . eps = eps self . create_scale = create_scale self . create_offset = create_offset self . scale_init = scale_init or jnp . ones self . offset_init = offset_init or jnp . zeros apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/layer_normalization.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , inputs , scale = None , offset = None ) Connects the layer norm. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [N, ..., C] . required scale Optional[jax.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized. Source code in elegy/nn/layer_normalization.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def call ( self , inputs : jnp . ndarray , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Connects the layer norm. Args: inputs: An array, where the data format is ``[N, ..., C]``. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized. \"\"\" if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) axis = self . axis if isinstance ( axis , slice ): axis = tuple ( range ( inputs . ndim )[ axis ]) mean = jnp . mean ( inputs , axis = axis , keepdims = True ) variance = jnp . var ( inputs , axis = axis , keepdims = True ) param_shape = inputs . shape [ - 1 :] if self . create_scale : scale = hooks . get_parameter ( \"scale\" , param_shape , jnp . float32 , initializer = self . scale_init ) elif scale is None : scale = np . array ( 1.0 , dtype = inputs . dtype ) if self . create_offset : offset = hooks . get_parameter ( \"offset\" , param_shape , jnp . float32 , initializer = self . offset_init ) elif offset is None : offset = np . array ( 0.0 , dtype = inputs . dtype ) scale = jnp . broadcast_to ( scale , inputs . shape ) offset = jnp . broadcast_to ( offset , inputs . shape ) mean = jnp . broadcast_to ( mean , inputs . shape ) inv = scale * jax . lax . rsqrt ( variance + self . eps ) return inv * ( inputs - mean ) + offset get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/layer_normalization.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/layer_normalization.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/layer_normalization.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/layer_normalization.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/layer_normalization.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/layer_normalization.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"LayerNormalization"},{"location":"api/nn/LayerNormalization/#elegynnlayernormalization","text":"","title":"elegy.nn.LayerNormalization"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization","text":"LayerNorm module. See: https://arxiv.org/abs/1607.06450.","title":"elegy.nn.layer_normalization.LayerNormalization"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.__init__","text":"Constructs a LayerNorm module. Parameters: Name Type Description Default axis Union[int, Sequence[int], slice] Integer, list of integers, or slice indicating which axes to normalize over. -1 create_scale bool Bool, defines whether to create a trainable scale per channel applied after the normalization. True create_offset bool Bool, defines whether to create a trainable offset per channel applied after normalization and scaling. True eps float Small epsilon to avoid division by zero variance. Defaults 1e-5 , as in the paper and Sonnet. 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for gain (aka scale). By default, one. None offset_init Optional[elegy.types.Initializer] Optional initializer for bias (aka offset). By default, zero. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/layer_normalization.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , axis : Union [ int , Sequence [ int ], slice ] = - 1 , create_scale : bool = True , create_offset : bool = True , eps : float = 1e-5 , scale_init : Optional [ initializers . Initializer ] = None , offset_init : Optional [ initializers . Initializer ] = None , ** kwargs ): \"\"\"Constructs a LayerNorm module. Args: axis: Integer, list of integers, or slice indicating which axes to normalize over. create_scale: Bool, defines whether to create a trainable scale per channel applied after the normalization. create_offset: Bool, defines whether to create a trainable offset per channel applied after normalization and scaling. eps: Small epsilon to avoid division by zero variance. Defaults ``1e-5``, as in the paper and Sonnet. scale_init: Optional initializer for gain (aka scale). By default, one. offset_init: Optional initializer for bias (aka offset). By default, zero. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if not create_scale and scale_init is not None : raise ValueError ( \"Cannot set `scale_init` if `create_scale=False`.\" ) if not create_offset and offset_init is not None : raise ValueError ( \"Cannot set `offset_init` if `create_offset=False`.\" ) if isinstance ( axis , slice ): self . axis = axis elif isinstance ( axis , int ): self . axis = ( axis ,) elif isinstance ( axis , collections . Iterable ) and all ( isinstance ( ax , int ) for ax in axis ): self . axis = tuple ( axis ) else : raise ValueError ( \"`axis` should be an int, slice or iterable of ints.\" ) self . eps = eps self . create_scale = create_scale self . create_offset = create_offset self . scale_init = scale_init or jnp . ones self . offset_init = offset_init or jnp . zeros","title":"__init__()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/layer_normalization.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.call","text":"Connects the layer norm. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [N, ..., C] . required scale Optional[jax.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized. Source code in elegy/nn/layer_normalization.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def call ( self , inputs : jnp . ndarray , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Connects the layer norm. Args: inputs: An array, where the data format is ``[N, ..., C]``. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized. \"\"\" if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) axis = self . axis if isinstance ( axis , slice ): axis = tuple ( range ( inputs . ndim )[ axis ]) mean = jnp . mean ( inputs , axis = axis , keepdims = True ) variance = jnp . var ( inputs , axis = axis , keepdims = True ) param_shape = inputs . shape [ - 1 :] if self . create_scale : scale = hooks . get_parameter ( \"scale\" , param_shape , jnp . float32 , initializer = self . scale_init ) elif scale is None : scale = np . array ( 1.0 , dtype = inputs . dtype ) if self . create_offset : offset = hooks . get_parameter ( \"offset\" , param_shape , jnp . float32 , initializer = self . offset_init ) elif offset is None : offset = np . array ( 0.0 , dtype = inputs . dtype ) scale = jnp . broadcast_to ( scale , inputs . shape ) offset = jnp . broadcast_to ( offset , inputs . shape ) mean = jnp . broadcast_to ( mean , inputs . shape ) inv = scale * jax . lax . rsqrt ( variance + self . eps ) return inv * ( inputs - mean ) + offset","title":"call()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/layer_normalization.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/layer_normalization.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/layer_normalization.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/layer_normalization.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/layer_normalization.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/layer_normalization.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/Linear/","text":"elegy.nn.Linear Linear module. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , output_size , with_bias = True , w_init = None , b_init = None , ** kwargs ) special Constructs the Linear module. Parameters: Name Type Description Default output_size int Output dimensionality. required with_bias bool Whether to add a bias to the output. True w_init Optional[elegy.types.Initializer] Optional initializer for weights. By default, uses random values from truncated normal, with stddev 1 / sqrt(fan_in) . See https://arxiv.org/abs/1502.03167v3. None b_init Optional[elegy.types.Initializer] Optional initializer for bias. By default, zero. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/linear.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , output_size : int , with_bias : bool = True , w_init : tp . Optional [ Initializer ] = None , b_init : tp . Optional [ Initializer ] = None , ** kwargs ): \"\"\" Constructs the Linear module. Arguments: output_size: Output dimensionality. with_bias: Whether to add a bias to the output. w_init: Optional initializer for weights. By default, uses random values from truncated normal, with stddev `1 / sqrt(fan_in)`. See https://arxiv.org/abs/1502.03167v3. b_init: Optional initializer for bias. By default, zero. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) self . input_size = None self . output_size = output_size self . with_bias = with_bias self . w_init = w_init self . b_init = b_init or jnp . zeros apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/linear.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/linear.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/linear.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/linear.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/linear.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/linear.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/linear.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Linear"},{"location":"api/nn/Linear/#elegynnlinear","text":"","title":"elegy.nn.Linear"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear","text":"Linear module.","title":"elegy.nn.linear.Linear"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.__init__","text":"Constructs the Linear module. Parameters: Name Type Description Default output_size int Output dimensionality. required with_bias bool Whether to add a bias to the output. True w_init Optional[elegy.types.Initializer] Optional initializer for weights. By default, uses random values from truncated normal, with stddev 1 / sqrt(fan_in) . See https://arxiv.org/abs/1502.03167v3. None b_init Optional[elegy.types.Initializer] Optional initializer for bias. By default, zero. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/linear.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , output_size : int , with_bias : bool = True , w_init : tp . Optional [ Initializer ] = None , b_init : tp . Optional [ Initializer ] = None , ** kwargs ): \"\"\" Constructs the Linear module. Arguments: output_size: Output dimensionality. with_bias: Whether to add a bias to the output. w_init: Optional initializer for weights. By default, uses random values from truncated normal, with stddev `1 / sqrt(fan_in)`. See https://arxiv.org/abs/1502.03167v3. b_init: Optional initializer for bias. By default, zero. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) self . input_size = None self . output_size = output_size self . with_bias = with_bias self . w_init = w_init self . b_init = b_init or jnp . zeros","title":"__init__()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/linear.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/linear.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/linear.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/linear.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/linear.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/linear.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/linear.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/Reshape/","text":"elegy.nn.Reshape Reshapes input Tensor, preserving the batch dimension. For example, given an input tensor with shape [B, H, W, C, D] : B , H , W , C , D = range ( 1 , 6 ) x = jnp . ones ([ B , H , W , C , D ]) The default behavior when output_shape is (-1, D) is to flatten all dimensions between B and D : mod = elegy . nn . Reshape ( output_shape = ( - 1 , D )) assert mod ( x ) . shape == ( B , H * W * C , D ) You can change the number of preserved leading dimensions via preserve_dims : mod = elegy . nn . Reshape ( output_shape = ( - 1 , D ), preserve_dims = 2 ) assert mod ( x ) . shape == ( B , H , W * C , D ) mod = elegy . nn . Reshape ( output_shape = ( - 1 , D ), preserve_dims = 3 ) assert mod ( x ) . shape == ( B , H , W , C , D ) mod = elegy . nn . Reshape ( output_shape = ( - 1 , D ), preserve_dims = 4 ) assert mod ( x ) . shape == ( B , H , W , C , 1 , D ) submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. __init__ ( self , output_shape , preserve_dims = 1 , ** kwargs ) special Constructs a Reshape module. Parameters: Name Type Description Default output_shape Sequence[int] Shape to reshape the input tensor to while preserving its first preserve_dims dimensions. When the special value -1 appears in output_shape the corresponding size is automatically inferred. Note that -1 can only appear once in output_shape . To flatten all non-batch dimensions use Flatten . required preserve_dims int Number of leading dimensions that will not be reshaped. 1 kwargs Additional keyword arguments passed to Module. {} Exceptions: Type Description ValueError If preserve_dims is not positive. Source code in elegy/nn/flatten.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , output_shape : types . Shape , preserve_dims : int = 1 , ** kwargs ): \"\"\" Constructs a `Reshape` module. Args: output_shape: Shape to reshape the input tensor to while preserving its first `preserve_dims` dimensions. When the special value -1 appears in `output_shape` the corresponding size is automatically inferred. Note that -1 can only appear once in `output_shape`. To flatten all non-batch dimensions use `Flatten`. preserve_dims: Number of leading dimensions that will not be reshaped. kwargs: Additional keyword arguments passed to Module. Raises: ValueError: If `preserve_dims` is not positive. \"\"\" super () . __init__ ( ** kwargs ) if preserve_dims <= 0 : raise ValueError ( \"Argument preserve_dims should be >= 1.\" ) if output_shape . count ( - 1 ) > 1 : raise ValueError ( \"-1 can only occur once in `output_shape`.\" ) self . output_shape = tuple ( output_shape ) self . preserve_dims = preserve_dims apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/flatten.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , inputs ) Parameters: Name Type Description Default inputs ndarray the array to be reshaped. required Returns: Type Description ndarray A reshaped array. Source code in elegy/nn/flatten.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Arguments: inputs: the array to be reshaped. Returns: A reshaped array. \"\"\" if inputs . ndim <= self . preserve_dims : return inputs if - 1 in self . output_shape : reshaped_shape = _infer_shape ( self . output_shape , inputs . shape [ self . preserve_dims :] ) else : reshaped_shape = self . output_shape shape = inputs . shape [: self . preserve_dims ] + reshaped_shape return jnp . reshape ( inputs , shape ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/flatten.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/flatten.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/flatten.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/flatten.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/flatten.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/flatten.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Reshape"},{"location":"api/nn/Reshape/#elegynnreshape","text":"","title":"elegy.nn.Reshape"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape","text":"Reshapes input Tensor, preserving the batch dimension. For example, given an input tensor with shape [B, H, W, C, D] : B , H , W , C , D = range ( 1 , 6 ) x = jnp . ones ([ B , H , W , C , D ]) The default behavior when output_shape is (-1, D) is to flatten all dimensions between B and D : mod = elegy . nn . Reshape ( output_shape = ( - 1 , D )) assert mod ( x ) . shape == ( B , H * W * C , D ) You can change the number of preserved leading dimensions via preserve_dims : mod = elegy . nn . Reshape ( output_shape = ( - 1 , D ), preserve_dims = 2 ) assert mod ( x ) . shape == ( B , H , W * C , D ) mod = elegy . nn . Reshape ( output_shape = ( - 1 , D ), preserve_dims = 3 ) assert mod ( x ) . shape == ( B , H , W , C , D ) mod = elegy . nn . Reshape ( output_shape = ( - 1 , D ), preserve_dims = 4 ) assert mod ( x ) . shape == ( B , H , W , C , 1 , D )","title":"elegy.nn.flatten.Reshape"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.__init__","text":"Constructs a Reshape module. Parameters: Name Type Description Default output_shape Sequence[int] Shape to reshape the input tensor to while preserving its first preserve_dims dimensions. When the special value -1 appears in output_shape the corresponding size is automatically inferred. Note that -1 can only appear once in output_shape . To flatten all non-batch dimensions use Flatten . required preserve_dims int Number of leading dimensions that will not be reshaped. 1 kwargs Additional keyword arguments passed to Module. {} Exceptions: Type Description ValueError If preserve_dims is not positive. Source code in elegy/nn/flatten.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , output_shape : types . Shape , preserve_dims : int = 1 , ** kwargs ): \"\"\" Constructs a `Reshape` module. Args: output_shape: Shape to reshape the input tensor to while preserving its first `preserve_dims` dimensions. When the special value -1 appears in `output_shape` the corresponding size is automatically inferred. Note that -1 can only appear once in `output_shape`. To flatten all non-batch dimensions use `Flatten`. preserve_dims: Number of leading dimensions that will not be reshaped. kwargs: Additional keyword arguments passed to Module. Raises: ValueError: If `preserve_dims` is not positive. \"\"\" super () . __init__ ( ** kwargs ) if preserve_dims <= 0 : raise ValueError ( \"Argument preserve_dims should be >= 1.\" ) if output_shape . count ( - 1 ) > 1 : raise ValueError ( \"-1 can only occur once in `output_shape`.\" ) self . output_shape = tuple ( output_shape ) self . preserve_dims = preserve_dims","title":"__init__()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/flatten.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.call","text":"Parameters: Name Type Description Default inputs ndarray the array to be reshaped. required Returns: Type Description ndarray A reshaped array. Source code in elegy/nn/flatten.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Arguments: inputs: the array to be reshaped. Returns: A reshaped array. \"\"\" if inputs . ndim <= self . preserve_dims : return inputs if - 1 in self . output_shape : reshaped_shape = _infer_shape ( self . output_shape , inputs . shape [ self . preserve_dims :] ) else : reshaped_shape = self . output_shape shape = inputs . shape [: self . preserve_dims ] + reshaped_shape return jnp . reshape ( inputs , shape )","title":"call()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/flatten.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/flatten.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/flatten.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/flatten.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/flatten.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/flatten.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/Sequential/","text":"elegy.nn.Sequential Sequentially calls the given list of layers. Note that Sequential is limited in the range of possible architectures it can handle. This is a deliberate design decision; Sequential is only meant to be used for the simple case of fusing together modules/ops where the input of a particular module/op is the output of the previous one. Another restriction is that it is not possible to have extra arguments in the call method that are passed to the constituents of the module - for example, if there is a BatchNorm module in Sequential and the user wishes to switch the training flag. If this is the desired use case, the recommended solution is to subclass Module and implement call : class CustomModule ( elegy . Module ): def call ( self , x , training ): x = elegy . nn . Conv2D ( 32 , 4 , 2 )( x ) x = elegy . nn . BatchNorm ( True , True , 0.9 )( x , training ) x = jax . nn . relu ( x ) return x submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/sequential_module.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , * args , ** kwargs ) Connects all layers. args and *kwargs are passed to the first layer. Source code in elegy/nn/sequential_module.py 119 120 121 def call ( self , * args , ** kwargs ): \"\"\"Connects all layers. *args and **kwargs are passed to the first layer.\"\"\" return sequential ( * self . layers )( * args , ** kwargs ) get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/sequential_module.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/sequential_module.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/sequential_module.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/sequential_module.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/sequential_module.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/sequential_module.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"Sequential"},{"location":"api/nn/Sequential/#elegynnsequential","text":"","title":"elegy.nn.Sequential"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential","text":"Sequentially calls the given list of layers. Note that Sequential is limited in the range of possible architectures it can handle. This is a deliberate design decision; Sequential is only meant to be used for the simple case of fusing together modules/ops where the input of a particular module/op is the output of the previous one. Another restriction is that it is not possible to have extra arguments in the call method that are passed to the constituents of the module - for example, if there is a BatchNorm module in Sequential and the user wishes to switch the training flag. If this is the desired use case, the recommended solution is to subclass Module and implement call : class CustomModule ( elegy . Module ): def call ( self , x , training ): x = elegy . nn . Conv2D ( 32 , 4 , 2 )( x ) x = elegy . nn . BatchNorm ( True , True , 0.9 )( x , training ) x = jax . nn . relu ( x ) return x","title":"elegy.nn.sequential_module.Sequential"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/nn/sequential_module.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.call","text":"Connects all layers. args and *kwargs are passed to the first layer. Source code in elegy/nn/sequential_module.py 119 120 121 def call ( self , * args , ** kwargs ): \"\"\"Connects all layers. *args and **kwargs are passed to the first layer.\"\"\" return sequential ( * self . layers )( * args , ** kwargs )","title":"call()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/sequential_module.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/nn/sequential_module.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/nn/sequential_module.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/nn/sequential_module.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/sequential_module.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/nn/sequential_module.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/nn/sequential/","text":"elegy.nn.sequential Connects all layers. args and *kwargs are passed to the first layer. def call ( self , x ): mlp = elegy . nn . sequential ( elegy . nn . Linear ( 64 ), jax . nn . relu , elegy . nn . Linear ( 32 ), jax . nn . relu , elegy . nn . Linear ( 10 ), jax . nn . softmax , ) y = mlp ( x ) ... Note sequential is not a Module , that is, it wont create a scope over the layers it runs, in constrast to Sequential layers are eagerly instantiate outside of sequential and just passed to it to automate the execution. Parameters: Name Type Description Default layers Callable[..., Any] Modules or functions passed as *args () Returns: Type Description Callable[..., Any] A callable that waits for the inputs and applies the layers sequentially. Source code in elegy/nn/sequential_module.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def sequential ( * layers : tp . Callable [ ... , tp . Any ]) -> tp . Callable [ ... , tp . Any ]: \"\"\" Connects all layers. *args and **kwargs are passed to the first layer. ```python def call(self, x): mlp = elegy.nn.sequential( elegy.nn.Linear(64), jax.nn.relu, elegy.nn.Linear(32), jax.nn.relu, elegy.nn.Linear(10), jax.nn.softmax, ) y = mlp(x) ... ``` !!! Note `sequential` is not a `Module`, that is, it wont create a scope over the layers it runs, in constrast to `Sequential` layers are eagerly instantiate outside of `sequential` and just passed to it to automate the execution. Arguments: layers: Modules or functions passed as `*args` Returns: A callable that waits for the inputs and applies the layers sequentially. \"\"\" def call ( inputs , * args , ** kwargs ): if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] if not context . module_c : raise ValueError ( \"Cannot execute `sequential` outside of a module's `call` or `init`.\" ) module : Module = context . module_c [ - 1 ] out = inputs for i , layer in enumerate ( layers ): if i == 0 : out = layer ( out , * args , ** kwargs ) else : out = layer ( out ) if not isinstance ( layer , Module ): name = ( layer . __name__ if hasattr ( layer , \"__name__\" ) else layer . __class__ . __name__ ) hooks . add_summary ( name , out ) return out else : raise ValueError ( \"Cannot execute `sequential` outside of an `elegy.context`\" ) return call","title":"sequential"},{"location":"api/nn/sequential/#elegynnsequential","text":"","title":"elegy.nn.sequential"},{"location":"api/nn/sequential/#elegy.nn.sequential_module.sequential","text":"Connects all layers. args and *kwargs are passed to the first layer. def call ( self , x ): mlp = elegy . nn . sequential ( elegy . nn . Linear ( 64 ), jax . nn . relu , elegy . nn . Linear ( 32 ), jax . nn . relu , elegy . nn . Linear ( 10 ), jax . nn . softmax , ) y = mlp ( x ) ... Note sequential is not a Module , that is, it wont create a scope over the layers it runs, in constrast to Sequential layers are eagerly instantiate outside of sequential and just passed to it to automate the execution. Parameters: Name Type Description Default layers Callable[..., Any] Modules or functions passed as *args () Returns: Type Description Callable[..., Any] A callable that waits for the inputs and applies the layers sequentially. Source code in elegy/nn/sequential_module.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def sequential ( * layers : tp . Callable [ ... , tp . Any ]) -> tp . Callable [ ... , tp . Any ]: \"\"\" Connects all layers. *args and **kwargs are passed to the first layer. ```python def call(self, x): mlp = elegy.nn.sequential( elegy.nn.Linear(64), jax.nn.relu, elegy.nn.Linear(32), jax.nn.relu, elegy.nn.Linear(10), jax.nn.softmax, ) y = mlp(x) ... ``` !!! Note `sequential` is not a `Module`, that is, it wont create a scope over the layers it runs, in constrast to `Sequential` layers are eagerly instantiate outside of `sequential` and just passed to it to automate the execution. Arguments: layers: Modules or functions passed as `*args` Returns: A callable that waits for the inputs and applies the layers sequentially. \"\"\" def call ( inputs , * args , ** kwargs ): if LOCAL . contexts : context : Context = LOCAL . contexts [ - 1 ] if not context . module_c : raise ValueError ( \"Cannot execute `sequential` outside of a module's `call` or `init`.\" ) module : Module = context . module_c [ - 1 ] out = inputs for i , layer in enumerate ( layers ): if i == 0 : out = layer ( out , * args , ** kwargs ) else : out = layer ( out ) if not isinstance ( layer , Module ): name = ( layer . __name__ if hasattr ( layer , \"__name__\" ) else layer . __class__ . __name__ ) hooks . add_summary ( name , out ) return out else : raise ValueError ( \"Cannot execute `sequential` outside of an `elegy.context`\" ) return call","title":"elegy.nn.sequential_module.sequential"},{"location":"api/regularizers/GlobalL1/","text":"elegy.regularizers.GlobalL1 Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . regularizers . GlobalL1 ( l = 1e-5 ) ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L1 regularization factor. 0.01 kwargs Additional keyword arguments passed to Module. {} Returns: Type Description GlobalL1L2 An L1 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l1.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def GlobalL1 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l1_regularization\" , ** kwargs ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.regularizers.GlobalL1(l=1e-5) ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L1 regularization factor. kwargs: Additional keyword arguments passed to Module. Returns: An L1 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l1 = l , reduction = reduction , name = name , ** kwargs )","title":"GlobalL1"},{"location":"api/regularizers/GlobalL1/#elegyregularizersgloball1","text":"","title":"elegy.regularizers.GlobalL1"},{"location":"api/regularizers/GlobalL1/#elegy.regularizers.global_l1.GlobalL1","text":"Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . regularizers . GlobalL1 ( l = 1e-5 ) ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L1 regularization factor. 0.01 kwargs Additional keyword arguments passed to Module. {} Returns: Type Description GlobalL1L2 An L1 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l1.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def GlobalL1 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l1_regularization\" , ** kwargs ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.regularizers.GlobalL1(l=1e-5) ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L1 regularization factor. kwargs: Additional keyword arguments passed to Module. Returns: An L1 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l1 = l , reduction = reduction , name = name , ** kwargs )","title":"elegy.regularizers.global_l1.GlobalL1"},{"location":"api/regularizers/GlobalL1L2/","text":"elegy.regularizers.GlobalL1L2 A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . regularizers . GlobalL1L2 ( l1 = 1e-5 , l2 = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Attributes: Name Type Description l1 L1 regularization factor. l2 L2 regularization factor. submodules: Dict [ str , Any ] inherited property readonly A dictionary with all submodules contained in this Module. apply ( self , parameters = None , states = None , rng = None , get_summaries = False , training = True ) inherited Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/regularizers/global_l1l2.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn call ( self , parameters ) Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default parameters Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]] A structure with all the parameters of the model. required Source code in elegy/regularizers/global_l1l2.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def call ( self , parameters : hk . Params ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: parameters: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in jax . tree_leaves ( parameters ) ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in jax . tree_leaves ( parameters ) ) return regularization get_parameters ( self ) inherited Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/regularizers/global_l1l2.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params get_states ( self , _initial = False ) inherited Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/regularizers/global_l1l2.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states init ( self , rng = None ) inherited Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/regularizers/global_l1l2.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn reset ( self ) inherited Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/regularizers/global_l1l2.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self ) set_parameters ( self , values ) inherited Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/regularizers/global_l1l2.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" ) set_states ( self , values ) inherited Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/regularizers/global_l1l2.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegyregularizersgloball1l2","text":"","title":"elegy.regularizers.GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2","text":"A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . regularizers . GlobalL1L2 ( l1 = 1e-5 , l2 = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Attributes: Name Type Description l1 L1 regularization factor. l2 L2 regularization factor.","title":"elegy.regularizers.global_l1l2.GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.submodules","text":"A dictionary with all submodules contained in this Module.","title":"submodules"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.apply","text":"Applies your function injecting some context arguments. Parameters: Name Type Description Default parameters Optional[Dict] None states Optional[Dict] None rng Optional[Union[numpy.ndarray, int]] None get_summaries bool False Returns: Type Description ApplyCallable A function with the same signature as call that will execute the computation given the context arguments passed to apply . Source code in elegy/regularizers/global_l1l2.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def apply ( self , parameters : tp . Optional [ tp . Dict ] = None , states : tp . Optional [ tp . Dict ] = None , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None , get_summaries : bool = False , training : bool = True , ) -> \"ApplyCallable\" : \"\"\" Applies your function injecting some context arguments. Arguments: parameters: states: rng: get_summaries: Returns: A function with the same signature as `call` that will execute the computation given the context arguments passed to `apply`. \"\"\" @utils . wraps ( self . call ) def apply_fn ( * args , ** kwargs ): current_parameters = self . get_parameters () current_states = self . get_states () assert current_parameters is not None assert current_states is not None if parameters is not None : self . set_parameters ( parameters ) if states is not None : self . set_states ( states ) with context ( rng = rng , building = False , training = training , get_summaries = get_summaries , ) as ctx : outputs = self ( * args , ** kwargs ) output_parameters = self . get_parameters () output_states = self . get_states () if parameters is not None : self . set_parameters ( current_parameters ) if states is not None : self . set_states ( current_states ) return ( outputs , ApplyContext ( parameters = output_parameters , states = output_states , losses = ctx . losses , metrics = ctx . metrics , summaries = ctx . summaries , ), ) return apply_fn","title":"apply()"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.call","text":"Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default parameters Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]] A structure with all the parameters of the model. required Source code in elegy/regularizers/global_l1l2.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def call ( self , parameters : hk . Params ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: parameters: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in jax . tree_leaves ( parameters ) ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in jax . tree_leaves ( parameters ) ) return regularization","title":"call()"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.get_parameters","text":"Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/regularizers/global_l1l2.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def get_parameters ( self ) -> types . Parameters : \"\"\" Recursively collects a dictionary with the parameters of this module and all submodules within it. Returns: \"\"\" params = module_tree_map ( lambda module : { key : getattr ( module , key ) for key in getattr ( module , \"_params\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( params , tp . Dict ) return params","title":"get_parameters()"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.get_states","text":"Recursively collects a dictionary with the states of this module and all submodules within it. Returns: Type Description Dict[str, Any] Source code in elegy/regularizers/global_l1l2.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def get_states ( self , _initial : bool = False ) -> types . States : \"\"\" Recursively collects a dictionary with the states of this module and all submodules within it. Returns: \"\"\" states = module_tree_map ( lambda module : { key : getattr ( module , as_initial ( key )) if _initial else getattr ( module , key ) for key in getattr ( module , \"_states\" ) if hasattr ( module , key ) }, self , ) assert isinstance ( states , tp . Dict ) return states","title":"get_states()"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.init","text":"Initializes the module. Parameters: Name Type Description Default rng Optional[Union[numpy.ndarray, int]] None Returns: Type Description InitCallable Source code in elegy/regularizers/global_l1l2.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def init ( self , rng : tp . Optional [ tp . Union [ np . ndarray , int ]] = None ) -> \"InitCallable\" : \"\"\" Initializes the module. Arguments: rng: Returns: \"\"\" @utils . wraps ( self ) def init_fn ( * args , ** kwargs ): with context ( rng = rng , building = True , get_summaries = False ): self ( * args , ** kwargs ) params = self . get_parameters () initial_states = self . get_states ( _initial = True ) self . clear_initial_states () self . set_states ( initial_states ) return params , initial_states return init_fn","title":"init()"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.reset","text":"Recursively deletes the parameters and states of this module and all submodules within it. Source code in elegy/regularizers/global_l1l2.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def reset ( self ): \"\"\" Recursively deletes the parameters and states of this module and all submodules within it. \"\"\" def clear_module ( module : Module ): for name in module . _params : delattr ( module , name ) for name in module . _states : delattr ( module , name ) # module._params = set() # module._states = set() tree_exec ( clear_module , self )","title":"reset()"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.set_parameters","text":"Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/regularizers/global_l1l2.py 290 291 292 293 294 295 296 def set_parameters ( self , values : types . Parameters ) -> None : \"\"\" Recursively sets the parameters of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_params\" )","title":"set_parameters()"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.set_states","text":"Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. Source code in elegy/regularizers/global_l1l2.py 318 319 320 321 322 323 324 def set_states ( self , values : types . States ): \"\"\" Recursively sets the states of this module and all submodules within it given a dictionary with a corresponding structure. \"\"\" set_tree ( self , values , \"_states\" )","title":"set_states()"},{"location":"api/regularizers/GlobalL2/","text":"elegy.regularizers.GlobalL2 Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . losses . GlobaL2Regularization ( l = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L2 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L2 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l2.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def GlobalL2 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l2_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.losses.GlobaL2Regularization(l=1e-4), ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L2 regularization factor. Returns: An L2 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l2 = l , reduction = reduction , name = name )","title":"GlobalL2"},{"location":"api/regularizers/GlobalL2/#elegyregularizersgloball2","text":"","title":"elegy.regularizers.GlobalL2"},{"location":"api/regularizers/GlobalL2/#elegy.regularizers.global_l2.GlobalL2","text":"Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( module_fn , loss = [ elegy . losses . SparseCategoricalCrossentropy (), elegy . losses . GlobaL2Regularization ( l = 1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L2 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L2 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l2.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def GlobalL2 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l2_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.losses.GlobaL2Regularization(l=1e-4), ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L2 regularization factor. Returns: An L2 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l2 = l , reduction = reduction , name = name )","title":"elegy.regularizers.global_l2.GlobalL2"},{"location":"guides/contributing/","text":"Contributing This is a short guide on how to start contibuting to Elegy along with some best practices for the project. Setup We use poetry so the easiest way to setup a development environment is run poetry install Creating Losses and Metrics For this you can follow these guidelines: Each loss / metric should be defined in its own file. Inherit from either elegy.losses.loss.Loss or elegy.metrics.metric.Metric or an existing class that inherits from them. Try to use an existing metric or loss as a template You must provide documentation for the following: The class definition. The __init__ method. The call method. Try to port the documentation + signature from its Keras counter part. If so you must give credits to the original source file. You must include tests. If you there exists an equivalent loss/metric in Keras you must test numerical equivalence between both. Testing To execute all the tests just run pytest Documentation We use mkdocs . If you create a new object that requires documentation please do the following: Add a markdown file inside /docs/api in the appropriate location according to the project's structure. This file must: Contain the path of function / class as header Use mkdocstring to render the API information. Example: # elegy . losses . BinaryCrossentropy ::: elegy . losses . BinaryCrossentropy selection : inherited_members : true members : - call - __init__ Add and entry to mkdocs.yml inside nav pointing to this file. Checkout mkdocs.yml . To build and visualize the documentation locally run mkdocs serve Creating a PR Before sending a pull request make sure all test run and code is formatted with black : black .","title":"Contibuting"},{"location":"guides/contributing/#contributing","text":"This is a short guide on how to start contibuting to Elegy along with some best practices for the project.","title":"Contributing"},{"location":"guides/contributing/#setup","text":"We use poetry so the easiest way to setup a development environment is run poetry install","title":"Setup"},{"location":"guides/contributing/#creating-losses-and-metrics","text":"For this you can follow these guidelines: Each loss / metric should be defined in its own file. Inherit from either elegy.losses.loss.Loss or elegy.metrics.metric.Metric or an existing class that inherits from them. Try to use an existing metric or loss as a template You must provide documentation for the following: The class definition. The __init__ method. The call method. Try to port the documentation + signature from its Keras counter part. If so you must give credits to the original source file. You must include tests. If you there exists an equivalent loss/metric in Keras you must test numerical equivalence between both.","title":"Creating Losses and Metrics"},{"location":"guides/contributing/#testing","text":"To execute all the tests just run pytest","title":"Testing"},{"location":"guides/contributing/#documentation","text":"We use mkdocs . If you create a new object that requires documentation please do the following: Add a markdown file inside /docs/api in the appropriate location according to the project's structure. This file must: Contain the path of function / class as header Use mkdocstring to render the API information. Example: # elegy . losses . BinaryCrossentropy ::: elegy . losses . BinaryCrossentropy selection : inherited_members : true members : - call - __init__ Add and entry to mkdocs.yml inside nav pointing to this file. Checkout mkdocs.yml . To build and visualize the documentation locally run mkdocs serve","title":"Documentation"},{"location":"guides/contributing/#creating-a-pr","text":"Before sending a pull request make sure all test run and code is formatted with black : black .","title":"Creating a PR"},{"location":"guides/module-system/","text":"The Module System This is a guide to Elegy's underlying Module System. It will help get a better understanding of how Elegy interacts with Jax at the lower level, certain details about the hooks system and how it differs from other Deep Learning frameworks. Traditional Object Oriented Style We will begin by exploring other frameworks define Modules / Layers. It is very common to use Object Oriented architectures as backbones of Module systems as it helps frameworks keep track of the parameters and states each module might require. Here we will create a some very basic Linear and MLP modules which will seem very familiar: class Linear ( elegy . Module ): def __init__ ( self , n_in , n_out ): super () . __init__ () self . w = elegy . get_parameter ( \"w\" , [ x . shape [ - 1 ], self . n_out ], initializer = elegy . initializers . RandomUniform (), ) self . b = elegy . get_parameter ( \"b\" , [ n_out ], initializer = elegy . initializers . RandomUniform ()) def call ( self , x ): return jnp . dot ( x , self . w ) + self . b class MLP ( elegy . Module ): def __init__ ( self , n_in ): self . linear1 = Linear ( n_in , 64 ) self . linear2 = Linear ( 64 , 32 ) self . linear3 = Linear ( 32 , 1 ) def call ( self , x ): x = self . linear1 ( x ) x = jax . nn . relu ( x ) x = self . linear2 ( x ) x = jax . nn . relu ( x ) x = self . linear3 ( x ) return x Here we just defined a simple linear layer and used it inside a MLP with 3 layers. Pytorch and Keras users should feel very familiar with this type of code: we define parameters or other submodules in the __init__ method, and use them during the call (forward) method. Keras users users might complain that if we do things this way we loose the ability to do shape inference, but don't worry, we will fix that latter. Fow now it is important to notice that here we use our first hook: get_parameter . Elegy Hooks Hooks are a way in which we can manage state while preserving functional purity (in the end). Elegy's hook system is ported and expanded from Haiku, but hook-based functional architectures in other areas like web development have proven valuable, React Hooks being a recent notable success. In Elegy we have the following list of hooks: Hook Description get_parameter Gives us access to a trainable parameter. get_state Gives us access to some state. This is used in layers like BatchNormalization and in most of the metrics. set_state Lets us update a state. When used in conjunction with get_state it lets use express an iterative computation. next_rng_key Gives us access to a unique PRNGKey we can pass to functions like jax.random.uniform and friends. is_training Tells use whether training is currently happening or not. add_loss Lets us declare a loss in some intermediate layer. add_metric Lets us declare a metric in some intermediate layer. add_summary Lets us declare a summary in some intermediate layer. Note If you use existing Module s you might not need to worry much about these hooks, but keep them in mind if you are developing your own custom modules. Module Hooks: Functional Style In the initial example we used hooks in a very shy manner to replicate the behavior of of other frameworks, now we will go beyond. The first thing we need to know is that: Quote Modules are hooks This means that module instantiation taps into the hook system, and that hooks are aware of the module in which they are executing in. In practice this will mean that we will be able to move a lot of the code defined on the __init__ method to the call method: class Linear ( elegy . Module ): def __init__ ( self , n_out ): super () . __init__ () self . n_out = n_out def call ( self , x ): w = elegy . get_parameter ( \"w\" , [ x . shape [ - 1 ], self . n_out ], initializer = elegy . initializers . RandomUniform (), ) b = elegy . get_parameter ( \"b\" , [ self . n_out ], initializer = jnp . zeros ) return jnp . dot ( x , w ) + b class MLP ( elegy . Module ): def call ( self , x ): x = Linear ( 64 )( x ) x = jax . nn . relu ( x ) x = Linear ( 32 )( x ) x = jax . nn . relu ( x ) x = Linear ( 1 )( x ) return x What happened here? Lets decompose it into two parts. First we moved the get_parameter definitions on the Linear module to the call method: class Linear ( elegy . Module ): def __init__ ( self , n_out ): super () . __init__ () self . n_out = n_out def call ( self , x ): w = elegy . get_parameter ( \"w\" , [ x . shape [ - 1 ], self . n_out ], initializer = elegy . initializers . RandomUniform (), ) b = elegy . get_parameter ( \"b\" , [ self . n_out ], initializer = jnp . zeros ) return jnp . dot ( x , w ) + b As you see this allows us to do shape inference since we have access to the inputs when defining our parameter's shape. Second, we also moved the instantiation of the Linear modules in MLP from __init__ to call : class MLP ( elegy . Module ): def call ( self , x ): x = Linear ( 64 )( x ) x = jax . nn . relu ( x ) x = Linear ( 32 )( x ) x = jax . nn . relu ( x ) x = Linear ( 1 )( x ) return x Here we are using Modules as hooks. While it may appear as if we are instantiating 3 new Linear modules on every call , Elegy is actually caching them behind the scenes with the help of Python metaclasses. There is one important rule you have to follow: Quote You must use hooks unconditionally This moto comes from React and it means that the module always has to call the same amount of hooks, and for module hooks specifically they must be called in the same order. For example the following code is invalid: def call ( self , x ): if x . shape [ 0 ] > 5 : x = elegy . nn . Conv2D ( 32 , [ 3 , 3 ])( x ) x = elegy . nn . Linear ( 48 , [ 3 , 3 ])( x ) else : x = elegy . nn . Linear ( 48 , [ 3 , 3 ])( x ) x = elegy . nn . Conv2D ( 32 , [ 3 , 3 ])( x ) return x Here Linear and Conv2D are dangerously swapped based on some condition. If you want to do this you can just clare them unconditionally and use them inside the condition: def call ( self , x ): linear = elegy . nn . Linear ( 48 , [ 3 , 3 ]) conv2d = elegy . nn . Conv2D ( 32 , [ 3 , 3 ]) if x . shape [ 0 ] > 5 : x = conv2d ( x ) x = linear ( x ) else : x = linear ( x ) x = conv2d ( x ) return x init & apply This functional freedom inside Modules comes at a cost outside of them which is that you cannot call the top-level module directly. If you try to run this code: x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () y = mlp ( x ) You will get the following error: ValueError: Cannot execute call outside of a elegy.context In practice this means that you will have to use the methods init and apply to manage your modules: x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () rngs = elegy . PRNGSequence ( 42 ) mlp . init ( rng = next ( rngs ))( x ) y_pred , ctx = mlp . apply ( rng = next ( rngs ))( x ) A lot is happening here so lets unpack it. First we used Module.init(...) and passed it an rng key. x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () rngs = elegy . PRNGSequence ( 42 ) mlp . init ( rng = next ( rngs ))( x ) y_pred , ctx = mlp . apply ( rng = next ( rngs ))( x ) This key is necessary because Linear uses elegy.initializers.RandomUniform which requires a random key to initialize our weights. init takes in some parameters and returns callable which expect the same arguments as call and will initialize our module. Next we use Module.apply(...) very similarly but this time we get back some predictions and a context: x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () rngs = elegy . PRNGSequence ( 42 ) mlp . init ( rng = next ( rngs ))( x ) y_pred , ctx = mlp . apply ( rng = next ( rngs ))( x ) Right now we won't use this context object but its useful to know that this object will collect most of the information given by hooks like get_parameter , get_state , add_loss , add_metric , etc. Hooks Preserve References In our MLP class we where able to create the Linear modules at their call site, this simplified our code a lot but we've seem to lost the reference to these modules. Having reference to other modules is critical for being able to e.g. easily compose modules that might be trained separately like in transfer learning, or being able to easily decompose / extract a sub-module and use it separately like when using the decoder of a VAE by itself to generate new samples. Because of this, Elegy actually assigns all submodules, parameters, and states as properties of the module: x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () rngs = elegy . PRNGSequence ( 42 ) mlp . init ( rng = next ( rngs ))( x ) linear , linear_1 , linear_2 = mlp . linear , mlp . linear_1 , mlp . linear_2 y_pred , ctx = mlp . apply ( rng = next ( rngs ))( x ) assert linear is mlp . linear and linear_1 is mlp . linear_1 and linear_2 is mlp . linear_2 As you see we were able to access all the linear layer references. More over, we verified that these reference don't change during execution. Each submodule gets assigned to a a unique field name based on its class name and order of creation. You can customize this name by using the name argument available in the Module 's constructor. Managing State A big theme in Jax is that is that state and computation are separate, this is a requirement because in order for combinators like jax.grad and jax.jit to work you need pure functions, and pure functions usually require you to extract state and turn it into an input. To achieve this we will use additional feature from init and apply that where created for this purpose: x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () rngs = elegy . PRNGSequence ( 42 ) parameters , states = mlp . init ( rng = next ( rngs ))( x ) y_pred , ctx = mlp . apply ( parameters = parameters , states = states , rng = next ( rngs ))( x ) The first thing is that init actually returns the initial parameters and states , these are dictionaries whose structure has a 1-to-1 correspondence with the structure of the network. The second is that apply accepts these parameters and states as arguments. This is a step in the right direction since now our state is externalized. Low-level Training Loop Using the previous plus regular jax we can actually implement a basic training loop for our module. We will go ahead and show the solution and explain it afterwards: def loss ( parameters , rng , x , y ): y_pred , ctx = mlp . apply ( parameters = parameters , states = states , rng = rng , training = True )( x ) return jnp . mean ( jnp . square ( y - y_pred )) @jax . jit def update ( parameters , rng , x , y ): gradients = jax . grad ( loss )( parameters , rng , x , y ) parameters = jax . tree_multimap ( lambda p , g : p - 0.001 * g , parameters , gradients ) return parameters x = np . random . uniform ( size = ( 15 , 3 )) y = np . random . uniform ( size = ( 15 , 1 )) mlp = MLP () parameters , states = mlp . init ( rng = next ( rngs ))( x ) for step in range ( 1000 ): parameters = update ( parameters , next ( rngs ), x , y ) mlp . set_parameters ( parameters ) Here we created the functions loss and update , plus a minimal training loop. In order to for us to be able to calculate gradients of the loss with respect to the parameters we need for loss to take them an argument along with the inputs x and labels y : def loss ( parameters , rng , x , y ): y_pred , ctx = mlp . apply ( parameters = parameters , states = states , rng = rng , training = True )( x ) return jnp . mean ( jnp . square ( y - y_pred )) @jax . jit def update ( parameters , rng , x , y ): gradients = jax . grad ( loss )( parameters , rng , x , y ) parameters = jax . tree_multimap ( lambda p , g : p - 0.001 * g , parameters , gradients ) return parameters Note that grad by default calculate the gradient of the function with respect to the first argument, which in this case is a structure with all the parameters. Because we are using jax.jit we also require that any desired changes propagated as outputs: def loss ( parameters , rng , x , y ): y_pred , ctx = mlp . apply ( parameters = parameters , states = states , rng = rng , training = True )( x ) return jnp . mean ( jnp . square ( y - y_pred )) @jax . jit def update ( parameters , rng , x , y ): gradients = jax . grad ( loss )( parameters , rng , x , y ) parameters = jax . tree_multimap ( lambda p , g : p - 0.001 * g , parameters , gradients ) return parameters The strategy is to iteratively update the parameters of the network by feeding them as inputs and reassigning them after the output. x = np . random . uniform ( size = ( 15 , 3 )) y = np . random . uniform ( size = ( 15 , 1 )) mlp = MLP () parameters , states = mlp . init ( rng = next ( rngs ))( x ) for step in range ( 1000 ): parameters = update ( parameters , next ( rngs ), x , y ) mlp . set_parameters ( parameters ) Note that once we are done training we can actually insert these learned parameters back into our module objects by using set_parameters . High Level Equivalent If all this seems a bit too manual for you don't worry, you can can easily express all the previous in a few lines of code using an elegy.Model : model = elegy . Model ( module = elegy . nn . Sequential ( lambda : [ elegy . nn . Linear ( 64 ), jax . nn . relu , elegy . nn . Linear ( 32 ), jax . nn . relu , elegy . nn . Linear ( 1 ), ] ), loss = elegy . losses . MeanSquaredError (), ) model . fit ( x = np . random . uniform ( size = ( 15 , 3 )), y = np . random . uniform ( size = ( 15 , 1 )), batch_size = 15 , epochs = 1000 , )","title":"The Module System"},{"location":"guides/module-system/#the-module-system","text":"This is a guide to Elegy's underlying Module System. It will help get a better understanding of how Elegy interacts with Jax at the lower level, certain details about the hooks system and how it differs from other Deep Learning frameworks.","title":"The Module System"},{"location":"guides/module-system/#traditional-object-oriented-style","text":"We will begin by exploring other frameworks define Modules / Layers. It is very common to use Object Oriented architectures as backbones of Module systems as it helps frameworks keep track of the parameters and states each module might require. Here we will create a some very basic Linear and MLP modules which will seem very familiar: class Linear ( elegy . Module ): def __init__ ( self , n_in , n_out ): super () . __init__ () self . w = elegy . get_parameter ( \"w\" , [ x . shape [ - 1 ], self . n_out ], initializer = elegy . initializers . RandomUniform (), ) self . b = elegy . get_parameter ( \"b\" , [ n_out ], initializer = elegy . initializers . RandomUniform ()) def call ( self , x ): return jnp . dot ( x , self . w ) + self . b class MLP ( elegy . Module ): def __init__ ( self , n_in ): self . linear1 = Linear ( n_in , 64 ) self . linear2 = Linear ( 64 , 32 ) self . linear3 = Linear ( 32 , 1 ) def call ( self , x ): x = self . linear1 ( x ) x = jax . nn . relu ( x ) x = self . linear2 ( x ) x = jax . nn . relu ( x ) x = self . linear3 ( x ) return x Here we just defined a simple linear layer and used it inside a MLP with 3 layers. Pytorch and Keras users should feel very familiar with this type of code: we define parameters or other submodules in the __init__ method, and use them during the call (forward) method. Keras users users might complain that if we do things this way we loose the ability to do shape inference, but don't worry, we will fix that latter. Fow now it is important to notice that here we use our first hook: get_parameter .","title":"Traditional Object Oriented Style"},{"location":"guides/module-system/#elegy-hooks","text":"Hooks are a way in which we can manage state while preserving functional purity (in the end). Elegy's hook system is ported and expanded from Haiku, but hook-based functional architectures in other areas like web development have proven valuable, React Hooks being a recent notable success. In Elegy we have the following list of hooks: Hook Description get_parameter Gives us access to a trainable parameter. get_state Gives us access to some state. This is used in layers like BatchNormalization and in most of the metrics. set_state Lets us update a state. When used in conjunction with get_state it lets use express an iterative computation. next_rng_key Gives us access to a unique PRNGKey we can pass to functions like jax.random.uniform and friends. is_training Tells use whether training is currently happening or not. add_loss Lets us declare a loss in some intermediate layer. add_metric Lets us declare a metric in some intermediate layer. add_summary Lets us declare a summary in some intermediate layer. Note If you use existing Module s you might not need to worry much about these hooks, but keep them in mind if you are developing your own custom modules.","title":"Elegy Hooks"},{"location":"guides/module-system/#module-hooks-functional-style","text":"In the initial example we used hooks in a very shy manner to replicate the behavior of of other frameworks, now we will go beyond. The first thing we need to know is that: Quote Modules are hooks This means that module instantiation taps into the hook system, and that hooks are aware of the module in which they are executing in. In practice this will mean that we will be able to move a lot of the code defined on the __init__ method to the call method: class Linear ( elegy . Module ): def __init__ ( self , n_out ): super () . __init__ () self . n_out = n_out def call ( self , x ): w = elegy . get_parameter ( \"w\" , [ x . shape [ - 1 ], self . n_out ], initializer = elegy . initializers . RandomUniform (), ) b = elegy . get_parameter ( \"b\" , [ self . n_out ], initializer = jnp . zeros ) return jnp . dot ( x , w ) + b class MLP ( elegy . Module ): def call ( self , x ): x = Linear ( 64 )( x ) x = jax . nn . relu ( x ) x = Linear ( 32 )( x ) x = jax . nn . relu ( x ) x = Linear ( 1 )( x ) return x What happened here? Lets decompose it into two parts. First we moved the get_parameter definitions on the Linear module to the call method: class Linear ( elegy . Module ): def __init__ ( self , n_out ): super () . __init__ () self . n_out = n_out def call ( self , x ): w = elegy . get_parameter ( \"w\" , [ x . shape [ - 1 ], self . n_out ], initializer = elegy . initializers . RandomUniform (), ) b = elegy . get_parameter ( \"b\" , [ self . n_out ], initializer = jnp . zeros ) return jnp . dot ( x , w ) + b As you see this allows us to do shape inference since we have access to the inputs when defining our parameter's shape. Second, we also moved the instantiation of the Linear modules in MLP from __init__ to call : class MLP ( elegy . Module ): def call ( self , x ): x = Linear ( 64 )( x ) x = jax . nn . relu ( x ) x = Linear ( 32 )( x ) x = jax . nn . relu ( x ) x = Linear ( 1 )( x ) return x Here we are using Modules as hooks. While it may appear as if we are instantiating 3 new Linear modules on every call , Elegy is actually caching them behind the scenes with the help of Python metaclasses. There is one important rule you have to follow: Quote You must use hooks unconditionally This moto comes from React and it means that the module always has to call the same amount of hooks, and for module hooks specifically they must be called in the same order. For example the following code is invalid: def call ( self , x ): if x . shape [ 0 ] > 5 : x = elegy . nn . Conv2D ( 32 , [ 3 , 3 ])( x ) x = elegy . nn . Linear ( 48 , [ 3 , 3 ])( x ) else : x = elegy . nn . Linear ( 48 , [ 3 , 3 ])( x ) x = elegy . nn . Conv2D ( 32 , [ 3 , 3 ])( x ) return x Here Linear and Conv2D are dangerously swapped based on some condition. If you want to do this you can just clare them unconditionally and use them inside the condition: def call ( self , x ): linear = elegy . nn . Linear ( 48 , [ 3 , 3 ]) conv2d = elegy . nn . Conv2D ( 32 , [ 3 , 3 ]) if x . shape [ 0 ] > 5 : x = conv2d ( x ) x = linear ( x ) else : x = linear ( x ) x = conv2d ( x ) return x","title":"Module Hooks: Functional Style"},{"location":"guides/module-system/#init-apply","text":"This functional freedom inside Modules comes at a cost outside of them which is that you cannot call the top-level module directly. If you try to run this code: x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () y = mlp ( x ) You will get the following error: ValueError: Cannot execute call outside of a elegy.context In practice this means that you will have to use the methods init and apply to manage your modules: x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () rngs = elegy . PRNGSequence ( 42 ) mlp . init ( rng = next ( rngs ))( x ) y_pred , ctx = mlp . apply ( rng = next ( rngs ))( x ) A lot is happening here so lets unpack it. First we used Module.init(...) and passed it an rng key. x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () rngs = elegy . PRNGSequence ( 42 ) mlp . init ( rng = next ( rngs ))( x ) y_pred , ctx = mlp . apply ( rng = next ( rngs ))( x ) This key is necessary because Linear uses elegy.initializers.RandomUniform which requires a random key to initialize our weights. init takes in some parameters and returns callable which expect the same arguments as call and will initialize our module. Next we use Module.apply(...) very similarly but this time we get back some predictions and a context: x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () rngs = elegy . PRNGSequence ( 42 ) mlp . init ( rng = next ( rngs ))( x ) y_pred , ctx = mlp . apply ( rng = next ( rngs ))( x ) Right now we won't use this context object but its useful to know that this object will collect most of the information given by hooks like get_parameter , get_state , add_loss , add_metric , etc.","title":"init &amp; apply"},{"location":"guides/module-system/#hooks-preserve-references","text":"In our MLP class we where able to create the Linear modules at their call site, this simplified our code a lot but we've seem to lost the reference to these modules. Having reference to other modules is critical for being able to e.g. easily compose modules that might be trained separately like in transfer learning, or being able to easily decompose / extract a sub-module and use it separately like when using the decoder of a VAE by itself to generate new samples. Because of this, Elegy actually assigns all submodules, parameters, and states as properties of the module: x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () rngs = elegy . PRNGSequence ( 42 ) mlp . init ( rng = next ( rngs ))( x ) linear , linear_1 , linear_2 = mlp . linear , mlp . linear_1 , mlp . linear_2 y_pred , ctx = mlp . apply ( rng = next ( rngs ))( x ) assert linear is mlp . linear and linear_1 is mlp . linear_1 and linear_2 is mlp . linear_2 As you see we were able to access all the linear layer references. More over, we verified that these reference don't change during execution. Each submodule gets assigned to a a unique field name based on its class name and order of creation. You can customize this name by using the name argument available in the Module 's constructor.","title":"Hooks Preserve References"},{"location":"guides/module-system/#managing-state","text":"A big theme in Jax is that is that state and computation are separate, this is a requirement because in order for combinators like jax.grad and jax.jit to work you need pure functions, and pure functions usually require you to extract state and turn it into an input. To achieve this we will use additional feature from init and apply that where created for this purpose: x = np . random . uniform ( size = ( 15 , 3 )) mlp = MLP () rngs = elegy . PRNGSequence ( 42 ) parameters , states = mlp . init ( rng = next ( rngs ))( x ) y_pred , ctx = mlp . apply ( parameters = parameters , states = states , rng = next ( rngs ))( x ) The first thing is that init actually returns the initial parameters and states , these are dictionaries whose structure has a 1-to-1 correspondence with the structure of the network. The second is that apply accepts these parameters and states as arguments. This is a step in the right direction since now our state is externalized.","title":"Managing State"},{"location":"guides/module-system/#low-level-training-loop","text":"Using the previous plus regular jax we can actually implement a basic training loop for our module. We will go ahead and show the solution and explain it afterwards: def loss ( parameters , rng , x , y ): y_pred , ctx = mlp . apply ( parameters = parameters , states = states , rng = rng , training = True )( x ) return jnp . mean ( jnp . square ( y - y_pred )) @jax . jit def update ( parameters , rng , x , y ): gradients = jax . grad ( loss )( parameters , rng , x , y ) parameters = jax . tree_multimap ( lambda p , g : p - 0.001 * g , parameters , gradients ) return parameters x = np . random . uniform ( size = ( 15 , 3 )) y = np . random . uniform ( size = ( 15 , 1 )) mlp = MLP () parameters , states = mlp . init ( rng = next ( rngs ))( x ) for step in range ( 1000 ): parameters = update ( parameters , next ( rngs ), x , y ) mlp . set_parameters ( parameters ) Here we created the functions loss and update , plus a minimal training loop. In order to for us to be able to calculate gradients of the loss with respect to the parameters we need for loss to take them an argument along with the inputs x and labels y : def loss ( parameters , rng , x , y ): y_pred , ctx = mlp . apply ( parameters = parameters , states = states , rng = rng , training = True )( x ) return jnp . mean ( jnp . square ( y - y_pred )) @jax . jit def update ( parameters , rng , x , y ): gradients = jax . grad ( loss )( parameters , rng , x , y ) parameters = jax . tree_multimap ( lambda p , g : p - 0.001 * g , parameters , gradients ) return parameters Note that grad by default calculate the gradient of the function with respect to the first argument, which in this case is a structure with all the parameters. Because we are using jax.jit we also require that any desired changes propagated as outputs: def loss ( parameters , rng , x , y ): y_pred , ctx = mlp . apply ( parameters = parameters , states = states , rng = rng , training = True )( x ) return jnp . mean ( jnp . square ( y - y_pred )) @jax . jit def update ( parameters , rng , x , y ): gradients = jax . grad ( loss )( parameters , rng , x , y ) parameters = jax . tree_multimap ( lambda p , g : p - 0.001 * g , parameters , gradients ) return parameters The strategy is to iteratively update the parameters of the network by feeding them as inputs and reassigning them after the output. x = np . random . uniform ( size = ( 15 , 3 )) y = np . random . uniform ( size = ( 15 , 1 )) mlp = MLP () parameters , states = mlp . init ( rng = next ( rngs ))( x ) for step in range ( 1000 ): parameters = update ( parameters , next ( rngs ), x , y ) mlp . set_parameters ( parameters ) Note that once we are done training we can actually insert these learned parameters back into our module objects by using set_parameters .","title":"Low-level Training Loop"},{"location":"guides/module-system/#high-level-equivalent","text":"If all this seems a bit too manual for you don't worry, you can can easily express all the previous in a few lines of code using an elegy.Model : model = elegy . Model ( module = elegy . nn . Sequential ( lambda : [ elegy . nn . Linear ( 64 ), jax . nn . relu , elegy . nn . Linear ( 32 ), jax . nn . relu , elegy . nn . Linear ( 1 ), ] ), loss = elegy . losses . MeanSquaredError (), ) model . fit ( x = np . random . uniform ( size = ( 15 , 3 )), y = np . random . uniform ( size = ( 15 , 1 )), batch_size = 15 , epochs = 1000 , )","title":"High Level Equivalent"},{"location":"guides/modules-losses-metrics/","text":"Modules, Losses, and Metrics This guide goes into depth on how modules, losses and metrics work in Elegy when used with an elegy.Model . For more in-depth explanation on how they work internally check out the Module System guide. Keras Limitations One of our goals with Elegy was to solve Keras restrictions around the type of losses and metrics you can define. When creating a complex model with multiple outputs in Keras, say output_a and output_b , you are forced to define losses and metrics per-output only: model . compile ( loss = { \"output_a\" : keras . losses . BinaryCrossentropy ( from_logits = True ), \"output_b\" : keras . losses . CategoricalCrossentropy ( from_logits = True ), }, metrics = { \"output_a\" : keras . losses . BinaryAccuracy ( from_logits = True ), \"output_b\" : keras . losses . CategoricalAccuracy ( from_logits = True ), }, ... ) This very restrictive, in particular it doesn't allow the following: Losses and metrics that combine multiple outputs with multiple labels. A single loss/metrics based on multiple outputs (a especial case of the previous). Losses and metrics that depend on other variables such as inputs, parameters, states, etc. Most of these are usually solvable by tricks such as: * Concatenating the outputs / labels * Passing the inputs and other kind of information as labels. * Using the functional API which is more flexible (but it only runs on graph mode making it very painful to debug). It is clear that these solution are hacky, sometimes they are non-obvious, and depending on the problem they can be insufficient. Dependency Injection Elegy solves the previous problems by introducing a dependency injection mechanism that allows the user to express complex functions by simply declaring the variables it wants to use by their name . The following parameters are available for the different callables you pass to Elegy: parameter Description Module Metric Loss x Inputs of the model corresponding to the x argument of fit * x x y_true The input labels corresponding to the y argument of fit x x y_pred Outputs of the model x x sample_weight Importance of each sample x x class_weight Importance of each class x x training Whether training is currently in progress x x x parameters The learnable parameters of the model x x states The non-learnable parameters of the model x x Note The content of x is technically passed to the model's Module but the parameter name \"x\" will bare no special meaning in that context. Modules Modules define the architecture of the network, their primary task (in Elegy terms) is transforming the inputs x into outputs y_pred . To make it easy to consume the content of x , Elegy has some special but very simple rules on how the signature of any Module can be structured: 1. If x is simply an array it will be passed directly: class SomeModule ( elegy . Module ): def call ( self , m ): ... model = elegy . Model ( SomeModule (), ... ) a = get_inputs () model . fit ( x = a , ... ) In this case a is passed as m . 2. If x is a tuple , then x will be expanded positional arguments a.k.a. *args , this means that the module will have define exactly as many arguments as there are inputs. For example: class SomeModule ( elegy . Module ): def call ( self , m , n ): ... model = elegy . Model ( SomeModule (), ... ) a , b = get_inputs () model . fit ( x = ( a , b ), ... ) In this case a is passed as m and b is passed as n . 3. If x is a dict , then x will be expanded as keyword arguments a.k.a. **kwargs , in this case the module can optionally request any key defined in x as an argument. For example: class SomeModule ( elegy . Module ): def call ( self , n , o ): ... model = elegy . Model ( SomeModule (), ... ) a , b , c = get_inputs () model . fit ( x = { \"m\" : a , \"n\" : b , \"o\" : c }, ... ) Here only n and o are requested by name and you get as input its values b and c , the variable m with the content of a is safely ignored. If you want to request all the avaiable inputs you can use **kwargs . Losses Losses can request all the available parameters that Elegy provides for dependency injection. A typical loss will request the y_true and y_pred values (as its common / enforced in Keras). The Mean Squared Error loss for example is easily defined in these terms: class MSE ( elegy . Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 ) ... X_train , y_train = get_inputs () model . fit ( x = X_train , y = y_train , loss = MSE (), ) Here the input y is passed as y_true to MSE . For an auto-encoder however, it makes perfect sense to define the loss only in terms of x (according to the math) and Elegy lets you do exactly that: class AutoEncoderLoss ( elegy . Loss ): def call ( self , x , y_pred ): return jnp . mean ( jnp . square ( x - y_pred ), axis =- 1 ) ... X_train , _ = get_inputs () model . fit ( x = X_train , loss = AutoEncoderLoss (), ) Notice thanks to this we didn't have to define y on the fit method. Note An alternative here is to just use the previous definition of MSE and define y=X_train . However, avoiding the creation of redundant information is good in general and being explicit about dependencies helps documenting the behaviour of the model. Partitioning a loss If you have a complex loss function that is just a sum of different parts that have to be compute together you might define something like this: class SomeComplexFunction ( elegy . Loss ): def call ( self , x , y_true , y_pred , parameters , ... ): ... return a + b + c Elegy lets you return a dict specifying the name of each part: class SomeComplexFunction ( elegy . Loss ): def call ( self , x , y_true , y_pred , parameters , ... ): ... return { \"a\" : a , \"b\" : b , \"c\" : c , } Elegy will use this information to show you each loss separate in the logs / Tensorboard / History with the names: some_complex_function_loss/a some_complex_function_loss/b some_complex_function_loss/c Each individual loss will still be subject to the sample_weight and reduction behavior as specified to SomeComplexFunction . Multiple Outputs + Labels The Model 's constructor loss argument can accept a single Loss , a list or dict of losses, and even nested structures of the previous, yet in Elegy the form of loss is not strictly related to structure of input labels and outputs of the model. This is very different to Keras where each loss has to be matched with exactly one (label, output) pair. Elegy's method of dealing with multiple outputs and labels is super simple: Quote y_true will contain the entire structure passed to y . y_pred will contain the entire structure output by the Module . This means there are no restrictions on how you structure the loss function. According to this rule Keras and Elegy behave the same when there is only one output and one label because there is no structure. Both framework will allow you to define something like: model = Model ( ... loss = elegy . losses . CategoricalCrossentropy ( from_logits = True ) ) However, if you have many outputs and many labels, Elegy will just pass their structures to your loss and you will be able to do whatever you want by e.g. indexing these structures: class MyLoss ( Elegy . Loss ): def call ( self , y_true , y_pred ): return some_function ( y_true [ \"label_a\" ], y_pred [ \"output_a\" ], y_true [ \"label_b\" ] ) model = Model ( ... loss = elegy . losses . MyLoss () ) This example assumes the y_true and y_pred are dictionaries but they can also be tuples or nested structures. This strategy gives you maximal flexibility but come with the additional cost of having to implement your own loss function. Keras-like behavior While having this flexibility available is good, there is a common scenario that Keras covers really well: what if you really just need one loss per (label, output) pair? In other words, how can we achieve equivalent behaviour of the following Keras code? class MyModel ( keras . Model ): def call ( self , x ): ... return { \"key_a\" : key_a , \"key_b\" : key_b , ... } ... model . compile ( loss = { \"key_a\" : keras . losses . BinaryCrossentropy (), \"key_b\" : keras . losses . MeanSquaredError (), ... }, loss_weights = { \"key_a\" : 10.0 , \"key_b\" : 1.0 , ... }, ) To do this Elegy lets each Loss optionally filter / index the y_true and y_pred arguments based on a string key (for dict s) or integer key (for tuple s) in the constructor's on parameter: class MyModule ( elegy . Module ): def call ( self , x ): ... return { \"key_a\" : key_a , \"key_b\" : key_b , ... } ... model = elegy . Model ( module = MyModule (), loss = [ elegy . losses . BinaryCrossentropy ( on = \"key_a\" , weight = 10.0 ), elegy . losses . MeanSquaredError ( on = \"key_b\" , weight = 1.0 ), ... ] ) This is almost exactly how Keras behaves except each loss is explicitly aware of which part of the output / label its supposed to attend to. The previous is roughly equivalent to manually indexing y_true and y_pred and passing the resulting value to the loss in question like this: model = elegy . Model ( module = MyModule (), loss = [ lambda y_true , y_pred : elegy . losses . BinaryCrossentropy ( weight = 10.0 )( y_true = y_true [ \"key_a\" ], y_pred = y_pred [ \"key_a\" ], ), lambda y_true , y_pred : elegy . losses . MeanSquaredError ( weight = 1.0 )( y_true = y_true [ \"key_b\" ], y_pred = y_pred [ \"key_b\" ], ), ... ] ) Note For the same reasons Elegy doesn't support the loss_weights parameter as defined in keras.compile . Nonetheless, each loss accepts a weight argument directly, as seen in the examples above, which you can use to recover this behavior. Metrics Metrics behave exactly like losses except for one thing: Quote Metrics can hold state. As in Keras, Elegy metrics are cumulative so they update their internal state on every step. From a user's perspective this means that you can use the elegy.get_state and elegy.set_state hooks when implementing your own metrics. Here is an example of a simple cumulative implementation of Accuracy which uses state hooks: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = elegy . get_state ( \"total\" , initializer = 0 ) count = elegy . get_state ( \"count\" , initializer = 0 ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) elegy . set_state ( \"total\" , total ) elegy . set_state ( \"count\" , count ) return total / count For a more in-depth description of how Elegy's hook system works check out the Module System guide.","title":"Modules, Losses, and Metrics"},{"location":"guides/modules-losses-metrics/#modules-losses-and-metrics","text":"This guide goes into depth on how modules, losses and metrics work in Elegy when used with an elegy.Model . For more in-depth explanation on how they work internally check out the Module System guide.","title":"Modules, Losses, and Metrics"},{"location":"guides/modules-losses-metrics/#keras-limitations","text":"One of our goals with Elegy was to solve Keras restrictions around the type of losses and metrics you can define. When creating a complex model with multiple outputs in Keras, say output_a and output_b , you are forced to define losses and metrics per-output only: model . compile ( loss = { \"output_a\" : keras . losses . BinaryCrossentropy ( from_logits = True ), \"output_b\" : keras . losses . CategoricalCrossentropy ( from_logits = True ), }, metrics = { \"output_a\" : keras . losses . BinaryAccuracy ( from_logits = True ), \"output_b\" : keras . losses . CategoricalAccuracy ( from_logits = True ), }, ... ) This very restrictive, in particular it doesn't allow the following: Losses and metrics that combine multiple outputs with multiple labels. A single loss/metrics based on multiple outputs (a especial case of the previous). Losses and metrics that depend on other variables such as inputs, parameters, states, etc. Most of these are usually solvable by tricks such as: * Concatenating the outputs / labels * Passing the inputs and other kind of information as labels. * Using the functional API which is more flexible (but it only runs on graph mode making it very painful to debug). It is clear that these solution are hacky, sometimes they are non-obvious, and depending on the problem they can be insufficient.","title":"Keras Limitations"},{"location":"guides/modules-losses-metrics/#dependency-injection","text":"Elegy solves the previous problems by introducing a dependency injection mechanism that allows the user to express complex functions by simply declaring the variables it wants to use by their name . The following parameters are available for the different callables you pass to Elegy: parameter Description Module Metric Loss x Inputs of the model corresponding to the x argument of fit * x x y_true The input labels corresponding to the y argument of fit x x y_pred Outputs of the model x x sample_weight Importance of each sample x x class_weight Importance of each class x x training Whether training is currently in progress x x x parameters The learnable parameters of the model x x states The non-learnable parameters of the model x x Note The content of x is technically passed to the model's Module but the parameter name \"x\" will bare no special meaning in that context.","title":"Dependency Injection"},{"location":"guides/modules-losses-metrics/#modules","text":"Modules define the architecture of the network, their primary task (in Elegy terms) is transforming the inputs x into outputs y_pred . To make it easy to consume the content of x , Elegy has some special but very simple rules on how the signature of any Module can be structured: 1. If x is simply an array it will be passed directly: class SomeModule ( elegy . Module ): def call ( self , m ): ... model = elegy . Model ( SomeModule (), ... ) a = get_inputs () model . fit ( x = a , ... ) In this case a is passed as m . 2. If x is a tuple , then x will be expanded positional arguments a.k.a. *args , this means that the module will have define exactly as many arguments as there are inputs. For example: class SomeModule ( elegy . Module ): def call ( self , m , n ): ... model = elegy . Model ( SomeModule (), ... ) a , b = get_inputs () model . fit ( x = ( a , b ), ... ) In this case a is passed as m and b is passed as n . 3. If x is a dict , then x will be expanded as keyword arguments a.k.a. **kwargs , in this case the module can optionally request any key defined in x as an argument. For example: class SomeModule ( elegy . Module ): def call ( self , n , o ): ... model = elegy . Model ( SomeModule (), ... ) a , b , c = get_inputs () model . fit ( x = { \"m\" : a , \"n\" : b , \"o\" : c }, ... ) Here only n and o are requested by name and you get as input its values b and c , the variable m with the content of a is safely ignored. If you want to request all the avaiable inputs you can use **kwargs .","title":"Modules"},{"location":"guides/modules-losses-metrics/#losses","text":"Losses can request all the available parameters that Elegy provides for dependency injection. A typical loss will request the y_true and y_pred values (as its common / enforced in Keras). The Mean Squared Error loss for example is easily defined in these terms: class MSE ( elegy . Loss ): def call ( self , y_true , y_pred ): return jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 ) ... X_train , y_train = get_inputs () model . fit ( x = X_train , y = y_train , loss = MSE (), ) Here the input y is passed as y_true to MSE . For an auto-encoder however, it makes perfect sense to define the loss only in terms of x (according to the math) and Elegy lets you do exactly that: class AutoEncoderLoss ( elegy . Loss ): def call ( self , x , y_pred ): return jnp . mean ( jnp . square ( x - y_pred ), axis =- 1 ) ... X_train , _ = get_inputs () model . fit ( x = X_train , loss = AutoEncoderLoss (), ) Notice thanks to this we didn't have to define y on the fit method. Note An alternative here is to just use the previous definition of MSE and define y=X_train . However, avoiding the creation of redundant information is good in general and being explicit about dependencies helps documenting the behaviour of the model.","title":"Losses"},{"location":"guides/modules-losses-metrics/#partitioning-a-loss","text":"If you have a complex loss function that is just a sum of different parts that have to be compute together you might define something like this: class SomeComplexFunction ( elegy . Loss ): def call ( self , x , y_true , y_pred , parameters , ... ): ... return a + b + c Elegy lets you return a dict specifying the name of each part: class SomeComplexFunction ( elegy . Loss ): def call ( self , x , y_true , y_pred , parameters , ... ): ... return { \"a\" : a , \"b\" : b , \"c\" : c , } Elegy will use this information to show you each loss separate in the logs / Tensorboard / History with the names: some_complex_function_loss/a some_complex_function_loss/b some_complex_function_loss/c Each individual loss will still be subject to the sample_weight and reduction behavior as specified to SomeComplexFunction .","title":"Partitioning a loss"},{"location":"guides/modules-losses-metrics/#multiple-outputs-labels","text":"The Model 's constructor loss argument can accept a single Loss , a list or dict of losses, and even nested structures of the previous, yet in Elegy the form of loss is not strictly related to structure of input labels and outputs of the model. This is very different to Keras where each loss has to be matched with exactly one (label, output) pair. Elegy's method of dealing with multiple outputs and labels is super simple: Quote y_true will contain the entire structure passed to y . y_pred will contain the entire structure output by the Module . This means there are no restrictions on how you structure the loss function. According to this rule Keras and Elegy behave the same when there is only one output and one label because there is no structure. Both framework will allow you to define something like: model = Model ( ... loss = elegy . losses . CategoricalCrossentropy ( from_logits = True ) ) However, if you have many outputs and many labels, Elegy will just pass their structures to your loss and you will be able to do whatever you want by e.g. indexing these structures: class MyLoss ( Elegy . Loss ): def call ( self , y_true , y_pred ): return some_function ( y_true [ \"label_a\" ], y_pred [ \"output_a\" ], y_true [ \"label_b\" ] ) model = Model ( ... loss = elegy . losses . MyLoss () ) This example assumes the y_true and y_pred are dictionaries but they can also be tuples or nested structures. This strategy gives you maximal flexibility but come with the additional cost of having to implement your own loss function.","title":"Multiple Outputs + Labels"},{"location":"guides/modules-losses-metrics/#keras-like-behavior","text":"While having this flexibility available is good, there is a common scenario that Keras covers really well: what if you really just need one loss per (label, output) pair? In other words, how can we achieve equivalent behaviour of the following Keras code? class MyModel ( keras . Model ): def call ( self , x ): ... return { \"key_a\" : key_a , \"key_b\" : key_b , ... } ... model . compile ( loss = { \"key_a\" : keras . losses . BinaryCrossentropy (), \"key_b\" : keras . losses . MeanSquaredError (), ... }, loss_weights = { \"key_a\" : 10.0 , \"key_b\" : 1.0 , ... }, ) To do this Elegy lets each Loss optionally filter / index the y_true and y_pred arguments based on a string key (for dict s) or integer key (for tuple s) in the constructor's on parameter: class MyModule ( elegy . Module ): def call ( self , x ): ... return { \"key_a\" : key_a , \"key_b\" : key_b , ... } ... model = elegy . Model ( module = MyModule (), loss = [ elegy . losses . BinaryCrossentropy ( on = \"key_a\" , weight = 10.0 ), elegy . losses . MeanSquaredError ( on = \"key_b\" , weight = 1.0 ), ... ] ) This is almost exactly how Keras behaves except each loss is explicitly aware of which part of the output / label its supposed to attend to. The previous is roughly equivalent to manually indexing y_true and y_pred and passing the resulting value to the loss in question like this: model = elegy . Model ( module = MyModule (), loss = [ lambda y_true , y_pred : elegy . losses . BinaryCrossentropy ( weight = 10.0 )( y_true = y_true [ \"key_a\" ], y_pred = y_pred [ \"key_a\" ], ), lambda y_true , y_pred : elegy . losses . MeanSquaredError ( weight = 1.0 )( y_true = y_true [ \"key_b\" ], y_pred = y_pred [ \"key_b\" ], ), ... ] ) Note For the same reasons Elegy doesn't support the loss_weights parameter as defined in keras.compile . Nonetheless, each loss accepts a weight argument directly, as seen in the examples above, which you can use to recover this behavior.","title":"Keras-like behavior"},{"location":"guides/modules-losses-metrics/#metrics","text":"Metrics behave exactly like losses except for one thing: Quote Metrics can hold state. As in Keras, Elegy metrics are cumulative so they update their internal state on every step. From a user's perspective this means that you can use the elegy.get_state and elegy.set_state hooks when implementing your own metrics. Here is an example of a simple cumulative implementation of Accuracy which uses state hooks: class Accuracy ( elegy . Metric ): def call ( self , y_true , y_pred ): total = elegy . get_state ( \"total\" , initializer = 0 ) count = elegy . get_state ( \"count\" , initializer = 0 ) total += jnp . sum ( y_true == y_pred ) count += jnp . prod ( y_true . shape ) elegy . set_state ( \"total\" , total ) elegy . set_state ( \"count\" , count ) return total / count For a more in-depth description of how Elegy's hook system works check out the Module System guide.","title":"Metrics"}]}